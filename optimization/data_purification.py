#!/usr/bin/env python3
"""
ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œ - ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
- í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
- ë…ë¦½ì  íŠ¹ì„± ì¶”ì¶œ
- ë…¸ì´ì¦ˆ ì œê±°
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_regression
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class DataPurifier:
    """ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œ - ë‹¤ì¤‘ê³µì„ ì„± ì œê±°"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}
        self.feature_selector = None

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_pure_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ìˆœìˆ˜ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ìˆœìˆ˜ ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë…¸ì´ì¦ˆ ê¸°ë°˜)
            print("ğŸ” ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # ë°©ë²• 1: ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜
            np.random.seed(42)
            random_target = np.random.uniform(1, 10, len(big5_numeric))

            # ë°©ë²• 2: ì™¸ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš© (Big5 ì™„ì „ ì œì™¸)
            external_target = (
                cmi_numeric.mean(axis=1) * 0.4
                + rppg_numeric.mean(axis=1) * 0.3
                + voice_numeric.mean(axis=1) * 0.3
            )

            # ë°©ë²• 3: ë‘ íƒ€ê²Ÿì˜ ê°€ì¤‘ í‰ê·  (ë…ë¦½ì„± ë³´ì¥)
            targets = random_target * 0.7 + external_target * 0.3

            print(f"âœ… ìˆœìˆ˜ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")
            print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {targets.std():.4f}")
            print(f"   ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜!")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def analyze_correlation(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        print("ğŸ” ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")

        # í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
        X_df = pd.DataFrame(X)
        correlation_matrix = X_df.corr().abs()

        # ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ì°¾ê¸°
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr = correlation_matrix.iloc[i, j]
                if corr > 0.8:  # ë†’ì€ ìƒê´€ê´€ê³„
                    high_corr_pairs.append(
                        {"feature1": i, "feature2": j, "correlation": corr}
                    )

        # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
        target_correlations = []
        for i in range(X.shape[1]):
            corr = np.corrcoef(X[:, i], y)[0, 1]
            target_correlations.append({"feature": i, "correlation": abs(corr)})

        # ìƒê´€ê´€ê³„ ê²°ê³¼ ì •ë ¬
        target_correlations.sort(key=lambda x: x["correlation"], reverse=True)

        print(f"   ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ìŒ: {len(high_corr_pairs)}ê°œ")
        print(f"   ìƒìœ„ 5ê°œ íƒ€ê²Ÿ ìƒê´€ê´€ê³„:")
        for i, corr_info in enumerate(target_correlations[:5]):
            print(
                f"     {i+1}. í”¼ì²˜ {corr_info['feature']}: {corr_info['correlation']:.4f}"
            )

        return {
            "high_corr_pairs": high_corr_pairs,
            "target_correlations": target_correlations,
        }

    def remove_multicollinearity(
        self, X: np.ndarray, threshold: float = 0.8
    ) -> np.ndarray:
        """ë‹¤ì¤‘ê³µì„ ì„± ì œê±°"""
        print("ğŸ”§ ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ì¤‘...")

        # ë¶„ì‚°ì´ ë‚®ì€ í”¼ì²˜ ì œê±°
        variance_selector = VarianceThreshold(threshold=0.01)
        X_variance = variance_selector.fit_transform(X)
        print(f"   ë¶„ì‚° ê¸°ë°˜ ì œê±° í›„: {X_variance.shape[1]}ê°œ í”¼ì²˜")

        # ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ ì œê±°
        X_df = pd.DataFrame(X_variance)
        correlation_matrix = X_df.corr().abs()

        # ì œê±°í•  í”¼ì²˜ ì¸ë±ìŠ¤
        to_drop = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] > threshold:
                    # ë¶„ì‚°ì´ ë‚®ì€ í”¼ì²˜ ì œê±°
                    if X_df.iloc[:, i].var() < X_df.iloc[:, j].var():
                        to_drop.add(i)
                    else:
                        to_drop.add(j)

        # í”¼ì²˜ ì œê±°
        X_cleaned = X_df.drop(columns=X_df.columns[list(to_drop)]).values
        print(f"   ë‹¤ì¤‘ê³µì„ ì„± ì œê±° í›„: {X_cleaned.shape[1]}ê°œ í”¼ì²˜")

        return X_cleaned

    def create_independent_features(self, multimodal_data: Dict) -> np.ndarray:
        """ë…ë¦½ì  íŠ¹ì„± ì¶”ì¶œ"""
        print("ğŸ”§ ë…ë¦½ì  íŠ¹ì„± ì¶”ì¶œ ì¤‘...")

        # 1. ê¸°ë³¸ í”¼ì²˜ë§Œ ì‚¬ìš©
        X_basic = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        print(f"   ê¸°ë³¸ í”¼ì²˜ ìˆ˜: {X_basic.shape[1]}")

        # 2. ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
        X_cleaned = self.remove_multicollinearity(X_basic, threshold=0.7)

        # 3. PCAë¡œ ì°¨ì› ì¶•ì†Œ (ë…ë¦½ì„± ë³´ì¥)
        print("   PCA ì°¨ì› ì¶•ì†Œ ì¤‘...")
        pca = PCA(n_components=min(20, X_cleaned.shape[1]), random_state=42)
        X_pca = pca.fit_transform(X_cleaned)

        print(f"   PCA í›„: {X_pca.shape[1]}ê°œ í”¼ì²˜")
        print(f"   ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_.sum():.4f}")

        return X_pca

    def create_simple_models(self):
        """ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "ridge": Ridge(alpha=10.0),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "svr": SVR(kernel="rbf", C=0.1, gamma="scale"),  # ë§¤ìš° ë³´ìˆ˜ì 
            "random_forest": RandomForestRegressor(
                n_estimators=50,
                max_depth=3,
                min_samples_split=100,
                min_samples_leaf=50,
                max_features="sqrt",
                random_state=42,
                n_jobs=-1,
            ),
            "xgboost": XGBRegressor(
                n_estimators=50,
                learning_rate=0.1,
                max_depth=3,
                subsample=0.7,
                colsample_bytree=0.7,
                reg_alpha=1.0,
                reg_lambda=1.0,
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=50,
                learning_rate=0.1,
                max_depth=3,
                subsample=0.7,
                colsample_bytree=0.7,
                reg_alpha=1.0,
                reg_lambda=1.0,
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
        }

        print(f"âœ… {len(self.models)}ê°œ ë‹¨ìˆœí•œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_pure_data(
        self, X: np.ndarray, y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # StandardScalerë¡œ ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["pure_ensemble"] = scaler

        # í”¼ì²˜ ì„ íƒ (ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ ì„ íƒ - ê³¼ì í•© ë°©ì§€)
        print("   í”¼ì²˜ ì„ íƒ ì¤‘...")
        self.feature_selector = SelectKBest(score_func=f_regression, k=10)
        X_selected = self.feature_selector.fit_transform(X_scaled, y)

        print(f"âœ… ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_selected.shape}")
        print(f"   ì›ë³¸ í”¼ì²˜ ìˆ˜: {X.shape[1]}")
        print(f"   ì„ íƒëœ í”¼ì²˜ ìˆ˜: {X_selected.shape[1]}")

        return X_selected, y

    def train_simple_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸš€ ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (3ë‹¨ê³„)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.3, random_state=42
        )

        model_results = {}
        kf = KFold(n_splits=3, shuffle=True, random_state=42)

        for name, model in self.models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦
                cv_r2_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model,
                    X_train,
                    y_train,
                    cv=kf,
                    scoring="neg_root_mean_squared_error",
                    n_jobs=-1,
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ê²€ì¦ ì„±ëŠ¥
                val_pred = model.predict(X_val)
                val_r2 = r2_score(y_val, val_pred)
                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))

                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                # ê³¼ì í•© ê°„ê²© ê³„ì‚°
                overfitting_gap = val_r2 - test_r2

                model_results[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "val_r2": val_r2,
                    "val_rmse": val_rmse,
                    "test_r2": test_r2,
                    "test_rmse": test_rmse,
                    "overfitting_gap": overfitting_gap,
                    "model": model,
                }

                print(f"     CV RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f})")
                print(f"     Val RÂ²: {val_r2:.4f}")
                print(f"     Test RÂ²: {test_r2:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {overfitting_gap:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ì í•© ê°„ê²© ê³ ë ¤)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )

        print(f"âœ… {len(valid_models)}ê°œ ë‹¨ìˆœí•œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ë‹¨ìˆœí•œ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (ê³¼ì í•© ê°„ê²© ê³ ë ¤):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(
                f"   {i}. {name}: Test RÂ² = {scores['test_r2']:.4f}, ê³¼ì í•© = {scores['overfitting_gap']:.4f}"
            )

        return model_results

    def create_simple_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_results: Dict
    ) -> Dict:
        """ë‹¨ìˆœí•œ ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ”„ ë‹¨ìˆœí•œ ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 2ê°œ ëª¨ë¸ë§Œ ì„ íƒ (ê³¼ì í•© ë°©ì§€)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )
        top_models = [name for name, _ in sorted_models[:2]]

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        predictions = []
        weights = []

        for name in top_models:
            if name in model_results and model_results[name] is not None:
                model = model_results[name]["model"]
                pred = model.predict(X_test)
                predictions.append(pred)
                # ê³¼ì í•© ê°„ê²©ì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜
                weight = (
                    model_results[name]["test_r2"]
                    - model_results[name]["overfitting_gap"]
                )
                weights.append(max(weight, 0.1))

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y_test, ensemble_pred)
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y_test))
        correlation = np.corrcoef(ensemble_pred, y_test)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y_test,
            "selected_models": top_models,
            "model_weights": dict(zip(top_models, weights)),
        }

        print(f"âœ… ë‹¨ìˆœí•œ ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_data_purification(self, limit: int = 10000) -> Dict:
        """ë°ì´í„° ì •ì œ ì‹¤í–‰"""
        print("ğŸš€ ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œ ì‹œì‘ - ë‹¤ì¤‘ê³µì„ ì„± ì œê±°")
        print("=" * 60)

        # 1. ìˆœìˆ˜ ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_pure_data(limit)

        # 2. ë…ë¦½ì  íŠ¹ì„± ì¶”ì¶œ
        X_engineered = self.create_independent_features(multimodal_data)

        # 3. ìƒê´€ê´€ê³„ ë¶„ì„
        correlation_analysis = self.analyze_correlation(X_engineered, targets)

        # 4. ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ ìƒì„±
        self.create_simple_models()

        # 5. ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_pure_data(X_engineered, targets)

        # 6. ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ í›ˆë ¨
        model_results = self.train_simple_models(X, y)

        # 7. ë‹¨ìˆœí•œ ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_simple_ensemble(X, y, model_results)

        # 8. ê²°ê³¼ ì €ì¥
        results = {
            "correlation_analysis": correlation_analysis,
            "individual_models_results": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "val_r2": scores["val_r2"] if scores else None,
                    "test_r2": scores["test_r2"] if scores else None,
                    "overfitting_gap": scores["overfitting_gap"] if scores else None,
                }
                for name, scores in model_results.items()
            },
            "ensemble_results": ensemble_results,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": X_engineered.shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_results.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "purification_status": "ë°ì´í„° ì •ì œ ì™„ë£Œ",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("data_purification_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œ - ë‹¤ì¤‘ê³µì„ ì„± ì œê±°")
    print("=" * 60)

    purifier = DataPurifier()
    results = purifier.run_data_purification(limit=10000)

    print("\nğŸ“Š ë°ì´í„° ì •ì œ ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")


if __name__ == "__main__":
    main()
