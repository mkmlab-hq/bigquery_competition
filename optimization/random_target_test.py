#!/usr/bin/env python3
"""
ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ - ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ê²€ì¦
- ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- ë°ì´í„° ëˆ„ì¶œì´ ìˆë‹¤ë©´ ëœë¤ íƒ€ê²Ÿì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ê²ƒ
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class RandomTargetTester:
    """ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            print("ğŸ” ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            np.random.seed(42)
            random_targets = np.random.uniform(1, 10, limit)  # 1-10 ë²”ìœ„ì˜ ì™„ì „ ëœë¤

            print(f"âœ… ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ:")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Random Targets: {random_targets.shape}")
            print(f"   ëœë¤ íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {random_targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {random_targets.std():.4f}")
            print(f"     ìµœì†Œê°’: {random_targets.min():.4f}")
            print(f"     ìµœëŒ€ê°’: {random_targets.max():.4f}")

            return multimodal_data, random_targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_models(self):
        """ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=10,
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=1.0),
            "elastic_net": ElasticNet(alpha=0.1, l1_ratio=0.5),
            "svr": SVR(kernel="rbf", C=1.0, gamma="scale"),
        }

        print(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_data(
        self, multimodal_data: Dict, targets: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ê²°í•©
        X = np.concatenate(
            [
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # RobustScalerë¡œ ì •ê·œí™”
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["robust_ensemble"] = scaler

        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape}")
        return X_scaled, targets

    def test_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        print("ğŸš€ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        model_results = {}

        for name, model in self.models.items():
            print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {name}")

            try:
                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ì˜ˆì¸¡
                train_pred = model.predict(X_train)
                test_pred = model.predict(X_test)

                # ì„±ëŠ¥ í‰ê°€
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                model_results[name] = {
                    "train_r2": train_r2,
                    "test_r2": test_r2,
                    "train_rmse": train_rmse,
                    "test_rmse": test_rmse,
                    "overfitting_gap": train_r2 - test_r2,
                }

                print(f"     í›ˆë ¨ RÂ²: {train_r2:.4f}, í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
                print(f"     í›ˆë ¨ RMSE: {train_rmse:.4f}, í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {train_r2 - test_r2:.4f}")

            except Exception as e:
                print(f"     âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        return model_results

    def run_random_target_test(self, limit: int = 10000) -> Dict:
        """ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_real_bigquery_data(limit)

        # 2. ëª¨ë¸ë“¤ ìƒì„±
        self.create_models()

        # 3. ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_data(multimodal_data, targets)

        # 4. ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸
        model_results = self.test_models(X, y)

        # 5. ê²°ê³¼ ë¶„ì„
        valid_results = {k: v for k, v in model_results.items() if v is not None}

        if valid_results:
            avg_test_r2 = np.mean([r["test_r2"] for r in valid_results.values()])
            max_test_r2 = np.max([r["test_r2"] for r in valid_results.values()])
            min_test_r2 = np.min([r["test_r2"] for r in valid_results.values()])

            print(f"\nğŸ“Š ëœë¤ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"   í‰ê·  í…ŒìŠ¤íŠ¸ RÂ²: {avg_test_r2:.4f}")
            print(f"   ìµœëŒ€ í…ŒìŠ¤íŠ¸ RÂ²: {max_test_r2:.4f}")
            print(f"   ìµœì†Œ í…ŒìŠ¤íŠ¸ RÂ²: {min_test_r2:.4f}")

            # ë°ì´í„° ëˆ„ì¶œ íŒë‹¨
            if avg_test_r2 > 0.5:
                print("   ğŸš¨ ê²½ê³ : ëœë¤ íƒ€ê²Ÿì—ì„œë„ ë†’ì€ ì„±ëŠ¥! ë°ì´í„° ëˆ„ì¶œ ì˜ì‹¬!")
            elif avg_test_r2 > 0.2:
                print("   âš ï¸ ì£¼ì˜: ëœë¤ íƒ€ê²Ÿì—ì„œ ì¤‘ê°„ ì„±ëŠ¥. ì¼ë¶€ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„±.")
            else:
                print("   âœ… ì–‘í˜¸: ëœë¤ íƒ€ê²Ÿì—ì„œ ë‚®ì€ ì„±ëŠ¥. ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ.")

        # 6. ê²°ê³¼ ì €ì¥
        results = {
            "model_results": model_results,
            "summary": {
                "avg_test_r2": float(avg_test_r2) if valid_results else 0,
                "max_test_r2": float(max_test_r2) if valid_results else 0,
                "min_test_r2": float(min_test_r2) if valid_results else 0,
                "data_leakage_suspected": avg_test_r2 > 0.5 if valid_results else False,
            },
        }

        with open("random_target_test_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ - ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ê²€ì¦")
    print("=" * 60)

    tester = RandomTargetTester()
    results = tester.run_random_target_test(limit=10000)

    print("\nğŸ“Š ëœë¤ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   í‰ê·  í…ŒìŠ¤íŠ¸ RÂ²: {results['summary']['avg_test_r2']:.4f}")
    print(f"   ë°ì´í„° ëˆ„ì¶œ ì˜ì‹¬: {results['summary']['data_leakage_suspected']}")


if __name__ == "__main__":
    main()
