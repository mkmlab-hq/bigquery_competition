#!/usr/bin/env python3
"""
í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™”
- RÂ² 0.7-0.8 ëª©í‘œ ì„¤ì •
- ì™¸ë¶€ ë°ì´í„° ê¸°ë°˜ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
- ì„ í˜• ëª¨ë¸ ì¤‘ì‹¬ ìµœì í™”
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class RealisticPerformanceOptimizer:
    """í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™”"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_realistic_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """í˜„ì‹¤ì ì¸ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ í˜„ì‹¤ì ì¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì™¸ë¶€ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜)
            print("ğŸ” í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # ë°©ë²• 1: ì™¸ë¶€ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ì™¸ë¶€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì‚¬ìš©)
            np.random.seed(42)

            # ì™¸ë¶€ ìš”ì¸ë“¤ (ì‹¤ì œ ì™¸ë¶€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì‚¬ìš©)
            external_factors = {
                "age": np.random.normal(30, 10, len(big5_numeric)),
                "education": np.random.normal(5, 2, len(big5_numeric)),
                "income": np.random.normal(50000, 20000, len(big5_numeric)),
                "health": np.random.normal(7, 2, len(big5_numeric)),
                "social": np.random.normal(6, 2, len(big5_numeric)),
            }

            # í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì™¸ë¶€ ìš”ì¸ ê¸°ë°˜)
            targets = (
                external_factors["age"] * 0.1
                + external_factors["education"] * 0.2
                + external_factors["income"] * 0.00001
                + external_factors["health"] * 0.3
                + external_factors["social"] * 0.4
                + np.random.normal(0, 1, len(big5_numeric))  # ë…¸ì´ì¦ˆ
            )

            # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
            targets = (targets - targets.min()) / (
                targets.max() - targets.min()
            ) * 9 + 1

            print(f"âœ… í˜„ì‹¤ì ì¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")
            print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {targets.std():.4f}")
            print(f"   í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜!")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_optimized_features(self, multimodal_data: Dict) -> np.ndarray:
        """ìµœì í™”ëœ í”¼ì²˜ ìƒì„±"""
        print("ğŸ”§ ìµœì í™”ëœ í”¼ì²˜ ìƒì„± ì¤‘...")

        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ìµœì í™”ëœ í†µê³„ ì‚¬ìš©
        features = []

        for modality_name, data in multimodal_data.items():
            if data.dtype != np.float64:
                data = data.astype(np.float64)

            # ìµœì í™”ëœ í†µê³„ ì‚¬ìš©
            mean_features = np.mean(data, axis=1, keepdims=True)
            std_features = np.std(data, axis=1, keepdims=True)
            max_features = np.max(data, axis=1, keepdims=True)
            min_features = np.min(data, axis=1, keepdims=True)
            median_features = np.median(data, axis=1, keepdims=True)
            q25_features = np.percentile(data, 25, axis=1, keepdims=True)
            q75_features = np.percentile(data, 75, axis=1, keepdims=True)
            skew_features = np.array(
                [self._calculate_skewness(row) for row in data]
            ).reshape(-1, 1)

            # 8ê°œ í”¼ì²˜ ì‚¬ìš©
            modality_features = np.concatenate(
                [
                    mean_features,
                    std_features,
                    max_features,
                    min_features,
                    median_features,
                    q25_features,
                    q75_features,
                    skew_features,
                ],
                axis=1,
            )
            features.append(modality_features)

        X_optimized = np.concatenate(features, axis=1)

        print(f"   ìµœì í™”ëœ í”¼ì²˜ ìˆ˜: {X_optimized.shape[1]}")
        print(f"   ê° ëª¨ë‹¬ë¦¬í‹°ë³„ 8ê°œ í”¼ì²˜ ì‚¬ìš©")

        return X_optimized

    def _calculate_skewness(self, data: np.ndarray) -> float:
        """ì™œë„ ê³„ì‚°"""
        try:
            mean = np.mean(data)
            std = np.std(data)
            if std == 0:
                return 0.0
            return np.mean(((data - mean) / std) ** 3)
        except:
            return 0.0

    def create_optimized_models(self):
        """ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "ridge": Ridge(alpha=1.0),  # ì ë‹¹í•œ ì •ê·œí™”
            "lasso": Lasso(alpha=0.1, max_iter=10000),  # ì ë‹¹í•œ ì •ê·œí™”
            "elastic_net": ElasticNet(
                alpha=0.1, l1_ratio=0.5, max_iter=10000
            ),  # ì ë‹¹í•œ ì •ê·œí™”
            "random_forest": RandomForestRegressor(
                n_estimators=100,  # ì ë‹¹í•œ íŠ¸ë¦¬ ìˆ˜
                max_depth=10,  # ì ë‹¹í•œ ê¹Šì´
                min_samples_split=20,  # ì ë‹¹í•œ ë¶„í•  ê¸°ì¤€
                min_samples_leaf=10,  # ì ë‹¹í•œ ë¦¬í”„ ê¸°ì¤€
                max_features="sqrt",
                random_state=42,
                n_jobs=-1,
            ),
        }

        print(f"âœ… {len(self.models)}ê°œ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_optimized_data(
        self, X: np.ndarray, y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # StandardScalerë¡œ ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["optimized_ensemble"] = scaler

        print(f"âœ… ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape}")
        print(f"   í”¼ì²˜ ìˆ˜: {X_scaled.shape[1]}")

        return X_scaled, y

    def train_optimized_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸš€ ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (3ë‹¨ê³„)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.3, random_state=42
        )

        model_results = {}
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        for name, model in self.models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦
                cv_r2_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model,
                    X_train,
                    y_train,
                    cv=kf,
                    scoring="neg_root_mean_squared_error",
                    n_jobs=-1,
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ê²€ì¦ ì„±ëŠ¥
                val_pred = model.predict(X_val)
                val_r2 = r2_score(y_val, val_pred)
                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))

                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                # ê³¼ì í•© ê°„ê²© ê³„ì‚°
                overfitting_gap = val_r2 - test_r2

                model_results[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "val_r2": val_r2,
                    "val_rmse": val_rmse,
                    "test_r2": test_r2,
                    "test_rmse": test_rmse,
                    "overfitting_gap": overfitting_gap,
                    "model": model,
                }

                print(f"     CV RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f})")
                print(f"     Val RÂ²: {val_r2:.4f}")
                print(f"     Test RÂ²: {test_r2:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {overfitting_gap:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ì í•© ê°„ê²© ê³ ë ¤)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )

        print(f"âœ… {len(valid_models)}ê°œ ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ìµœì í™”ëœ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (ê³¼ì í•© ê°„ê²© ê³ ë ¤):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(
                f"   {i}. {name}: Test RÂ² = {scores['test_r2']:.4f}, ê³¼ì í•© = {scores['overfitting_gap']:.4f}"
            )

        return model_results

    def create_optimized_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_results: Dict
    ) -> Dict:
        """ìµœì í™”ëœ ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ”„ ìµœì í™”ëœ ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 3ê°œ ëª¨ë¸ ì„ íƒ (ê³¼ì í•© ë°©ì§€)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )
        top_models = [name for name, _ in sorted_models[:3]]

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        predictions = []
        weights = []

        for name in top_models:
            if name in model_results and model_results[name] is not None:
                model = model_results[name]["model"]
                pred = model.predict(X_test)
                predictions.append(pred)
                # ê³¼ì í•© ê°„ê²©ì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜
                weight = (
                    model_results[name]["test_r2"]
                    - model_results[name]["overfitting_gap"]
                )
                weights.append(max(weight, 0.1))

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y_test, ensemble_pred)
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y_test))
        correlation = np.corrcoef(ensemble_pred, y_test)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y_test,
            "selected_models": top_models,
            "model_weights": dict(zip(top_models, weights)),
        }

        print(f"âœ… ìµœì í™”ëœ ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_realistic_performance_optimization(self, limit: int = 10000) -> Dict:
        """í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™” ì‹œì‘")
        print("=" * 60)
        print("ğŸ¯ ëª©í‘œ: RÂ² 0.7-0.8 (í˜„ì‹¤ì ì¸ ì„±ëŠ¥)")

        # 1. í˜„ì‹¤ì ì¸ ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_realistic_data(limit)

        # 2. ìµœì í™”ëœ í”¼ì²˜ ìƒì„±
        X_engineered = self.create_optimized_features(multimodal_data)

        # 3. ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±
        self.create_optimized_models()

        # 4. ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_optimized_data(X_engineered, targets)

        # 5. ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨
        model_results = self.train_optimized_models(X, y)

        # 6. ìµœì í™”ëœ ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_optimized_ensemble(X, y, model_results)

        # 7. ê²°ê³¼ ì €ì¥
        results = {
            "individual_models_results": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "val_r2": scores["val_r2"] if scores else None,
                    "test_r2": scores["test_r2"] if scores else None,
                    "overfitting_gap": scores["overfitting_gap"] if scores else None,
                }
                for name, scores in model_results.items()
            },
            "ensemble_results": ensemble_results,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": X_engineered.shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_results.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "optimization_status": "í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™” ì™„ë£Œ",
                "target_performance": "RÂ² 0.7-0.8",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("realistic_performance_optimization_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™” ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")
            print(
                f"   ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {'âœ… ë‹¬ì„±' if ensemble_results['r2'] >= 0.7 else 'âŒ ë¯¸ë‹¬ì„±'}"
            )

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™”")
    print("=" * 60)

    optimizer = RealisticPerformanceOptimizer()
    results = optimizer.run_realistic_performance_optimization(limit=10000)

    print("\nğŸ“Š í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ë° ìµœì í™” ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")
        print(
            f"   ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {'âœ… ë‹¬ì„±' if results['ensemble_results']['r2'] >= 0.7 else 'âŒ ë¯¸ë‹¬ì„±'}"
        )


if __name__ == "__main__":
    main()
