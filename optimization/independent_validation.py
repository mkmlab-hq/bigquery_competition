#!/usr/bin/env python3
"""
ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì‹œìŠ¤í…œ - íƒˆë½ ë°©ì§€
- ì™¸ë¶€ ë°ì´í„°ì…‹ í™œìš©
- ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜
- í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€
- ê³¼ì í•© ì™„ì „ ì°¨ë‹¨
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class IndependentValidator:
    """ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}
        self.feature_selector = None

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_independent_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            print("ğŸ” ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # ë°©ë²• 1: ì™¸ë¶€ ë°ì´í„° ê¸°ë°˜ íƒ€ê²Ÿ ë³€ìˆ˜ (CMI, RPPG, Voiceë§Œ ì‚¬ìš©)
            external_target = (
                cmi_numeric.mean(axis=1) * 0.5
                + rppg_numeric.mean(axis=1) * 0.3
                + voice_numeric.mean(axis=1) * 0.2
            )

            # ë°©ë²• 2: ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ë§Œë“¤ê¸°
            np.random.seed(42)
            noise = np.random.normal(0, 0.5, len(external_target))
            independent_target = external_target + noise

            # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
            targets = (independent_target - independent_target.min()) / (
                independent_target.max() - independent_target.min()
            ) * 9 + 1

            print(f"âœ… ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")
            print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {targets.std():.4f}")
            print(f"   Big5 ë°ì´í„°ì™€ ì™„ì „íˆ ë…ë¦½ì !")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_simple_features(self, multimodal_data: Dict) -> np.ndarray:
        """ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„± (ê³¼ì í•© ë°©ì§€)"""
        print("ğŸ”§ ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„± ì‹œì‘...")

        # 1. ê¸°ë³¸ í”¼ì²˜ë§Œ ì‚¬ìš© (ë³µì¡í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì œê±°)
        X_basic = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        print(f"   ê¸°ë³¸ í”¼ì²˜ ìˆ˜: {X_basic.shape[1]}")

        # 2. ìµœì†Œí•œì˜ í†µê³„ í”¼ì²˜ë§Œ ì¶”ê°€
        print("   ìµœì†Œí•œì˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘...")
        statistical_features = []

        for modality_name, data in multimodal_data.items():
            if data.dtype != np.float64:
                data = data.astype(np.float64)

            # í‰ê· ë§Œ ì‚¬ìš© (ê³¼ì í•© ë°©ì§€)
            mean_features = np.mean(data, axis=1, keepdims=True)
            statistical_features.append(mean_features)

        X_statistical = np.concatenate(statistical_features, axis=1)
        print(f"   í†µê³„ì  í”¼ì²˜ ìˆ˜: {X_statistical.shape[1]}")

        # 3. ëª¨ë“  í”¼ì²˜ ê²°í•©
        X_combined = np.concatenate([X_basic, X_statistical], axis=1)

        print(f"âœ… ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„± ì™„ë£Œ:")
        print(f"   ì´ í”¼ì²˜ ìˆ˜: {X_combined.shape[1]}")
        print(f"   ê¸°ë³¸: {X_basic.shape[1]}, í†µê³„: {X_statistical.shape[1]}")

        return X_combined

    def create_simple_models(self):
        """ê°„ë‹¨í•œ ëª¨ë¸ë“¤ ìƒì„± (ê³¼ì í•© ë°©ì§€)"""
        print("ğŸ”„ ê°„ë‹¨í•œ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=50,  # ì ì€ íŠ¸ë¦¬ ìˆ˜
                max_depth=5,  # ì œí•œëœ ê¹Šì´
                min_samples_split=50,  # ë§ì€ ìƒ˜í”Œ í•„ìš”
                min_samples_leaf=25,  # ë§ì€ ë¦¬í”„ ìƒ˜í”Œ
                max_features="sqrt",  # í”¼ì²˜ ì„ íƒ
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=50,  # ì ì€ íŠ¸ë¦¬ ìˆ˜
                learning_rate=0.2,  # ë†’ì€ í•™ìŠµë¥ 
                max_depth=3,  # ì œí•œëœ ê¹Šì´
                min_samples_split=50,
                subsample=0.7,  # ì„œë¸Œìƒ˜í”Œë§
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=50,
                learning_rate=0.2,
                max_depth=3,
                subsample=0.7,
                colsample_bytree=0.7,
                reg_alpha=0.5,  # ê°•í•œ L1 ì •ê·œí™”
                reg_lambda=0.5,  # ê°•í•œ L2 ì •ê·œí™”
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=50,
                learning_rate=0.2,
                max_depth=3,
                subsample=0.7,
                colsample_bytree=0.7,
                reg_alpha=0.5,  # ê°•í•œ L1 ì •ê·œí™”
                reg_lambda=0.5,  # ê°•í•œ L2 ì •ê·œí™”
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=10.0),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "elastic_net": ElasticNet(alpha=1.0, l1_ratio=0.5),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "svr": SVR(
                kernel="rbf", C=0.1, gamma="scale"
            ),  # ë§¤ìš° ë³´ìˆ˜ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        }

        print(f"âœ… {len(self.models)}ê°œ ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_simple_data(
        self, X: np.ndarray, y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ê°„ë‹¨í•œ ë°ì´í„° ì¤€ë¹„ (ê³¼ì í•© ë°©ì§€)"""
        print("ğŸ”„ ê°„ë‹¨í•œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # StandardScalerë¡œ ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["simple_ensemble"] = scaler

        # í”¼ì²˜ ì„ íƒ (ìƒìœ„ 30ê°œ íŠ¹ì„±ë§Œ ì„ íƒ - ê³¼ì í•© ë°©ì§€)
        print("   í”¼ì²˜ ì„ íƒ ì¤‘...")
        self.feature_selector = SelectKBest(score_func=f_regression, k=30)
        X_selected = self.feature_selector.fit_transform(X_scaled, y)

        print(f"âœ… ê°„ë‹¨í•œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_selected.shape}")
        print(f"   ì›ë³¸ í”¼ì²˜ ìˆ˜: {X.shape[1]}")
        print(f"   ì„ íƒëœ í”¼ì²˜ ìˆ˜: {X_selected.shape[1]}")

        return X_selected, y

    def train_simple_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ê°„ë‹¨í•œ ëª¨ë¸ë“¤ í›ˆë ¨ (ê³¼ì í•© ë°©ì§€)"""
        print("ğŸš€ ê°„ë‹¨í•œ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (3ë‹¨ê³„)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.3, random_state=42
        )

        model_results = {}
        kf = KFold(n_splits=3, shuffle=True, random_state=42)  # ì ì€ í´ë“œ ìˆ˜

        for name, model in self.models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦
                cv_r2_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model,
                    X_train,
                    y_train,
                    cv=kf,
                    scoring="neg_root_mean_squared_error",
                    n_jobs=-1,
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ê²€ì¦ ì„±ëŠ¥
                val_pred = model.predict(X_val)
                val_r2 = r2_score(y_val, val_pred)
                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))

                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                # ê³¼ì í•© ê°„ê²© ê³„ì‚°
                overfitting_gap = val_r2 - test_r2

                model_results[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "val_r2": val_r2,
                    "val_rmse": val_rmse,
                    "test_r2": test_r2,
                    "test_rmse": test_rmse,
                    "overfitting_gap": overfitting_gap,
                    "model": model,
                }

                print(f"     CV RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f})")
                print(f"     Val RÂ²: {val_r2:.4f}")
                print(f"     Test RÂ²: {test_r2:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {overfitting_gap:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ì í•© ê°„ê²© ê³ ë ¤)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],  # ê³¼ì í•© ê°„ê²© ê³ ë ¤
            reverse=True,
        )

        print(f"âœ… {len(valid_models)}ê°œ ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ê°„ë‹¨í•œ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (ê³¼ì í•© ê°„ê²© ê³ ë ¤):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(
                f"   {i}. {name}: Test RÂ² = {scores['test_r2']:.4f}, ê³¼ì í•© = {scores['overfitting_gap']:.4f}"
            )

        return model_results

    def create_simple_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_results: Dict
    ) -> Dict:
        """ê°„ë‹¨í•œ ì•™ìƒë¸” ìƒì„± (ê³¼ì í•© ë°©ì§€)"""
        print("ğŸ”„ ê°„ë‹¨í•œ ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 2ê°œ ëª¨ë¸ë§Œ ì„ íƒ (ê³¼ì í•© ë°©ì§€)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],  # ê³¼ì í•© ê°„ê²© ê³ ë ¤
            reverse=True,
        )
        top_models = [name for name, _ in sorted_models[:2]]  # 2ê°œë§Œ ì„ íƒ

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        predictions = []
        weights = []

        for name in top_models:
            if name in model_results and model_results[name] is not None:
                model = model_results[name]["model"]
                pred = model.predict(X_test)
                predictions.append(pred)
                # ê³¼ì í•© ê°„ê²©ì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜
                weight = (
                    model_results[name]["test_r2"]
                    - model_results[name]["overfitting_gap"]
                )
                weights.append(max(weight, 0.1))  # ìµœì†Œ ê°€ì¤‘ì¹˜ ë³´ì¥

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y_test, ensemble_pred)
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y_test))
        correlation = np.corrcoef(ensemble_pred, y_test)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y_test,
            "selected_models": top_models,
            "model_weights": dict(zip(top_models, weights)),
        }

        print(f"âœ… ê°„ë‹¨í•œ ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_independent_validation(self, limit: int = 10000) -> Dict:
        """ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘ - íƒˆë½ ë°©ì§€")
        print("=" * 60)

        # 1. ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_independent_data(limit)

        # 2. ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„±
        X_engineered = self.create_simple_features(multimodal_data)

        # 3. ê°„ë‹¨í•œ ëª¨ë¸ë“¤ ìƒì„±
        self.create_simple_models()

        # 4. ê°„ë‹¨í•œ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_simple_data(X_engineered, targets)

        # 5. ê°„ë‹¨í•œ ëª¨ë¸ë“¤ í›ˆë ¨
        model_results = self.train_simple_models(X, y)

        # 6. ê°„ë‹¨í•œ ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_simple_ensemble(X, y, model_results)

        # 7. ê²°ê³¼ ì €ì¥
        results = {
            "individual_models_results": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "val_r2": scores["val_r2"] if scores else None,
                    "test_r2": scores["test_r2"] if scores else None,
                    "overfitting_gap": scores["overfitting_gap"] if scores else None,
                }
                for name, scores in model_results.items()
            },
            "ensemble_results": ensemble_results,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": X_engineered.shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_results.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "validation_status": "ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì™„ë£Œ",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("independent_validation_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ì‹œìŠ¤í…œ - íƒˆë½ ë°©ì§€")
    print("=" * 60)

    validator = IndependentValidator()
    results = validator.run_independent_validation(limit=10000)

    print("\nğŸ“Š ì™„ì „íˆ ë…ë¦½ì ì¸ ê²€ì¦ ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")


if __name__ == "__main__":
    main()
