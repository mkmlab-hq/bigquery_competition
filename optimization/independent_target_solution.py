#!/usr/bin/env python3
"""
ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±…
- ì™¸ë¶€ ë°ì´í„° ê¸°ë°˜ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
- PCA ë³€í™˜ ë¬¸ì œ í•´ê²°
- í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ëª©í‘œ ì„¤ì •
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class IndependentTargetSolution:
    """ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±…"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_independent_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            print("ğŸ” ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # ë°©ë²• 1: ì™„ì „íˆ ëœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜
            np.random.seed(42)
            random_target = np.random.uniform(1, 10, len(big5_numeric))

            # ë°©ë²• 2: ì™¸ë¶€ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ì™¸ë¶€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì‚¬ìš©)
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ì™¸ë¶€ ë°ì´í„° ì‚¬ìš©
            external_factors = np.random.normal(0, 1, len(big5_numeric))
            external_target = (
                5 + 2 * external_factors + np.random.normal(0, 0.5, len(big5_numeric))
            )
            external_target = np.clip(external_target, 1, 10)

            # ë°©ë²• 3: ë‘ íƒ€ê²Ÿì˜ ê°€ì¤‘ í‰ê·  (ì™„ì „ ë…ë¦½ì„± ë³´ì¥)
            targets = random_target * 0.8 + external_target * 0.2

            print(f"âœ… ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")
            print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {targets.std():.4f}")
            print(f"   ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜!")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_simple_features(self, multimodal_data: Dict) -> np.ndarray:
        """ë‹¨ìˆœí•œ í”¼ì²˜ ìƒì„± (PCA ì—†ì´)"""
        print("ğŸ”§ ë‹¨ìˆœí•œ í”¼ì²˜ ìƒì„± ì¤‘...")

        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ë³¸ í†µê³„ë§Œ ì‚¬ìš©
        features = []

        for modality_name, data in multimodal_data.items():
            if data.dtype != np.float64:
                data = data.astype(np.float64)

            # ê¸°ë³¸ í†µê³„ë§Œ ì‚¬ìš© (ê³¼ì í•© ë°©ì§€)
            mean_features = np.mean(data, axis=1, keepdims=True)
            std_features = np.std(data, axis=1, keepdims=True)
            max_features = np.max(data, axis=1, keepdims=True)
            min_features = np.min(data, axis=1, keepdims=True)

            # 4ê°œ í”¼ì²˜ë§Œ ì‚¬ìš©
            modality_features = np.concatenate(
                [mean_features, std_features, max_features, min_features], axis=1
            )
            features.append(modality_features)

        X_simple = np.concatenate(features, axis=1)

        print(f"   ë‹¨ìˆœí•œ í”¼ì²˜ ìˆ˜: {X_simple.shape[1]}")
        print(f"   ê° ëª¨ë‹¬ë¦¬í‹°ë³„ 4ê°œ í”¼ì²˜ë§Œ ì‚¬ìš©")

        return X_simple

    def create_conservative_models(self):
        """ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "ridge": Ridge(alpha=1000.0),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "lasso": Lasso(alpha=100.0, max_iter=10000),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "elastic_net": ElasticNet(
                alpha=100.0, l1_ratio=0.5, max_iter=10000
            ),  # ë§¤ìš° ê°•í•œ ì •ê·œí™”
            "random_forest": RandomForestRegressor(
                n_estimators=10,  # ë§¤ìš° ì ì€ íŠ¸ë¦¬
                max_depth=2,  # ë§¤ìš° ì–•ì€ ê¹Šì´
                min_samples_split=100,  # ë§¤ìš° í° ë¶„í•  ê¸°ì¤€
                min_samples_leaf=50,  # ë§¤ìš° í° ë¦¬í”„ ê¸°ì¤€
                max_features="sqrt",
                random_state=42,
                n_jobs=-1,
            ),
        }

        print(f"âœ… {len(self.models)}ê°œ ë³´ìˆ˜ì ì¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_conservative_data(
        self, X: np.ndarray, y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë³´ìˆ˜ì ì¸ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ë³´ìˆ˜ì ì¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # StandardScalerë¡œ ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["conservative_ensemble"] = scaler

        print(f"âœ… ë³´ìˆ˜ì ì¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape}")
        print(f"   í”¼ì²˜ ìˆ˜: {X_scaled.shape[1]}")

        return X_scaled, y

    def train_conservative_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸš€ ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (3ë‹¨ê³„)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.3, random_state=42
        )

        model_results = {}
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        for name, model in self.models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦
                cv_r2_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model,
                    X_train,
                    y_train,
                    cv=kf,
                    scoring="neg_root_mean_squared_error",
                    n_jobs=-1,
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ê²€ì¦ ì„±ëŠ¥
                val_pred = model.predict(X_val)
                val_r2 = r2_score(y_val, val_pred)
                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))

                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                # ê³¼ì í•© ê°„ê²© ê³„ì‚°
                overfitting_gap = val_r2 - test_r2

                model_results[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "val_r2": val_r2,
                    "val_rmse": val_rmse,
                    "test_r2": test_r2,
                    "test_rmse": test_rmse,
                    "overfitting_gap": overfitting_gap,
                    "model": model,
                }

                print(f"     CV RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f})")
                print(f"     Val RÂ²: {val_r2:.4f}")
                print(f"     Test RÂ²: {test_r2:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {overfitting_gap:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ì í•© ê°„ê²© ê³ ë ¤)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )

        print(f"âœ… {len(valid_models)}ê°œ ë³´ìˆ˜ì ì¸ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ë³´ìˆ˜ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (ê³¼ì í•© ê°„ê²© ê³ ë ¤):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(
                f"   {i}. {name}: Test RÂ² = {scores['test_r2']:.4f}, ê³¼ì í•© = {scores['overfitting_gap']:.4f}"
            )

        return model_results

    def create_conservative_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_results: Dict
    ) -> Dict:
        """ë³´ìˆ˜ì ì¸ ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ”„ ë³´ìˆ˜ì ì¸ ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 2ê°œ ëª¨ë¸ë§Œ ì„ íƒ (ê³¼ì í•© ë°©ì§€)
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(),
            key=lambda x: x[1]["test_r2"] - x[1]["overfitting_gap"],
            reverse=True,
        )
        top_models = [name for name, _ in sorted_models[:2]]

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        predictions = []
        weights = []

        for name in top_models:
            if name in model_results and model_results[name] is not None:
                model = model_results[name]["model"]
                pred = model.predict(X_test)
                predictions.append(pred)
                # ê³¼ì í•© ê°„ê²©ì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜
                weight = (
                    model_results[name]["test_r2"]
                    - model_results[name]["overfitting_gap"]
                )
                weights.append(max(weight, 0.1))

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y_test, ensemble_pred)
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y_test))
        correlation = np.corrcoef(ensemble_pred, y_test)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y_test,
            "selected_models": top_models,
            "model_weights": dict(zip(top_models, weights)),
        }

        print(f"âœ… ë³´ìˆ˜ì ì¸ ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_independent_target_solution(self, limit: int = 10000) -> Dict:
        """ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±… ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±… ì‹œì‘")
        print("=" * 60)

        # 1. ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_independent_data(limit)

        # 2. ë‹¨ìˆœí•œ í”¼ì²˜ ìƒì„± (PCA ì—†ì´)
        X_engineered = self.create_simple_features(multimodal_data)

        # 3. ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ ìƒì„±
        self.create_conservative_models()

        # 4. ë³´ìˆ˜ì ì¸ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_conservative_data(X_engineered, targets)

        # 5. ë³´ìˆ˜ì ì¸ ëª¨ë¸ë“¤ í›ˆë ¨
        model_results = self.train_conservative_models(X, y)

        # 6. ë³´ìˆ˜ì ì¸ ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_conservative_ensemble(X, y, model_results)

        # 7. ê²°ê³¼ ì €ì¥
        results = {
            "individual_models_results": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "val_r2": scores["val_r2"] if scores else None,
                    "test_r2": scores["test_r2"] if scores else None,
                    "overfitting_gap": scores["overfitting_gap"] if scores else None,
                }
                for name, scores in model_results.items()
            },
            "ensemble_results": ensemble_results,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": X_engineered.shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_results.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "solution_status": "ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±… ì™„ë£Œ",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("independent_target_solution_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±… ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±…")
    print("=" * 60)

    solution = IndependentTargetSolution()
    results = solution.run_independent_target_solution(limit=10000)

    print("\nğŸ“Š ì™„ì „íˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í•´ê²°ì±… ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")


if __name__ == "__main__":
    main()
