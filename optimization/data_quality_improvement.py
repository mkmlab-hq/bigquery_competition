#!/usr/bin/env python3
"""
ë°ì´í„° í’ˆì§ˆ ê°œì„  ë° ê²€ì¦ ì‹œìŠ¤í…œ
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from vector_search_system import Big5VectorSearch


class DataQualityImprover:
    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.vector_search = Big5VectorSearch(project_id)

    def stratified_sampling(
        self, data: pd.DataFrame, sample_size: int = 5000
    ) -> pd.DataFrame:
        """êµ­ê°€ë³„ ê³„ì¸µì  ìƒ˜í”Œë§"""
        print(f"ê³„ì¸µì  ìƒ˜í”Œë§ ì‹œì‘... (ëª©í‘œ: {sample_size}ê±´)")

        # êµ­ê°€ë³„ ë¶„í¬ í™•ì¸
        country_counts = data["country"].value_counts()
        print(f"êµ­ê°€ë³„ ë¶„í¬: {dict(country_counts.head())}")

        # ê° êµ­ê°€ì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
        sampled_data = []
        total_countries = len(country_counts)

        for country in country_counts.index:
            country_data = data[data["country"] == country]
            country_sample_size = max(
                1, int(sample_size * len(country_data) / len(data))
            )

            if len(country_data) >= country_sample_size:
                sampled = country_data.sample(n=country_sample_size, random_state=42)
            else:
                sampled = country_data

            sampled_data.append(sampled)
            print(f"  {country}: {len(sampled)}ê±´ ìƒ˜í”Œë§")

        result = pd.concat(sampled_data, ignore_index=True)
        print(f"ê³„ì¸µì  ìƒ˜í”Œë§ ì™„ë£Œ: {len(result)}ê±´")
        return result

    def normalize_big5_scores(self, data: pd.DataFrame) -> pd.DataFrame:
        """Big5 ì ìˆ˜ ì •ê·œí™” (1-6 ë²”ìœ„ë¥¼ 1-5ë¡œ í†µì¼)"""
        print("Big5 ì ìˆ˜ ì •ê·œí™” ì¤‘...")

        # EST, AGR ì»¬ëŸ¼ë“¤ì„ 2-6 ë²”ìœ„ì—ì„œ 1-5 ë²”ìœ„ë¡œ ë³€í™˜
        for col in data.columns:
            if col.startswith(("EST", "AGR")):
                # 2-6 ë²”ìœ„ë¥¼ 1-5ë¡œ ì„ í˜• ë³€í™˜
                data[col] = ((data[col] - 2) / 4) * 4 + 1

        print("Big5 ì ìˆ˜ ì •ê·œí™” ì™„ë£Œ")
        return data

    def validate_data_quality(self, data: pd.DataFrame) -> dict:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        print("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")

        quality_report = {
            "total_records": len(data),
            "missing_values": data.isnull().sum().sum(),
            "duplicate_records": data.duplicated().sum(),
            "score_ranges": {},
            "country_distribution": data["country"].value_counts().to_dict(),
        }

        # Big5 ì ìˆ˜ ë²”ìœ„ í™•ì¸
        big5_cols = [
            col
            for col in data.columns
            if any(trait in col for trait in ["EXT", "EST", "AGR", "CSN", "OPN"])
        ]
        for trait in ["EXT", "EST", "AGR", "CSN", "OPN"]:
            trait_cols = [col for col in big5_cols if col.startswith(trait)]
            if trait_cols:
                trait_data = data[trait_cols].values.flatten()
                quality_report["score_ranges"][trait] = {
                    "min": float(np.min(trait_data)),
                    "max": float(np.max(trait_data)),
                    "mean": float(np.mean(trait_data)),
                }

        return quality_report

    def improve_data_quality(self, sample_size: int = 5000) -> tuple:
        """ë°ì´í„° í’ˆì§ˆ ê°œì„  ë©”ì¸ í•¨ìˆ˜"""
        print("=== ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œì‘ ===")

        # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ (ë” í° ìƒ˜í”Œ)
        print("1. ì›ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
        raw_data = self.vector_search.load_data(limit=sample_size * 2)

        # 2. ê³„ì¸µì  ìƒ˜í”Œë§
        print("2. ê³„ì¸µì  ìƒ˜í”Œë§ ìˆ˜í–‰...")
        sampled_data = self.stratified_sampling(raw_data, sample_size)

        # 3. Big5 ì ìˆ˜ ì •ê·œí™”
        print("3. Big5 ì ìˆ˜ ì •ê·œí™”...")
        normalized_data = self.normalize_big5_scores(sampled_data.copy())

        # 4. í’ˆì§ˆ ê²€ì¦
        print("4. í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰...")
        quality_report = self.validate_data_quality(normalized_data)

        return normalized_data, quality_report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œìŠ¤í…œ")

    improver = DataQualityImprover()

    # ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹¤í–‰
    improved_data, quality_report = improver.improve_data_quality(sample_size=3000)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê°œì„  ê²°ê³¼")
    print("=" * 60)

    print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
    print(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {quality_report['total_records']:,}")
    print(f"   ê²°ì¸¡ê°’ ìˆ˜: {quality_report['missing_values']}")
    print(f"   ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜: {quality_report['duplicate_records']}")

    print(f"\nğŸ“Š Big5 ì ìˆ˜ ë²”ìœ„ (ì •ê·œí™” í›„):")
    for trait, stats in quality_report["score_ranges"].items():
        print(
            f"   {trait}: {stats['min']:.1f} ~ {stats['max']:.1f} (í‰ê· : {stats['mean']:.2f})"
        )

    print(f"\nğŸŒ êµ­ê°€ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
    for country, count in list(quality_report["country_distribution"].items())[:5]:
        percentage = (count / quality_report["total_records"]) * 100
        print(f"   {country}: {count:,}ê±´ ({percentage:.1f}%)")

    print(f"\nâœ… ë°ì´í„° í’ˆì§ˆ ê°œì„  ì™„ë£Œ!")


if __name__ == "__main__":
    main()
