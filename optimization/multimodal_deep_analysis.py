#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ì‹¬ì¸µ ë¶„ì„
- ê° ëª¨ë‹¬ë¦¬í‹° í†µê³„ì  íŠ¹ì„± ì‹¬ì¸µ ì¡°ì‚¬
- êµì°¨ ìƒê´€ê´€ê³„ ì •ë°€ ë¶„ì„
- ì ì¬ ê³µí†µ ë³€ìˆ˜ ë°œê²¬
"""

import json
import os
import warnings
from typing import Any, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from google.cloud import bigquery
from scipy.stats import pearsonr, spearmanr
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_regression
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class MultimodalDeepAnalyzer:
    """ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ì‹¬ì¸µ ë¶„ì„ê¸°"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_multimodal_data(self, limit: int = 10000) -> Dict[str, np.ndarray]:
        """ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”© ì¤‘...")

        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()
            voice_numeric = voice_df.select_dtypes(include=[np.number])

            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            print(f"âœ… ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.values.shape}")
            print(f"   Voice: {voice_numeric.shape}")

            return multimodal_data

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def analyze_modality_statistics(
        self, multimodal_data: Dict[str, np.ndarray]
    ) -> Dict[str, Dict]:
        """ê° ëª¨ë‹¬ë¦¬í‹° í†µê³„ì  íŠ¹ì„± ì‹¬ì¸µ ì¡°ì‚¬"""
        print("ğŸ” ê° ëª¨ë‹¬ë¦¬í‹° í†µê³„ì  íŠ¹ì„± ë¶„ì„ ì¤‘...")

        modality_stats = {}

        for modality_name, data in multimodal_data.items():
            print(f"   ë¶„ì„ ì¤‘: {modality_name}")

            # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
            if data.dtype != np.float64:
                data = data.astype(np.float64)

            # ê¸°ë³¸ í†µê³„
            stats = {
                "shape": data.shape,
                "mean": np.mean(data, axis=0),
                "std": np.std(data, axis=0),
                "min": np.min(data, axis=0),
                "max": np.max(data, axis=0),
                "median": np.median(data, axis=0),
                "skewness": self._calculate_skewness(data),
                "kurtosis": self._calculate_kurtosis(data),
                "missing_values": np.isnan(data).sum(),
                "zero_values": (data == 0).sum(),
                "negative_values": (data < 0).sum(),
            }

            # ë¶„í¬ íŠ¹ì„±
            stats["distribution_type"] = self._analyze_distribution(data)
            stats["outlier_ratio"] = self._calculate_outlier_ratio(data)
            stats["variance_ratio"] = np.var(data, axis=0) / (
                np.mean(data, axis=0) + 1e-8
            )

            modality_stats[modality_name] = stats

            print(f"     ë¶„í¬ ìœ í˜•: {stats['distribution_type']}")
            print(f"     ì´ìƒì¹˜ ë¹„ìœ¨: {stats['outlier_ratio']:.4f}")
            print(f"     ê²°ì¸¡ê°’: {stats['missing_values']}")

        return modality_stats

    def analyze_cross_correlations(
        self, multimodal_data: Dict[str, np.ndarray]
    ) -> Dict[str, Any]:
        """êµì°¨ ìƒê´€ê´€ê³„ ì •ë°€ ë¶„ì„"""
        print("ğŸ”— êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")

        # ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ëŒ€í‘œ íŠ¹ì„± ì¶”ì¶œ (í‰ê· ê°’)
        modality_representatives = {}
        for modality_name, data in multimodal_data.items():
            if data.shape[1] > 0:
                # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
                if data.dtype != np.float64:
                    data = data.astype(np.float64)
                # ê° ëª¨ë‹¬ë¦¬í‹°ì˜ í‰ê· ê°’ì„ ëŒ€í‘œê°’ìœ¼ë¡œ ì‚¬ìš©
                modality_representatives[modality_name] = np.mean(data, axis=1)

        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        correlation_matrix = np.corrcoef(list(modality_representatives.values()))
        modality_names = list(modality_representatives.keys())

        # ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
        mutual_info_matrix = np.zeros((len(modality_names), len(modality_names)))
        for i, (name1, data1) in enumerate(modality_representatives.items()):
            for j, (name2, data2) in enumerate(modality_representatives.items()):
                if i != j:
                    mi = mutual_info_regression(data1.reshape(-1, 1), data2)[0]
                    mutual_info_matrix[i, j] = mi

        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
        pearson_correlations = {}
        for i, name1 in enumerate(modality_names):
            for j, name2 in enumerate(modality_names):
                if i != j:
                    corr, p_value = pearsonr(
                        modality_representatives[name1], modality_representatives[name2]
                    )
                    pearson_correlations[f"{name1}_vs_{name2}"] = {
                        "correlation": corr,
                        "p_value": p_value,
                        "significant": p_value < 0.05,
                    }

        # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
        spearman_correlations = {}
        for i, name1 in enumerate(modality_names):
            for j, name2 in enumerate(modality_names):
                if i != j:
                    corr, p_value = spearmanr(
                        modality_representatives[name1], modality_representatives[name2]
                    )
                    spearman_correlations[f"{name1}_vs_{name2}"] = {
                        "correlation": corr,
                        "p_value": p_value,
                        "significant": p_value < 0.05,
                    }

        cross_correlation_results = {
            "correlation_matrix": correlation_matrix.tolist(),
            "modality_names": modality_names,
            "pearson_correlations": pearson_correlations,
            "spearman_correlations": spearman_correlations,
            "mutual_info_matrix": mutual_info_matrix.tolist(),
        }

        print("âœ… êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ")
        for pair, stats in pearson_correlations.items():
            print(
                f"   {pair}: r={stats['correlation']:.4f}, p={stats['p_value']:.4f}, sig={stats['significant']}"
            )

        return cross_correlation_results

    def discover_latent_common_variables(
        self, multimodal_data: Dict[str, np.ndarray]
    ) -> Dict[str, Any]:
        """ì ì¬ ê³µí†µ ë³€ìˆ˜ ë°œê²¬"""
        print("ğŸ” ì ì¬ ê³µí†µ ë³€ìˆ˜ ë°œê²¬ ì¤‘...")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ë°ì´í„° ê²°í•©
        all_data = []
        modality_info = []

        for modality_name, data in multimodal_data.items():
            if data.shape[1] > 0:
                all_data.append(data)
                modality_info.extend([modality_name] * data.shape[1])

        if not all_data:
            return {"error": "ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

        X_combined = np.concatenate(all_data, axis=1)

        # PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(10, X_combined.shape[1]))
        X_pca = pca.fit_transform(X_combined)

        # t-SNEë¥¼ í†µí•œ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ
        if X_combined.shape[0] > 1000:
            # í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ìƒ˜í”Œë§
            sample_indices = np.random.choice(X_combined.shape[0], 1000, replace=False)
            X_sample = X_combined[sample_indices]
        else:
            X_sample = X_combined

        tsne = TSNE(n_components=2, random_state=42)
        X_tsne = tsne.fit_transform(X_sample)

        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ PCA ì„±ë¶„ ë¶„ì„
        modality_pca_results = {}
        for modality_name, data in multimodal_data.items():
            if data.shape[1] > 1:
                pca_modality = PCA(n_components=min(3, data.shape[1]))
                X_modality_pca = pca_modality.fit_transform(data)

                modality_pca_results[modality_name] = {
                    "explained_variance_ratio": pca_modality.explained_variance_ratio_.tolist(),
                    "cumulative_variance": np.cumsum(
                        pca_modality.explained_variance_ratio_
                    ).tolist(),
                    "components": X_modality_pca.tolist(),
                }

        latent_variables = {
            "combined_pca": {
                "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
                "cumulative_variance": np.cumsum(
                    pca.explained_variance_ratio_
                ).tolist(),
                "components": X_pca.tolist(),
            },
            "tsne_embedding": X_tsne.tolist(),
            "modality_pca": modality_pca_results,
            "feature_importance": self._calculate_feature_importance(
                X_combined, modality_info
            ),
        }

        print("âœ… ì ì¬ ê³µí†µ ë³€ìˆ˜ ë°œê²¬ ì™„ë£Œ")
        print(f"   PCA ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_[:5]}")
        print(f"   ëˆ„ì  ì„¤ëª… ë¶„ì‚°: {np.cumsum(pca.explained_variance_ratio_)[:5]}")

        return latent_variables

    def _calculate_skewness(self, data: np.ndarray) -> np.ndarray:
        """ì™œë„ ê³„ì‚°"""
        from scipy.stats import skew

        return skew(data, axis=0)

    def _calculate_kurtosis(self, data: np.ndarray) -> np.ndarray:
        """ì²¨ë„ ê³„ì‚°"""
        from scipy.stats import kurtosis

        return kurtosis(data, axis=0)

    def _analyze_distribution(self, data: np.ndarray) -> str:
        """ë¶„í¬ ìœ í˜• ë¶„ì„"""
        if data.shape[1] == 0:
            return "empty"

        # ì²« ë²ˆì§¸ íŠ¹ì„±ì˜ ë¶„í¬ ë¶„ì„
        first_feature = data[:, 0]

        # ì •ê·œì„± ê²€ì • (Shapiro-Wilk)
        from scipy.stats import shapiro

        if len(first_feature) > 5000:
            # í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ìƒ˜í”Œë§
            sample = np.random.choice(first_feature, 5000, replace=False)
        else:
            sample = first_feature

        try:
            stat, p_value = shapiro(sample)
            if p_value > 0.05:
                return "normal"
            else:
                return "non_normal"
        except:
            return "unknown"

    def _calculate_outlier_ratio(self, data: np.ndarray) -> float:
        """ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚° (IQR ë°©ë²•)"""
        if data.shape[1] == 0:
            return 0.0

        outlier_count = 0
        total_count = data.size

        for i in range(data.shape[1]):
            Q1 = np.percentile(data[:, i], 25)
            Q3 = np.percentile(data[:, i], 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = (data[:, i] < lower_bound) | (data[:, i] > upper_bound)
            outlier_count += np.sum(outliers)

        return outlier_count / total_count

    def _calculate_feature_importance(
        self, X: np.ndarray, modality_info: List[str]
    ) -> Dict[str, float]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        if X.shape[1] == 0:
            return {}

        # ë¶„ì‚° ê¸°ë°˜ ì¤‘ìš”ë„
        variances = np.var(X, axis=0)

        # ëª¨ë‹¬ë¦¬í‹°ë³„ í‰ê·  ë¶„ì‚°
        modality_variances = {}
        for modality in set(modality_info):
            modality_indices = [i for i, m in enumerate(modality_info) if m == modality]
            if modality_indices:
                modality_variances[modality] = np.mean(variances[modality_indices])

        return modality_variances

    def run_deep_analysis(self, limit: int = 10000) -> Dict[str, Any]:
        """ì‹¬ì¸µ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ë©€í‹°ëª¨ë‹¬ ì‹¬ì¸µ ë¶„ì„ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ë¡œë”©
        multimodal_data = self.load_multimodal_data(limit)

        # 2. ê° ëª¨ë‹¬ë¦¬í‹° í†µê³„ì  íŠ¹ì„± ë¶„ì„
        modality_stats = self.analyze_modality_statistics(multimodal_data)

        # 3. êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„
        cross_correlations = self.analyze_cross_correlations(multimodal_data)

        # 4. ì ì¬ ê³µí†µ ë³€ìˆ˜ ë°œê²¬
        latent_variables = self.discover_latent_common_variables(multimodal_data)

        # 5. ê²°ê³¼ í†µí•©
        results = {
            "modality_statistics": modality_stats,
            "cross_correlations": cross_correlations,
            "latent_variables": latent_variables,
            "analysis_summary": self._generate_analysis_summary(
                modality_stats, cross_correlations, latent_variables
            ),
        }

        # 6. ê²°ê³¼ ì €ì¥
        try:
            with open("multimodal_deep_analysis_results.json", "w") as f:
                json.dump(self._convert_to_json_serializable(results), f, indent=2)
        except Exception as e:
            print(f"âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°„ë‹¨í•œ ìš”ì•½ë§Œ ì €ì¥
            summary_only = {
                "analysis_summary": results["analysis_summary"],
                "modality_shapes": {
                    name: stats["shape"]
                    for name, stats in results["modality_statistics"].items()
                },
                "correlation_summary": {
                    "strong_correlations": len(
                        [
                            c
                            for c in results["cross_correlations"][
                                "pearson_correlations"
                            ].values()
                            if abs(c["correlation"]) > 0.5
                        ]
                    ),
                    "weak_correlations": len(
                        [
                            c
                            for c in results["cross_correlations"][
                                "pearson_correlations"
                            ].values()
                            if abs(c["correlation"]) < 0.1
                        ]
                    ),
                },
            }
            with open("multimodal_deep_analysis_summary.json", "w") as f:
                json.dump(summary_only, f, indent=2)

        print("âœ… ë©€í‹°ëª¨ë‹¬ ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ!")
        self._print_analysis_summary(results["analysis_summary"])

        return results

    def _generate_analysis_summary(
        self, modality_stats: Dict, cross_correlations: Dict, latent_variables: Dict
    ) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        summary = {
            "total_modalities": len(modality_stats),
            "modality_shapes": {
                name: stats["shape"] for name, stats in modality_stats.items()
            },
            "strong_correlations": [],
            "weak_correlations": [],
            "pca_variance_explained": latent_variables.get("combined_pca", {}).get(
                "explained_variance_ratio", []
            )[:5],
            "key_insights": [],
        }

        # ê°•í•œ ìƒê´€ê´€ê³„ ì‹ë³„
        for pair, stats in cross_correlations.get("pearson_correlations", {}).items():
            if abs(stats["correlation"]) > 0.5:
                summary["strong_correlations"].append(
                    {
                        "pair": pair,
                        "correlation": stats["correlation"],
                        "p_value": stats["p_value"],
                    }
                )
            elif abs(stats["correlation"]) < 0.1:
                summary["weak_correlations"].append(
                    {
                        "pair": pair,
                        "correlation": stats["correlation"],
                        "p_value": stats["p_value"],
                    }
                )

        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        if summary["strong_correlations"]:
            summary["key_insights"].append("ì¼ë¶€ ëª¨ë‹¬ë¦¬í‹° ê°„ ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬")
        else:
            summary["key_insights"].append("ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ê°„ ìƒê´€ê´€ê³„ê°€ ì•½í•¨")

        if latent_variables.get("combined_pca", {}).get("explained_variance_ratio", []):
            first_pc_variance = latent_variables["combined_pca"][
                "explained_variance_ratio"
            ][0]
            if first_pc_variance > 0.3:
                summary["key_insights"].append("ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì´ ë†’ì€ ë¶„ì‚° ì„¤ëª…")
            else:
                summary["key_insights"].append("ì£¼ì„±ë¶„ë“¤ì´ ê³ ë¥´ê²Œ ë¶„ì‚°ë¨")

        return summary

    def _print_analysis_summary(self, summary: Dict[str, Any]):
        """ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"   ì´ ëª¨ë‹¬ë¦¬í‹° ìˆ˜: {summary['total_modalities']}")
        print(f"   ëª¨ë‹¬ë¦¬í‹° í¬ê¸°: {summary['modality_shapes']}")
        print(f"   ê°•í•œ ìƒê´€ê´€ê³„: {len(summary['strong_correlations'])}ê°œ")
        print(f"   ì•½í•œ ìƒê´€ê´€ê³„: {len(summary['weak_correlations'])}ê°œ")
        print(f"   PCA ì„¤ëª… ë¶„ì‚°: {summary['pca_variance_explained']}")
        print("   í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        for insight in summary["key_insights"]:
            print(f"     - {insight}")

    def _convert_to_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´ë¡œ ë³€í™˜"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.float32):
            return float(obj)
        elif isinstance(obj, np.float64):
            return float(obj)
        elif isinstance(obj, np.int32):
            return int(obj)
        elif isinstance(obj, np.int64):
            return int(obj)
        elif isinstance(obj, bool):
            return bool(obj)
        elif isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        else:
            return obj


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ì‹¬ì¸µ ë¶„ì„")
    print("=" * 60)

    analyzer = MultimodalDeepAnalyzer()
    results = analyzer.run_deep_analysis(limit=10000)

    print("\nğŸ“Š ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼:")
    print(f"   ë¶„ì„ ì™„ë£Œ: multimodal_deep_analysis_results.json")
    print(f"   ëª¨ë‹¬ë¦¬í‹° ìˆ˜: {len(results['modality_statistics'])}")
    print(f"   ìƒê´€ê´€ê³„ ë¶„ì„: ì™„ë£Œ")
    print(f"   ì ì¬ ë³€ìˆ˜ ë°œê²¬: ì™„ë£Œ")


if __name__ == "__main__":
    main()
