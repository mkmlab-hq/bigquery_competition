#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ
- ëª¨ë¸ ì••ì¶• ë° ìµœì í™”
- ì¶”ë¡  ì†ë„ í–¥ìƒ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
"""

import json
import multiprocessing as mp
import os
import threading
import time
import warnings
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import psutil
import seaborn as sns
import torch
import torch.jit
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

warnings.filterwarnings("ignore", category=UserWarning)


class OptimizedMultimodalDataset(Dataset):
    """ìµœì í™”ëœ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹"""

    def __init__(
        self,
        big5_data,
        cmi_data,
        rppg_data,
        voice_data,
        targets,
        use_caching=True,
        cache_size=1000,
    ):
        self.big5_data = torch.FloatTensor(big5_data)
        self.cmi_data = torch.FloatTensor(cmi_data)
        self.rppg_data = torch.FloatTensor(rppg_data)
        self.voice_data = torch.FloatTensor(voice_data)
        self.targets = torch.FloatTensor(targets)

        self.use_caching = use_caching
        self.cache_size = cache_size
        self.cache = {}

        # ë©”ëª¨ë¦¬ ìµœì í™”
        if use_caching:
            self._preload_cache()

    def _preload_cache(self):
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ìºì‹œì— ë¯¸ë¦¬ ë¡œë“œ"""
        cache_indices = np.random.choice(
            len(self), min(self.cache_size, len(self)), replace=False
        )

        for idx in cache_indices:
            self.cache[idx] = {
                "big5": self.big5_data[idx],
                "cmi": self.cmi_data[idx],
                "rppg": self.rppg_data[idx],
                "voice": self.voice_data[idx],
                "target": self.targets[idx],
            }

    def __len__(self):
        return len(self.big5_data)

    def __getitem__(self, idx):
        if self.use_caching and idx in self.cache:
            return self.cache[idx]

        return {
            "big5": self.big5_data[idx],
            "cmi": self.cmi_data[idx],
            "rppg": self.rppg_data[idx],
            "voice": self.voice_data[idx],
            "target": self.targets[idx],
        }


class QuantizedMultimodalNet(nn.Module):
    """ì–‘ìí™”ëœ ë©€í‹°ëª¨ë‹¬ ë„¤íŠ¸ì›Œí¬"""

    def __init__(self, big5_dim=25, cmi_dim=10, rppg_dim=15, voice_dim=20):
        super(QuantizedMultimodalNet, self).__init__()

        # ê²½ëŸ‰í™”ëœ ì¸ì½”ë”ë“¤
        self.big5_encoder = nn.Sequential(
            nn.Linear(big5_dim, 32), nn.ReLU(inplace=True), nn.Linear(32, 16)
        )

        self.cmi_encoder = nn.Sequential(
            nn.Linear(cmi_dim, 16), nn.ReLU(inplace=True), nn.Linear(16, 8)
        )

        self.rppg_encoder = nn.Sequential(
            nn.Linear(rppg_dim, 16), nn.ReLU(inplace=True), nn.Linear(16, 8)
        )

        self.voice_encoder = nn.Sequential(
            nn.Linear(voice_dim, 16), nn.ReLU(inplace=True), nn.Linear(16, 8)
        )

        # ìœµí•© ë ˆì´ì–´
        fusion_dim = 16 + 8 + 8 + 8
        self.fusion = nn.Sequential(
            nn.Linear(fusion_dim, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1),
        )

        # ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜
        self.modality_weights = nn.Parameter(torch.ones(4))

    def forward(self, big5, cmi, rppg, voice):
        # ê° ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©
        big5_encoded = self.big5_encoder(big5)
        cmi_encoded = self.cmi_encoder(cmi)
        rppg_encoded = self.rppg_encoder(rppg)
        voice_encoded = self.voice_encoder(voice)

        # ìœµí•©
        fused = torch.cat(
            [big5_encoded, cmi_encoded, rppg_encoded, voice_encoded], dim=1
        )
        output = self.fusion(fused)

        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = torch.softmax(self.modality_weights, dim=0)

        return output, weights


class MultimodalPerformanceOptimizer:
    """ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.scalers = {}
        self.optimized_model = None
        self.performance_metrics = {}

        print(f"ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   CPU ì½”ì–´ ìˆ˜: {mp.cpu_count()}")
        print(f"   ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / (1024**3):.1f}GB")

    def generate_optimization_data(
        self, n_samples: int = 50000
    ) -> Tuple[Dict, np.ndarray]:
        """ìµœì í™”ìš© ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±"""
        print(f"ğŸ“Š ìµœì í™”ìš© ë°ì´í„° ìƒì„± ì¤‘... ({n_samples}ê°œ ìƒ˜í”Œ)")

        np.random.seed(42)

        # Big5 ë°ì´í„°
        big5_data = np.random.normal(3.5, 1.0, (n_samples, 25))
        big5_data = np.clip(big5_data, 1.0, 5.0)

        # CMI ë°ì´í„°
        cmi_data = np.random.normal(50, 15, (n_samples, 10))
        cmi_data = np.clip(cmi_data, 0, 100)

        # RPPG ë°ì´í„°
        rppg_data = np.random.normal(70, 10, (n_samples, 15))
        rppg_data = np.clip(rppg_data, 40, 120)

        # Voice ë°ì´í„°
        voice_data = np.random.normal(200, 50, (n_samples, 20))
        voice_data = np.clip(voice_data, 50, 400)

        # íƒ€ê²Ÿ ìƒì„±
        big5_scores = {
            "EXT": big5_data[:, :5].mean(axis=1),
            "EST": big5_data[:, 5:10].mean(axis=1),
            "AGR": big5_data[:, 10:15].mean(axis=1),
            "CSN": big5_data[:, 15:20].mean(axis=1),
            "OPN": big5_data[:, 20:25].mean(axis=1),
        }

        targets = (
            big5_scores["EXT"] * 0.3
            + big5_scores["OPN"] * 0.25
            + (5 - big5_scores["EST"]) * 0.2
            + big5_scores["AGR"] * 0.15
            + big5_scores["CSN"] * 0.1
            + (cmi_data.mean(axis=1) / 100) * 0.1
            + (rppg_data.mean(axis=1) / 100) * 0.05
            + (voice_data.mean(axis=1) / 300) * 0.05
        )

        targets = (targets - targets.min()) / (targets.max() - targets.min()) * 9 + 1

        multimodal_data = {
            "big5": big5_data,
            "cmi": cmi_data,
            "rppg": rppg_data,
            "voice": voice_data,
        }

        print(f"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ: {n_samples}ê°œ ìƒ˜í”Œ")
        return multimodal_data, targets

    def optimize_data_loading(
        self,
        multimodal_data: Dict,
        targets: np.ndarray,
        batch_size: int = 256,
        num_workers: int = 4,
    ) -> DataLoader:
        """ë°ì´í„° ë¡œë”© ìµœì í™”"""
        print(f"âš¡ ë°ì´í„° ë¡œë”© ìµœì í™” ì¤‘...")

        # ë°ì´í„° ì •ê·œí™”
        for modality, data in multimodal_data.items():
            scaler = StandardScaler()
            multimodal_data[modality] = scaler.fit_transform(data)
            self.scalers[modality] = scaler

        # íƒ€ê²Ÿ ì •ê·œí™”
        target_scaler = StandardScaler()
        targets_scaled = target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        self.scalers["target"] = target_scaler

        # ìµœì í™”ëœ ë°ì´í„°ì…‹ ìƒì„±
        dataset = OptimizedMultimodalDataset(
            multimodal_data["big5"],
            multimodal_data["cmi"],
            multimodal_data["rppg"],
            multimodal_data["voice"],
            targets_scaled,
            use_caching=True,
            cache_size=1000,
        )

        # ìµœì í™”ëœ DataLoader ìƒì„±
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == "cuda" else False,
            persistent_workers=True if num_workers > 0 else False,
        )

        print(f"âœ… ë°ì´í„° ë¡œë”© ìµœì í™” ì™„ë£Œ:")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   ì›Œì»¤ ìˆ˜: {num_workers}")
        print(f"   ìºì‹œ ì‚¬ìš©: True")

        return dataloader

    def train_optimized_model(self, dataloader: DataLoader, epochs: int = 50) -> Dict:
        """ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸš€ ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (Epochs: {epochs})")

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.optimized_model = QuantizedMultimodalNet().to(self.device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.optimized_model.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, "min", patience=5, factor=0.5
        )

        training_history = []
        best_loss = float("inf")

        for epoch in range(epochs):
            start_time = time.time()

            # í›ˆë ¨
            self.optimized_model.train()
            train_loss = 0.0
            train_predictions = []
            train_targets = []

            for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
                big5 = batch["big5"].to(self.device, non_blocking=True)
                cmi = batch["cmi"].to(self.device, non_blocking=True)
                rppg = batch["rppg"].to(self.device, non_blocking=True)
                voice = batch["voice"].to(self.device, non_blocking=True)
                targets = batch["target"].to(self.device, non_blocking=True)

                optimizer.zero_grad()
                outputs, weights = self.optimized_model(big5, cmi, rppg, voice)
                loss = criterion(outputs.squeeze(), targets)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()
                train_predictions.extend(outputs.squeeze().detach().cpu().numpy())
                train_targets.extend(targets.detach().cpu().numpy())

            # í‰ê·  ì†ì‹¤ ê³„ì‚°
            train_loss /= len(dataloader)
            train_rmse = np.sqrt(
                np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)
            )

            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            scheduler.step(train_loss)

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            epoch_time = time.time() - start_time
            training_history.append(
                {
                    "epoch": epoch + 1,
                    "train_loss": train_loss,
                    "train_rmse": train_rmse,
                    "epoch_time": epoch_time,
                    "learning_rate": optimizer.param_groups[0]["lr"],
                }
            )

            # ìµœê³  ëª¨ë¸ ì €ì¥
            if train_loss < best_loss:
                best_loss = train_loss
                torch.save(
                    self.optimized_model.state_dict(), "best_optimized_model.pth"
                )

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (epoch + 1) % 10 == 0:
                print(
                    f"Epoch {epoch+1}: Loss={train_loss:.4f}, RMSE={train_rmse:.4f}, Time={epoch_time:.2f}s"
                )

        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        self.optimized_model.load_state_dict(torch.load("best_optimized_model.pth"))

        print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœê³  ì†ì‹¤: {best_loss:.4f}")

        return {
            "best_loss": best_loss,
            "total_epochs": epochs,
            "training_history": training_history,
        }

    def optimize_inference_speed(self, test_dataloader: DataLoader) -> Dict:
        """ì¶”ë¡  ì†ë„ ìµœì í™”"""
        print("âš¡ ì¶”ë¡  ì†ë„ ìµœì í™” ì¤‘...")

        self.optimized_model.eval()

        # 1. ê¸°ë³¸ ì¶”ë¡  ì†ë„ ì¸¡ì •
        start_time = time.time()
        predictions = []
        targets = []

        with torch.no_grad():
            for batch in test_dataloader:
                big5 = batch["big5"].to(self.device, non_blocking=True)
                cmi = batch["cmi"].to(self.device, non_blocking=True)
                rppg = batch["rppg"].to(self.device, non_blocking=True)
                voice = batch["voice"].to(self.device, non_blocking=True)
                target = batch["target"].to(self.device, non_blocking=True)

                outputs, weights = self.optimized_model(big5, cmi, rppg, voice)
                predictions.extend(outputs.squeeze().cpu().numpy())
                targets.extend(target.cpu().numpy())

        basic_time = time.time() - start_time
        basic_throughput = len(predictions) / basic_time

        # 2. TorchScript ìµœì í™”
        print("   TorchScript ìµœì í™” ì¤‘...")
        self.optimized_model.eval()

        # ì˜ˆì‹œ ì…ë ¥ ìƒì„±
        example_big5 = torch.randn(1, 25).to(self.device)
        example_cmi = torch.randn(1, 10).to(self.device)
        example_rppg = torch.randn(1, 15).to(self.device)
        example_voice = torch.randn(1, 20).to(self.device)

        # TorchScript ëª¨ë¸ ìƒì„±
        traced_model = torch.jit.trace(
            self.optimized_model,
            (example_big5, example_cmi, example_rppg, example_voice),
        )

        # TorchScript ëª¨ë¸ë¡œ ì¶”ë¡  ì†ë„ ì¸¡ì •
        start_time = time.time()
        predictions_ts = []

        with torch.no_grad():
            for batch in test_dataloader:
                big5 = batch["big5"].to(self.device, non_blocking=True)
                cmi = batch["cmi"].to(self.device, non_blocking=True)
                rppg = batch["rppg"].to(self.device, non_blocking=True)
                voice = batch["voice"].to(self.device, non_blocking=True)

                outputs, weights = traced_model(big5, cmi, rppg, voice)
                predictions_ts.extend(outputs.squeeze().cpu().numpy())

        torchscript_time = time.time() - start_time
        torchscript_throughput = len(predictions_ts) / torchscript_time

        # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
        print("   ë°°ì¹˜ í¬ê¸° ìµœì í™” ì¤‘...")
        batch_sizes = [32, 64, 128, 256, 512]
        batch_performance = {}

        for batch_size in batch_sizes:
            test_loader = DataLoader(
                test_dataloader.dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,  # ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹œ ì›Œì»¤ ë¹„í™œì„±í™”
                pin_memory=False,
            )

            start_time = time.time()
            with torch.no_grad():
                for batch in test_loader:
                    big5 = batch["big5"].to(self.device)
                    cmi = batch["cmi"].to(self.device)
                    rppg = batch["rppg"].to(self.device)
                    voice = batch["voice"].to(self.device)

                    outputs, weights = traced_model(big5, cmi, rppg, voice)

            batch_time = time.time() - start_time
            batch_throughput = len(test_loader.dataset) / batch_time

            batch_performance[batch_size] = {
                "time": batch_time,
                "throughput": batch_throughput,
            }

        # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
        optimal_batch_size = max(
            batch_performance.keys(), key=lambda x: batch_performance[x]["throughput"]
        )

        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_usage = psutil.Process().memory_info().rss / (1024**2)  # MB

        optimization_results = {
            "basic_inference": {
                "time": basic_time,
                "throughput": basic_throughput,
                "samples_per_second": basic_throughput,
            },
            "torchscript_inference": {
                "time": torchscript_time,
                "throughput": torchscript_throughput,
                "samples_per_second": torchscript_throughput,
                "speedup": torchscript_throughput / basic_throughput,
            },
            "batch_optimization": {
                "optimal_batch_size": optimal_batch_size,
                "batch_performance": batch_performance,
            },
            "memory_usage_mb": memory_usage,
            "device": str(self.device),
        }

        print(f"âœ… ì¶”ë¡  ì†ë„ ìµœì í™” ì™„ë£Œ:")
        print(f"   ê¸°ë³¸ ì¶”ë¡ : {basic_throughput:.1f} samples/sec")
        print(f"   TorchScript: {torchscript_throughput:.1f} samples/sec")
        print(f"   ì†ë„ í–¥ìƒ: {torchscript_throughput/basic_throughput:.2f}x")
        print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB")

        return optimization_results

    def create_performance_report(
        self,
        training_results: Dict,
        optimization_results: Dict,
        save_path: str = "performance_optimization_report.png",
    ):
        """ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. í›ˆë ¨ íˆìŠ¤í† ë¦¬
        history_df = pd.DataFrame(training_results["training_history"])

        axes[0, 0].plot(history_df["epoch"], history_df["train_loss"])
        axes[0, 0].set_title("Training Loss")
        axes[0, 0].set_xlabel("Epoch")
        axes[0, 0].set_ylabel("Loss")
        axes[0, 0].grid(True)

        axes[0, 1].plot(history_df["epoch"], history_df["train_rmse"])
        axes[0, 1].set_title("Training RMSE")
        axes[0, 1].set_xlabel("Epoch")
        axes[0, 1].set_ylabel("RMSE")
        axes[0, 1].grid(True)

        axes[0, 2].plot(history_df["epoch"], history_df["epoch_time"])
        axes[0, 2].set_title("Epoch Time")
        axes[0, 2].set_xlabel("Epoch")
        axes[0, 2].set_ylabel("Time (s)")
        axes[0, 2].grid(True)

        # 2. ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ
        methods = ["Basic", "TorchScript"]
        throughputs = [
            optimization_results["basic_inference"]["throughput"],
            optimization_results["torchscript_inference"]["throughput"],
        ]

        axes[1, 0].bar(methods, throughputs)
        axes[1, 0].set_title("Inference Throughput Comparison")
        axes[1, 0].set_ylabel("Samples/Second")
        for i, v in enumerate(throughputs):
            axes[1, 0].text(
                i, v + max(throughputs) * 0.01, f"{v:.1f}", ha="center", va="bottom"
            )

        # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
        batch_sizes = list(
            optimization_results["batch_optimization"]["batch_performance"].keys()
        )
        batch_throughputs = [
            optimization_results["batch_optimization"]["batch_performance"][bs][
                "throughput"
            ]
            for bs in batch_sizes
        ]

        axes[1, 1].plot(batch_sizes, batch_throughputs, "o-")
        axes[1, 1].set_title("Batch Size Optimization")
        axes[1, 1].set_xlabel("Batch Size")
        axes[1, 1].set_ylabel("Throughput (samples/sec)")
        axes[1, 1].grid(True)

        # ìµœì  ë°°ì¹˜ í¬ê¸° í‘œì‹œ
        optimal_bs = optimization_results["batch_optimization"]["optimal_batch_size"]
        optimal_throughput = optimization_results["batch_optimization"][
            "batch_performance"
        ][optimal_bs]["throughput"]
        axes[1, 1].axvline(x=optimal_bs, color="red", linestyle="--", alpha=0.7)
        axes[1, 1].text(
            optimal_bs,
            optimal_throughput,
            f"Optimal: {optimal_bs}",
            ha="center",
            va="bottom",
            color="red",
        )

        # 4. ì„±ëŠ¥ ìš”ì•½
        axes[1, 2].axis("off")
        summary_text = f"""
        Performance Optimization Summary
        
        Device: {optimization_results['device']}
        Memory Usage: {optimization_results['memory_usage_mb']:.1f} MB
        
        Inference Performance:
        â€¢ Basic: {optimization_results['basic_inference']['throughput']:.1f} samples/sec
        â€¢ TorchScript: {optimization_results['torchscript_inference']['throughput']:.1f} samples/sec
        â€¢ Speedup: {optimization_results['torchscript_inference']['speedup']:.2f}x
        
        Optimal Configuration:
        â€¢ Batch Size: {optimal_bs}
        â€¢ Best Throughput: {optimal_throughput:.1f} samples/sec
        
        Training Performance:
        â€¢ Total Epochs: {training_results['total_epochs']}
        â€¢ Best Loss: {training_results['best_loss']:.4f}
        â€¢ Avg Epoch Time: {history_df['epoch_time'].mean():.2f}s
        """

        axes[1, 2].text(
            0.1,
            0.9,
            summary_text,
            transform=axes[1, 2].transAxes,
            fontsize=10,
            verticalalignment="top",
            fontfamily="monospace",
        )

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"âœ… ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ ì €ì¥: {save_path}")

    def run_comprehensive_optimization(
        self, n_samples: int = 50000, epochs: int = 50
    ) -> Dict:
        """ì¢…í•© ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ì¢…í•© ì„±ëŠ¥ ìµœì í™” ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ìƒì„±
        multimodal_data, targets = self.generate_optimization_data(n_samples)

        # 2. ë°ì´í„° ë¡œë”© ìµœì í™”
        dataloader = self.optimize_data_loading(multimodal_data, targets)

        # 3. ëª¨ë¸ í›ˆë ¨
        training_results = self.train_optimized_model(dataloader, epochs)

        # 4. ì¶”ë¡  ì†ë„ ìµœì í™”
        optimization_results = self.optimize_inference_speed(dataloader)

        # 5. ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        self.create_performance_report(training_results, optimization_results)

        # 6. ê²°ê³¼ ì €ì¥
        results = {
            "training_results": training_results,
            "optimization_results": optimization_results,
            "system_info": {
                "device": str(self.device),
                "n_samples": n_samples,
                "epochs": epochs,
                "cpu_cores": mp.cpu_count(),
                "memory_gb": psutil.virtual_memory().total / (1024**3),
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("multimodal_optimization_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)

        print(f"\nğŸ‰ ì¢…í•© ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!")
        print(
            f"   ìµœì¢… ì¶”ë¡  ì†ë„: {optimization_results['torchscript_inference']['throughput']:.1f} samples/sec"
        )
        print(
            f"   ì†ë„ í–¥ìƒ: {optimization_results['torchscript_inference']['speedup']:.2f}x"
        )
        print(
            f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimization_results['batch_optimization']['optimal_batch_size']}"
        )

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§  ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 60)

    # ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    optimizer = MultimodalPerformanceOptimizer()

    # ì¢…í•© ìµœì í™” ì‹¤í–‰
    results = optimizer.run_comprehensive_optimization(n_samples=50000, epochs=50)

    print("\nğŸ¯ ìµœì í™” ê²°ê³¼ ìš”ì•½:")
    print(f"   ë””ë°”ì´ìŠ¤: {results['system_info']['device']}")
    print(f"   ìƒ˜í”Œ ìˆ˜: {results['system_info']['n_samples']}")
    print(
        f"   ìµœì¢… ì„±ëŠ¥: {results['optimization_results']['torchscript_inference']['throughput']:.1f} samples/sec"
    )


if __name__ == "__main__":
    main()
