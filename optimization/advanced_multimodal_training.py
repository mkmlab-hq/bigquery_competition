#!/usr/bin/env python3
"""
ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ
- ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ìœµí•©
- ì‹¤ì‹œê°„ í•™ìŠµ ë° ì ì‘
- ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨
"""

import json
import os
import time
import warnings
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

warnings.filterwarnings("ignore", category=UserWarning)


class MultimodalDataset(Dataset):
    """ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""

    def __init__(self, big5_data, cmi_data, rppg_data, voice_data, targets):
        self.big5_data = torch.FloatTensor(big5_data)
        self.cmi_data = torch.FloatTensor(cmi_data)
        self.rppg_data = torch.FloatTensor(rppg_data)
        self.voice_data = torch.FloatTensor(voice_data)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.big5_data)

    def __getitem__(self, idx):
        return {
            "big5": self.big5_data[idx],
            "cmi": self.cmi_data[idx],
            "rppg": self.rppg_data[idx],
            "voice": self.voice_data[idx],
            "target": self.targets[idx],
        }


class MultimodalFusionNet(nn.Module):
    """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹ ê²½ë§ - ê°•í™”ëœ ìœµí•© ì•Œê³ ë¦¬ì¦˜"""

    def __init__(
        self,
        big5_dim=25,
        cmi_dim=10,
        rppg_dim=15,
        voice_dim=20,
        hidden_dim=128,
        output_dim=1,
        dropout_rate=0.3,
        use_transformer=True,
        use_cross_attention=True,
    ):
        super(MultimodalFusionNet, self).__init__()
        
        self.use_transformer = use_transformer
        self.use_cross_attention = use_cross_attention

        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ê³ ê¸‰ ì¸ì½”ë”
        self.big5_encoder = self._create_advanced_encoder(big5_dim, hidden_dim, dropout_rate)
        self.cmi_encoder = self._create_advanced_encoder(cmi_dim, hidden_dim // 2, dropout_rate)
        self.rppg_encoder = self._create_advanced_encoder(rppg_dim, hidden_dim // 2, dropout_rate)
        self.voice_encoder = self._create_advanced_encoder(voice_dim, hidden_dim // 2, dropout_rate)

        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (ëª¨ë‹¬ë¦¬í‹° ê°„ ìƒí˜¸ì‘ìš©)
        if use_cross_attention:
            self.cross_attention = nn.MultiheadAttention(
                embed_dim=hidden_dim // 2,
                num_heads=8,
                dropout=dropout_rate,
                batch_first=True,
            )
            
            # ëª¨ë‹¬ë¦¬í‹°ë³„ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜ ë³€í™˜
            self.query_transform = nn.Linear(hidden_dim // 2, hidden_dim // 2)
            self.key_transform = nn.Linear(hidden_dim // 4, hidden_dim // 2)
            self.value_transform = nn.Linear(hidden_dim // 4, hidden_dim // 2)

        # íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ìœµí•© (ì„ íƒì )
        if use_transformer:
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=hidden_dim // 2,
                nhead=8,
                dim_feedforward=hidden_dim,
                dropout=dropout_rate,
                batch_first=True
            )
            self.transformer_fusion = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # ì ì‘í˜• ìœµí•© ë ˆì´ì–´
        fusion_input_dim = (hidden_dim // 2) + (hidden_dim // 4) * 3
        self.adaptive_fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, output_dim),
        )

        # ë™ì  ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜ í•™ìŠµ
        self.modality_weights = nn.Parameter(torch.ones(4))
        self.weight_gate = nn.Sequential(
            nn.Linear(hidden_dim // 2, 16),
            nn.ReLU(),
            nn.Linear(16, 4),
            nn.Softmax(dim=-1)
        )

        # ëª¨ë‹¬ë¦¬í‹°ë³„ ì¤‘ìš”ë„ í•™ìŠµ
        self.importance_networks = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim // 2 if i == 0 else hidden_dim // 4, 32),
                nn.ReLU(),
                nn.Linear(32, 1),
                nn.Sigmoid()
            ) for i in range(4)
        ])

    def _create_advanced_encoder(self, input_dim: int, output_dim: int, dropout_rate: float) -> nn.Module:
        """ê³ ê¸‰ ì¸ì½”ë” ìƒì„±"""
        return nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
        )

    def forward(self, big5, cmi, rppg, voice):
        """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ìœµí•© forward pass"""
        # ê° ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©
        big5_encoded = self.big5_encoder(big5)
        cmi_encoded = self.cmi_encoder(cmi)
        rppg_encoded = self.rppg_encoder(rppg)
        voice_encoded = self.voice_encoder(voice)

        # ëª¨ë‹¬ë¦¬í‹°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        importance_scores = []
        modalities = [big5_encoded, cmi_encoded, rppg_encoded, voice_encoded]
        
        for i, modality in enumerate(modalities):
            importance = self.importance_networks[i](modality)
            importance_scores.append(importance)

        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        dynamic_weights = self.weight_gate(big5_encoded)  # Big5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ì ìš© (ëª¨ë‹¬ë¦¬í‹° ê°„ ìƒí˜¸ì‘ìš©)
        if self.use_cross_attention:
            # Big5ë¥¼ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì— ì–´í…ì…˜
            query = self.query_transform(big5_encoded).unsqueeze(1)
            
            # ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë“¤ì„ í‚¤/ë°¸ë¥˜ë¡œ ë³€í™˜
            keys_values = []
            for modality in [cmi_encoded, rppg_encoded, voice_encoded]:
                key = self.key_transform(modality).unsqueeze(1)
                value = self.value_transform(modality).unsqueeze(1)
                keys_values.append(torch.cat([key, value], dim=-1))
            
            key_value = torch.cat(keys_values, dim=1)
            
            attended_features, cross_attention_weights = self.cross_attention(
                query, key_value, key_value
            )
            attended_features = attended_features.squeeze(1)
        else:
            attended_features = big5_encoded
            cross_attention_weights = None

        # íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ìœµí•© (ì„ íƒì )
        if self.use_transformer:
            # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ê²°í•©
            modality_sequence = torch.stack([
                big5_encoded, cmi_encoded, rppg_encoded, voice_encoded
            ], dim=1)
            
            # íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ìœµí•©
            transformer_output = self.transformer_fusion(modality_sequence)
            
            # í‰ê·  í’€ë§ìœ¼ë¡œ ìµœì¢… í‘œí˜„ ìƒì„±
            transformer_fused = transformer_output.mean(dim=1)
        else:
            transformer_fused = big5_encoded

        # ì ì‘í˜• ìœµí•©
        # 1. ê¸°ë³¸ ìœµí•© (ë‹¨ìˆœ ì—°ê²°)
        basic_fused = torch.cat([
            big5_encoded, cmi_encoded, rppg_encoded, voice_encoded
        ], dim=1)
        
        # 2. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœµí•©
        weighted_fused = (
            dynamic_weights[:, 0:1] * big5_encoded +
            dynamic_weights[:, 1:2] * cmi_encoded +
            dynamic_weights[:, 2:3] * rppg_encoded +
            dynamic_weights[:, 3:4] * voice_encoded
        )
        
        # 3. ì¤‘ìš”ë„ ê¸°ë°˜ ìœµí•©
        importance_weighted = (
            importance_scores[0] * big5_encoded +
            importance_scores[1] * cmi_encoded +
            importance_scores[2] * rppg_encoded +
            importance_scores[3] * voice_encoded
        )

        # ìµœì¢… ìœµí•© (ì—¬ëŸ¬ ìœµí•© ë°©ë²• ê²°í•©)
        final_fused = torch.cat([
            basic_fused,
            weighted_fused,
            importance_weighted,
            attended_features,
            transformer_fused
        ], dim=1)

        # ì ì‘í˜• ìœµí•© ë ˆì´ì–´ë¡œ ìµœì¢… ì˜ˆì¸¡
        output = self.adaptive_fusion(final_fused)

        # ë°˜í™˜ê°’ êµ¬ì„±
        attention_info = {
            'cross_attention_weights': cross_attention_weights,
            'dynamic_weights': dynamic_weights,
            'importance_scores': importance_scores,
            'modality_weights': torch.softmax(self.modality_weights, dim=0)
        }

        return output, attention_info, dynamic_weights


class AdvancedMultimodalTrainer:
    """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.scalers = {}
        self.model = None
        self.training_history = []

        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

    def generate_synthetic_multimodal_data(
        self, n_samples: int = 10000
    ) -> Tuple[Dict, np.ndarray]:
        """í•©ì„± ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ìƒì„±"""
        print(f"ğŸ“Š í•©ì„± ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ìƒì„± ì¤‘... ({n_samples}ê°œ ìƒ˜í”Œ)")

        np.random.seed(42)

        # Big5 ë°ì´í„° (25ê°œ íŠ¹ì„±)
        big5_data = np.random.normal(3.5, 1.0, (n_samples, 25))
        big5_data = np.clip(big5_data, 1.0, 5.0)

        # CMI ë°ì´í„° (10ê°œ íŠ¹ì„±)
        cmi_data = np.random.normal(50, 15, (n_samples, 10))
        cmi_data = np.clip(cmi_data, 0, 100)

        # RPPG ë°ì´í„° (15ê°œ íŠ¹ì„±)
        rppg_data = np.random.normal(70, 10, (n_samples, 15))
        rppg_data = np.clip(rppg_data, 40, 120)

        # Voice ë°ì´í„° (20ê°œ íŠ¹ì„±)
        voice_data = np.random.normal(200, 50, (n_samples, 20))
        voice_data = np.clip(voice_data, 50, 400)

        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë§Œì¡±ë„ ì ìˆ˜)
        # Big5ì˜ EXT, OPNì´ ë†’ê³  ESTê°€ ë‚®ì„ìˆ˜ë¡ ë§Œì¡±ë„ ë†’ìŒ
        big5_scores = {
            "EXT": big5_data[:, :5].mean(axis=1),
            "EST": big5_data[:, 5:10].mean(axis=1),
            "AGR": big5_data[:, 10:15].mean(axis=1),
            "CSN": big5_data[:, 15:20].mean(axis=1),
            "OPN": big5_data[:, 20:25].mean(axis=1),
        }

        # ë³µí•© íƒ€ê²Ÿ ë³€ìˆ˜
        targets = (
            big5_scores["EXT"] * 0.3
            + big5_scores["OPN"] * 0.25
            + (5 - big5_scores["EST"]) * 0.2
            + big5_scores["AGR"] * 0.15
            + big5_scores["CSN"] * 0.1
            + (cmi_data.mean(axis=1) / 100) * 0.1
            + (rppg_data.mean(axis=1) / 100) * 0.05
            + (voice_data.mean(axis=1) / 300) * 0.05
        )

        # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        targets = (targets - targets.min()) / (targets.max() - targets.min()) * 9 + 1

        multimodal_data = {
            "big5": big5_data,
            "cmi": cmi_data,
            "rppg": rppg_data,
            "voice": voice_data,
        }

        print(f"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ:")
        print(f"   Big5: {big5_data.shape}")
        print(f"   CMI: {cmi_data.shape}")
        print(f"   RPPG: {rppg_data.shape}")
        print(f"   Voice: {voice_data.shape}")
        print(f"   Targets: {targets.shape}")

        return multimodal_data, targets

    def prepare_data(
        self,
        multimodal_data: Dict,
        targets: np.ndarray,
        test_size: float = 0.2,
        val_size: float = 0.1,
    ) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° DataLoader ìƒì„±"""
        print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

        # ë°ì´í„° ì •ê·œí™”
        for modality, data in multimodal_data.items():
            scaler = StandardScaler()
            multimodal_data[modality] = scaler.fit_transform(data)
            self.scalers[modality] = scaler

        # íƒ€ê²Ÿ ì •ê·œí™”
        target_scaler = StandardScaler()
        targets_scaled = target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        self.scalers["target"] = target_scaler

        # ë°ì´í„° ë¶„í• 
        X_temp, X_test, y_temp, y_test = train_test_split(
            multimodal_data, targets_scaled, test_size=test_size, random_state=42
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size / (1 - test_size), random_state=42
        )

        # DataLoader ìƒì„±
        train_dataset = MultimodalDataset(
            X_train["big5"], X_train["cmi"], X_train["rppg"], X_train["voice"], y_train
        )
        val_dataset = MultimodalDataset(
            X_val["big5"], X_val["cmi"], X_val["rppg"], X_val["voice"], y_val
        )
        test_dataset = MultimodalDataset(
            X_test["big5"], X_test["cmi"], X_test["rppg"], X_test["voice"], y_test
        )

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"   í›ˆë ¨: {len(train_dataset)}ê°œ")
        print(f"   ê²€ì¦: {len(val_dataset)}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")

        return train_loader, val_loader, test_loader

    def train_model(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int = 100,
        learning_rate: float = 0.001,
    ) -> Dict:
        """ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (Epochs: {epochs})")

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = MultimodalFusionNet().to(self.device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, "min", patience=10, factor=0.5
        )

        best_val_loss = float("inf")
        patience_counter = 0
        early_stopping_patience = 20

        for epoch in range(epochs):
            # í›ˆë ¨
            self.model.train()
            train_loss = 0.0
            train_predictions = []
            train_targets = []

                for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                    big5 = batch["big5"].to(self.device)
                    cmi = batch["cmi"].to(self.device)
                    rppg = batch["rppg"].to(self.device)
                    voice = batch["voice"].to(self.device)
                    targets = batch["target"].to(self.device)

                    optimizer.zero_grad()
                    outputs, attention_info, dynamic_weights = self.model(
                        big5, cmi, rppg, voice
                    )
                    
                    # ê¸°ë³¸ ì†ì‹¤
                    basic_loss = criterion(outputs.squeeze(), targets)
                    
                    # ì •ê·œí™” ì†ì‹¤ (ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜ ê· í˜•)
                    modality_weights = attention_info['modality_weights']
                    weight_entropy = -torch.sum(modality_weights * torch.log(modality_weights + 1e-8))
                    weight_regularization = 0.01 * weight_entropy
                    
                    # ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ ì •ê·œí™”
                    importance_scores = attention_info['importance_scores']
                    importance_regularization = 0.001 * sum(torch.mean(score) for score in importance_scores)
                    
                    # ì´ ì†ì‹¤
                    total_loss = basic_loss + weight_regularization + importance_regularization
                    
                    total_loss.backward()
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()

                    train_loss += total_loss.item()
                    train_predictions.extend(outputs.squeeze().detach().cpu().numpy())
                    train_targets.extend(targets.detach().cpu().numpy())

            # ê²€ì¦
            self.model.eval()
            val_loss = 0.0
            val_predictions = []
            val_targets = []

                with torch.no_grad():
                    for batch in val_loader:
                        big5 = batch["big5"].to(self.device)
                        cmi = batch["cmi"].to(self.device)
                        rppg = batch["rppg"].to(self.device)
                        voice = batch["voice"].to(self.device)
                        targets = batch["target"].to(self.device)

                        outputs, attention_info, dynamic_weights = self.model(big5, cmi, rppg, voice)
                        loss = criterion(outputs.squeeze(), targets)
                        val_loss += loss.item()

                        val_predictions.extend(outputs.squeeze().cpu().numpy())
                        val_targets.extend(targets.cpu().numpy())

            # í‰ê·  ì†ì‹¤ ê³„ì‚°
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)

            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            scheduler.step(val_loss)

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            train_rmse = np.sqrt(
                np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)
            )
            val_rmse = np.sqrt(
                np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)
            )

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.training_history.append(
                {
                    "epoch": epoch + 1,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "train_rmse": train_rmse,
                    "val_rmse": val_rmse,
                    "learning_rate": optimizer.param_groups[0]["lr"],
                }
            )

            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                torch.save(self.model.state_dict(), "best_multimodal_model.pth")
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                print(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ (Epoch {epoch+1})")
                break

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (epoch + 1) % 10 == 0:
                print(
                    f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}"
                )
                print(f"  Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}")
                print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")

        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        self.model.load_state_dict(torch.load("best_multimodal_model.pth"))

        print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")

        return {
            "best_val_loss": best_val_loss,
            "total_epochs": epoch + 1,
            "training_history": self.training_history,
        }

    def evaluate_model(self, test_loader: DataLoader) -> Dict:
        """ëª¨ë¸ í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")

        self.model.eval()
        test_predictions = []
        test_targets = []
        attention_weights_list = []
        modality_weights_list = []

            with torch.no_grad():
                for batch in test_loader:
                    big5 = batch["big5"].to(self.device)
                    cmi = batch["cmi"].to(self.device)
                    rppg = batch["rppg"].to(self.device)
                    voice = batch["voice"].to(self.device)
                    targets = batch["target"].to(self.device)

                    outputs, attention_info, dynamic_weights = self.model(
                        big5, cmi, rppg, voice
                    )

                    test_predictions.extend(outputs.squeeze().cpu().numpy())
                    test_targets.extend(targets.cpu().numpy())
                    
                    # ì–´í…ì…˜ ì •ë³´ ì €ì¥
                    if attention_info['cross_attention_weights'] is not None:
                        attention_weights_list.append(attention_info['cross_attention_weights'].cpu().numpy())
                    modality_weights_list.append(dynamic_weights.cpu().numpy())

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        test_predictions = np.array(test_predictions)
        test_targets = np.array(test_targets)

        mse = np.mean((test_predictions - test_targets) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(test_predictions - test_targets))
        r2 = 1 - (
            np.sum((test_targets - test_predictions) ** 2)
            / np.sum((test_targets - np.mean(test_targets)) ** 2)
        )

        # ìƒê´€ê³„ìˆ˜
        correlation = np.corrcoef(test_predictions, test_targets)[0, 1]

        # ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜ ë¶„ì„
        avg_modality_weights = np.mean(modality_weights_list, axis=0)

        evaluation_results = {
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "r2": r2,
            "correlation": correlation,
            "modality_weights": {
                "big5": float(avg_modality_weights[0]),
                "cmi": float(avg_modality_weights[1]),
                "rppg": float(avg_modality_weights[2]),
                "voice": float(avg_modality_weights[3]),
            },
            "predictions": test_predictions,
            "targets": test_targets,
        }

        print(f"âœ… í‰ê°€ ì™„ë£Œ:")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return evaluation_results

    def create_visualizations(
        self, evaluation_results: Dict, save_dir: str = "multimodal_results"
    ):
        """ì‹œê°í™” ìƒì„±"""
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

        os.makedirs(save_dir, exist_ok=True)

        # 1. í›ˆë ¨ íˆìŠ¤í† ë¦¬
        history_df = pd.DataFrame(self.training_history)

        plt.figure(figsize=(15, 10))

        # ì†ì‹¤ ê·¸ë˜í”„
        plt.subplot(2, 3, 1)
        plt.plot(history_df["epoch"], history_df["train_loss"], label="Train Loss")
        plt.plot(history_df["epoch"], history_df["val_loss"], label="Validation Loss")
        plt.title("Training History - Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

        # RMSE ê·¸ë˜í”„
        plt.subplot(2, 3, 2)
        plt.plot(history_df["epoch"], history_df["train_rmse"], label="Train RMSE")
        plt.plot(history_df["epoch"], history_df["val_rmse"], label="Validation RMSE")
        plt.title("Training History - RMSE")
        plt.xlabel("Epoch")
        plt.ylabel("RMSE")
        plt.legend()
        plt.grid(True)

        # í•™ìŠµë¥  ê·¸ë˜í”„
        plt.subplot(2, 3, 3)
        plt.plot(history_df["epoch"], history_df["learning_rate"])
        plt.title("Learning Rate Schedule")
        plt.xlabel("Epoch")
        plt.ylabel("Learning Rate")
        plt.yscale("log")
        plt.grid(True)

        # ì˜ˆì¸¡ vs ì‹¤ì œ
        plt.subplot(2, 3, 4)
        plt.scatter(
            evaluation_results["targets"], evaluation_results["predictions"], alpha=0.6
        )
        plt.plot(
            [evaluation_results["targets"].min(), evaluation_results["targets"].max()],
            [evaluation_results["targets"].min(), evaluation_results["targets"].max()],
            "r--",
        )
        plt.title(f'Predictions vs Targets (RÂ² = {evaluation_results["r2"]:.3f})')
        plt.xlabel("Actual")
        plt.ylabel("Predicted")
        plt.grid(True)

        # ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜
        plt.subplot(2, 3, 5)
        modalities = list(evaluation_results["modality_weights"].keys())
        weights = list(evaluation_results["modality_weights"].values())
        plt.bar(modalities, weights)
        plt.title("Modality Weights")
        plt.ylabel("Weight")
        plt.xticks(rotation=45)

        # ì”ì°¨ ë¶„í¬
        plt.subplot(2, 3, 6)
        residuals = evaluation_results["predictions"] - evaluation_results["targets"]
        plt.hist(residuals, bins=30, alpha=0.7)
        plt.title("Residual Distribution")
        plt.xlabel("Residuals")
        plt.ylabel("Frequency")
        plt.grid(True)

        plt.tight_layout()
        plt.savefig(
            f"{save_dir}/multimodal_training_analysis.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        print(f"âœ… ì‹œê°í™” ì €ì¥: {save_dir}/multimodal_training_analysis.png")

    def run_comprehensive_training(
        self, n_samples: int = 10000, epochs: int = 100
    ) -> Dict:
        """ì¢…í•© í›ˆë ¨ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ìƒì„±
        multimodal_data, targets = self.generate_synthetic_multimodal_data(n_samples)

        # 2. ë°ì´í„° ì¤€ë¹„
        train_loader, val_loader, test_loader = self.prepare_data(
            multimodal_data, targets
        )

        # 3. ëª¨ë¸ í›ˆë ¨
        training_results = self.train_model(train_loader, val_loader, epochs)

        # 4. ëª¨ë¸ í‰ê°€
        evaluation_results = self.evaluate_model(test_loader)

        # 5. ì‹œê°í™” ìƒì„±
        self.create_visualizations(evaluation_results)

        # 6. ê²°ê³¼ ì €ì¥
        results = {
            "training_results": training_results,
            "evaluation_results": evaluation_results,
            "model_info": {
                "device": str(self.device),
                "n_samples": n_samples,
                "epochs_trained": training_results["total_epochs"],
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("multimodal_training_results.json", "w") as f:
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            json_results = {}
            for key, value in results.items():
                if isinstance(value, dict):
                    json_results[key] = {}
                    for k, v in value.items():
                        if isinstance(v, np.ndarray):
                            json_results[key][k] = v.tolist()
                        else:
                            json_results[key][k] = v
                else:
                    json_results[key] = value
            json.dump(json_results, f, indent=2)

        print(f"\nğŸ‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ìµœì¢… RÂ²: {evaluation_results['r2']:.4f}")
        print(f"   ìµœì¢… RMSE: {evaluation_results['rmse']:.4f}")
        print(f"   ëª¨ë‹¬ë¦¬í‹° ê°€ì¤‘ì¹˜: {evaluation_results['modality_weights']}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§  ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ")
    print("=" * 60)

    # í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    trainer = AdvancedMultimodalTrainer()

    # ì¢…í•© í›ˆë ¨ ì‹¤í–‰
    results = trainer.run_comprehensive_training(n_samples=10000, epochs=100)

    print("\nğŸ¯ í›ˆë ¨ ê²°ê³¼ ìš”ì•½:")
    print(f"   ë””ë°”ì´ìŠ¤: {results['model_info']['device']}")
    print(f"   ìƒ˜í”Œ ìˆ˜: {results['model_info']['n_samples']}")
    print(f"   í›ˆë ¨ ì—í¬í¬: {results['model_info']['epochs_trained']}")
    print(f"   ìµœì¢… ì„±ëŠ¥: RÂ² = {results['evaluation_results']['r2']:.4f}")


if __name__ == "__main__":
    main()
