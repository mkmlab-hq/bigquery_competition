#!/usr/bin/env python3
"""
ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ - BigQuery ì‹¤ì œ ë°ì´í„° ìµœì í™”
- ì‹¤ì œ BigQuery ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨
- RÂ² = 0.25 â†’ 0.50+ ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ
- ê³¼ì í•© ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ ìµœì í™”
"""

import json
import os
import time
import warnings
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
import torch
import torch.nn as nn
import torch.optim as optim
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    mean_squared_error,
    r2_score,
    roc_auc_score,
)
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.svm import SVR
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class BigQueryDataLoader:
    """BigQuery ì‹¤ì œ ë°ì´í„° ë¡œë”"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            print("ëŒ€ì²´ ë°ì´í„° ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.client = None

    def load_competition_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ëŒ€íšŒ ë°ì´í„° ë¡œë”©"""
        if self.client is None:
            print("BigQuery í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_improved_fallback_data(limit)

        print(f"ğŸ” BigQueryì—ì„œ ì‹¤ì œ ë°ì´í„° ë¡œë”© ì¤‘... (ì œí•œ: {limit}ê°œ)")

        # Big5 ë°ì´í„° ì¿¼ë¦¬
        big5_query = f"""
        SELECT 
            user_id,
            EXT_1, EXT_2, EXT_3, EXT_4, EXT_5,
            EST_1, EST_2, EST_3, EST_4, EST_5,
            AGR_1, AGR_2, AGR_3, AGR_4, AGR_5,
            CSN_1, CSN_2, CSN_3, CSN_4, CSN_5,
            OPN_1, OPN_2, OPN_3, OPN_4, OPN_5
        FROM `{self.project_id}.bigquery_competition.big5_data`
        LIMIT {limit}
        """

        # CMI ë°ì´í„° ì¿¼ë¦¬
        cmi_query = f"""
        SELECT 
            user_id,
            cmi_1, cmi_2, cmi_3, cmi_4, cmi_5,
            cmi_6, cmi_7, cmi_8, cmi_9, cmi_10
        FROM `{self.project_id}.bigquery_competition.cmi_data`
        LIMIT {limit}
        """

        # RPPG ë°ì´í„° ì¿¼ë¦¬
        rppg_query = f"""
        SELECT 
            user_id,
            rppg_1, rppg_2, rppg_3, rppg_4, rppg_5,
            rppg_6, rppg_7, rppg_8, rppg_9, rppg_10,
            rppg_11, rppg_12, rppg_13, rppg_14, rppg_15
        FROM `{self.project_id}.bigquery_competition.rppg_data`
        LIMIT {limit}
        """

        # Voice ë°ì´í„° ì¿¼ë¦¬
        voice_query = f"""
        SELECT 
            user_id,
            voice_1, voice_2, voice_3, voice_4, voice_5,
            voice_6, voice_7, voice_8, voice_9, voice_10,
            voice_11, voice_12, voice_13, voice_14, voice_15,
            voice_16, voice_17, voice_18, voice_19, voice_20
        FROM `{self.project_id}.bigquery_competition.voice_data`
        LIMIT {limit}
        """

        # íƒ€ê²Ÿ ë°ì´í„° ì¿¼ë¦¬
        target_query = f"""
        SELECT 
            user_id,
            target_value
        FROM `{self.project_id}.bigquery_competition.target_data`
        LIMIT {limit}
        """

        try:
            # ë°ì´í„° ë¡œë”©
            print("ğŸ“Š Big5 ë°ì´í„° ë¡œë”© ì¤‘...")
            big5_df = self.client.query(big5_query).to_dataframe()
            print(f"   Big5 ë°ì´í„°: {big5_df.shape}")

            print("ğŸ“Š CMI ë°ì´í„° ë¡œë”© ì¤‘...")
            cmi_df = self.client.query(cmi_query).to_dataframe()
            print(f"   CMI ë°ì´í„°: {cmi_df.shape}")

            print("ğŸ“Š RPPG ë°ì´í„° ë¡œë”© ì¤‘...")
            rppg_df = self.client.query(rppg_query).to_dataframe()
            print(f"   RPPG ë°ì´í„°: {rppg_df.shape}")

            print("ğŸ“Š Voice ë°ì´í„° ë¡œë”© ì¤‘...")
            voice_df = self.client.query(voice_query).to_dataframe()
            print(f"   Voice ë°ì´í„°: {voice_df.shape}")

            print("ğŸ“Š íƒ€ê²Ÿ ë°ì´í„° ë¡œë”© ì¤‘...")
            target_df = self.client.query(target_query).to_dataframe()
            print(f"   íƒ€ê²Ÿ ë°ì´í„°: {target_df.shape}")

            # ë°ì´í„° ë³‘í•©
            print("ğŸ”„ ë°ì´í„° ë³‘í•© ì¤‘...")
            merged_df = big5_df.merge(cmi_df, on="user_id", how="inner")
            merged_df = merged_df.merge(rppg_df, on="user_id", how="inner")
            merged_df = merged_df.merge(voice_df, on="user_id", how="inner")
            merged_df = merged_df.merge(target_df, on="user_id", how="inner")

            print(f"   ë³‘í•©ëœ ë°ì´í„°: {merged_df.shape}")

            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            print("ğŸ§¹ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
            merged_df = merged_df.dropna()
            print(f"   ê²°ì¸¡ê°’ ì œê±° í›„: {merged_df.shape}")

            # ë°ì´í„° ë¶„ë¦¬
            big5_cols = [
                col
                for col in merged_df.columns
                if col.startswith(("EXT_", "EST_", "AGR_", "CSN_", "OPN_"))
            ]
            cmi_cols = [col for col in merged_df.columns if col.startswith("cmi_")]
            rppg_cols = [col for col in merged_df.columns if col.startswith("rppg_")]
            voice_cols = [col for col in merged_df.columns if col.startswith("voice_")]

            multimodal_data = {
                "big5": merged_df[big5_cols].values,
                "cmi": merged_df[cmi_cols].values,
                "rppg": merged_df[rppg_cols].values,
                "voice": merged_df[voice_cols].values,
            }

            targets = merged_df["target_value"].values

            print("âœ… ì‹¤ì œ ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
            print(f"   Big5: {multimodal_data['big5'].shape}")
            print(f"   CMI: {multimodal_data['cmi'].shape}")
            print(f"   RPPG: {multimodal_data['rppg'].shape}")
            print(f"   Voice: {multimodal_data['voice'].shape}")
            print(f"   Targets: {targets.shape}")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            print("ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_improved_fallback_data(limit)

    def _generate_improved_fallback_data(self, limit: int) -> Tuple[Dict, np.ndarray]:
        """ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° íŒ¨í„´ ëª¨ë°©)"""
        print("ğŸ”„ ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„° ìƒì„± ì¤‘...")

        np.random.seed(42)

        # ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ BigQuery ë°ì´í„° íŒ¨í„´ ëª¨ë°©)
        big5_data = np.random.normal(3.2, 1.2, (limit, 25))
        big5_data = np.clip(big5_data, 1.0, 5.0)

        cmi_data = np.random.normal(55, 20, (limit, 10))
        cmi_data = np.clip(cmi_data, 0, 100)

        rppg_data = np.random.normal(72, 15, (limit, 15))
        rppg_data = np.clip(rppg_data, 45, 110)

        voice_data = np.random.normal(220, 60, (limit, 20))
        voice_data = np.clip(voice_data, 60, 450)

        # ë” ë³µì¡í•˜ê³  í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        big5_scores = {
            "EXT": big5_data[:, :5].mean(axis=1),
            "EST": big5_data[:, 5:10].mean(axis=1),
            "AGR": big5_data[:, 10:15].mean(axis=1),
            "CSN": big5_data[:, 15:20].mean(axis=1),
            "OPN": big5_data[:, 20:25].mean(axis=1),
        }

        # ë³µí•© íƒ€ê²Ÿ ë³€ìˆ˜ (ë” ë³µì¡í•œ ìƒí˜¸ì‘ìš©)
        targets = (
            big5_scores["EXT"] * 0.25
            + big5_scores["OPN"] * 0.20
            + (5 - big5_scores["EST"]) * 0.18
            + big5_scores["AGR"] * 0.15
            + big5_scores["CSN"] * 0.12
            + (cmi_data.mean(axis=1) / 100) * 0.05
            + (rppg_data.mean(axis=1) / 100) * 0.03
            + (voice_data.mean(axis=1) / 300) * 0.02
            + np.random.normal(0, 0.15, limit)  # ë…¸ì´ì¦ˆ ì¶”ê°€
        )

        # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        targets = (targets - targets.min()) / (targets.max() - targets.min()) * 9 + 1

        multimodal_data = {
            "big5": big5_data,
            "cmi": cmi_data,
            "rppg": rppg_data,
            "voice": voice_data,
        }

        print(f"âœ… ëŒ€ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
        print(f"   Big5: {big5_data.shape}")
        print(f"   CMI: {cmi_data.shape}")
        print(f"   RPPG: {rppg_data.shape}")
        print(f"   Voice: {voice_data.shape}")
        print(f"   Targets: {targets.shape}")

        return multimodal_data, targets


class MultimodalDataset(Dataset):
    """ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""

    def __init__(self, big5_data, cmi_data, rppg_data, voice_data, targets):
        self.big5_data = torch.FloatTensor(big5_data)
        self.cmi_data = torch.FloatTensor(cmi_data)
        self.rppg_data = torch.FloatTensor(rppg_data)
        self.voice_data = torch.FloatTensor(voice_data)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.big5_data)

    def __getitem__(self, idx):
        return {
            "big5": self.big5_data[idx],
            "cmi": self.cmi_data[idx],
            "rppg": self.rppg_data[idx],
            "voice": self.voice_data[idx],
            "target": self.targets[idx],
        }


class OptimizedMultimodalNet(nn.Module):
    """ìµœì í™”ëœ ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹ ê²½ë§ - ì„±ëŠ¥ í–¥ìƒ ë²„ì „"""

    def __init__(
        self,
        big5_dim=25,
        cmi_dim=10,
        rppg_dim=15,
        voice_dim=20,
        hidden_dim=128,
        output_dim=1,
        dropout_rate=0.3,
        use_transformer=False,
        use_cross_attention=False,
        weight_decay=1e-4,
    ):
        super(OptimizedMultimodalNet, self).__init__()

        self.use_transformer = use_transformer
        self.use_cross_attention = use_cross_attention
        self.weight_decay = weight_decay

        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ìµœì í™”ëœ ì¸ì½”ë”
        self.big5_encoder = self._create_optimized_encoder(
            big5_dim, hidden_dim, dropout_rate
        )
        self.cmi_encoder = self._create_optimized_encoder(
            cmi_dim, hidden_dim, dropout_rate
        )
        self.rppg_encoder = self._create_optimized_encoder(
            rppg_dim, hidden_dim, dropout_rate
        )
        self.voice_encoder = self._create_optimized_encoder(
            voice_dim, hidden_dim, dropout_rate
        )

        # ì ì‘í˜• ìœµí•© ë ˆì´ì–´ (ê³¼ì í•© ë°©ì§€)
        fusion_input_dim = 4 * hidden_dim
        self.adaptive_fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, output_dim),
        )

        # ëª¨ë‹¬ë¦¬í‹°ë³„ ì¤‘ìš”ë„ í•™ìŠµ
        self.importance_networks = nn.ModuleList(
            [
                nn.Sequential(
                    nn.Linear(hidden_dim, 64),
                    nn.ReLU(),
                    nn.Dropout(dropout_rate),
                    nn.Linear(64, 1),
                    nn.Sigmoid(),
                )
                for _ in range(4)
            ]
        )

        # ë™ì  ê°€ì¤‘ì¹˜ í•™ìŠµ
        self.weight_gate = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(32, 4),
            nn.Softmax(dim=-1),
        )

    def _create_optimized_encoder(
        self, input_dim: int, output_dim: int, dropout_rate: float
    ) -> nn.Module:
        """ìµœì í™”ëœ ì¸ì½”ë” ìƒì„± (ê³¼ì í•© ë°©ì§€)"""
        return nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(output_dim, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
        )

    def forward(self, big5, cmi, rppg, voice):
        """ìµœì í™”ëœ ë©€í‹°ëª¨ë‹¬ ìœµí•© forward pass"""
        # ê° ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©
        big5_encoded = self.big5_encoder(big5)
        cmi_encoded = self.cmi_encoder(cmi)
        rppg_encoded = self.rppg_encoder(rppg)
        voice_encoded = self.voice_encoder(voice)

        # ëª¨ë‹¬ë¦¬í‹°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        importance_scores = []
        modalities = [big5_encoded, cmi_encoded, rppg_encoded, voice_encoded]

        for i, modality in enumerate(modalities):
            importance = self.importance_networks[i](modality)
            importance_scores.append(importance)

        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        dynamic_weights = self.weight_gate(big5_encoded)

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœµí•©
        weighted_fused = (
            dynamic_weights[:, 0:1] * big5_encoded
            + dynamic_weights[:, 1:2] * cmi_encoded
            + dynamic_weights[:, 2:3] * rppg_encoded
            + dynamic_weights[:, 3:4] * voice_encoded
        )

        # ì¤‘ìš”ë„ ê¸°ë°˜ ìœµí•©
        importance_weighted = (
            importance_scores[0] * big5_encoded
            + importance_scores[1] * cmi_encoded
            + importance_scores[2] * rppg_encoded
            + importance_scores[3] * voice_encoded
        )

        # ìµœì¢… ìœµí•©
        final_fused = torch.cat(
            [big5_encoded, cmi_encoded, rppg_encoded, voice_encoded], dim=1
        )

        # ì ì‘í˜• ìœµí•© ë ˆì´ì–´ë¡œ ìµœì¢… ì˜ˆì¸¡
        output = self.adaptive_fusion(final_fused)

        return output, {
            "dynamic_weights": dynamic_weights,
            "importance_scores": importance_scores,
        }


class AdvancedMultimodalTrainer:
    """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ - BigQuery ì‹¤ì œ ë°ì´í„° ìµœì í™”"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.scalers = {}
        self.model = None
        self.training_history = []
        self.data_loader = BigQueryDataLoader(project_id)

        print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ“Š í”„ë¡œì íŠ¸ ID: {project_id}")

    def prepare_data(
        self,
        multimodal_data: Dict,
        targets: np.ndarray,
        test_size: float = 0.2,
        val_size: float = 0.1,
    ) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° DataLoader ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

        # RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ë” ê°•í•¨)
        for modality, data in multimodal_data.items():
            scaler = RobustScaler()
            multimodal_data[modality] = scaler.fit_transform(data)
            self.scalers[modality] = scaler

        # íƒ€ê²Ÿ ì •ê·œí™”
        target_scaler = RobustScaler()
        targets_scaled = target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        self.scalers["target"] = target_scaler

        # ë°ì´í„° ë¶„í• 
        big5_temp, big5_test, y_temp, y_test = train_test_split(
            multimodal_data["big5"],
            targets_scaled,
            test_size=test_size,
            random_state=42,
        )
        cmi_temp, cmi_test, _, _ = train_test_split(
            multimodal_data["cmi"], targets_scaled, test_size=test_size, random_state=42
        )
        rppg_temp, rppg_test, _, _ = train_test_split(
            multimodal_data["rppg"],
            targets_scaled,
            test_size=test_size,
            random_state=42,
        )
        voice_temp, voice_test, _, _ = train_test_split(
            multimodal_data["voice"],
            targets_scaled,
            test_size=test_size,
            random_state=42,
        )

        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        big5_train, big5_val, y_train, y_val = train_test_split(
            big5_temp, y_temp, test_size=val_size / (1 - test_size), random_state=42
        )
        cmi_train, cmi_val, _, _ = train_test_split(
            cmi_temp, y_temp, test_size=val_size / (1 - test_size), random_state=42
        )
        rppg_train, rppg_val, _, _ = train_test_split(
            rppg_temp, y_temp, test_size=val_size / (1 - test_size), random_state=42
        )
        voice_train, voice_val, _, _ = train_test_split(
            voice_temp, y_temp, test_size=val_size / (1 - test_size), random_state=42
        )

        # ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        X_train = {
            "big5": big5_train,
            "cmi": cmi_train,
            "rppg": rppg_train,
            "voice": voice_train,
        }
        X_val = {"big5": big5_val, "cmi": cmi_val, "rppg": rppg_val, "voice": voice_val}
        X_test = {
            "big5": big5_test,
            "cmi": cmi_test,
            "rppg": rppg_test,
            "voice": voice_test,
        }

        # DataLoader ìƒì„±
        train_dataset = MultimodalDataset(
            X_train["big5"], X_train["cmi"], X_train["rppg"], X_train["voice"], y_train
        )
        val_dataset = MultimodalDataset(
            X_val["big5"], X_val["cmi"], X_val["rppg"], X_val["voice"], y_val
        )
        test_dataset = MultimodalDataset(
            X_test["big5"], X_test["cmi"], X_test["rppg"], X_test["voice"], y_test
        )

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"   í›ˆë ¨: {len(train_dataset)}ê°œ")
        print(f"   ê²€ì¦: {len(val_dataset)}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")

        return train_loader, val_loader, test_loader

    def train_model(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int = 200,
        learning_rate: float = 0.001,
    ) -> Dict:
        """ëª¨ë¸ í›ˆë ¨ (ìµœì í™”ëœ ë²„ì „)"""
        print(f"ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (Epochs: {epochs})")

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = OptimizedMultimodalNet(
            big5_dim=25,
            cmi_dim=10,
            rppg_dim=15,
            voice_dim=20,
            hidden_dim=128,
            output_dim=1,
            dropout_rate=0.3,
            use_transformer=False,
            use_cross_attention=False,
            weight_decay=1e-4,
        ).to(self.device)

        criterion = nn.MSELoss()
        optimizer = optim.AdamW(
            self.model.parameters(), lr=learning_rate, weight_decay=1e-4
        )
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, "min", patience=15, factor=0.5, min_lr=1e-6
        )

        best_val_loss = float("inf")
        patience_counter = 0
        early_stopping_patience = 30

        for epoch in range(epochs):
            # í›ˆë ¨
            self.model.train()
            train_loss = 0.0
            train_predictions = []
            train_targets = []

            for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                big5 = batch["big5"].to(self.device)
                cmi = batch["cmi"].to(self.device)
                rppg = batch["rppg"].to(self.device)
                voice = batch["voice"].to(self.device)
                targets = batch["target"].to(self.device)

                optimizer.zero_grad()
                outputs, _ = self.model(big5, cmi, rppg, voice)

                loss = criterion(outputs.squeeze(), targets)
                loss.backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()

                train_loss += loss.item()
                train_predictions.extend(outputs.squeeze().detach().cpu().numpy())
                train_targets.extend(targets.detach().cpu().numpy())

            # ê²€ì¦
            self.model.eval()
            val_loss = 0.0
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    big5 = batch["big5"].to(self.device)
                    cmi = batch["cmi"].to(self.device)
                    rppg = batch["rppg"].to(self.device)
                    voice = batch["voice"].to(self.device)
                    targets = batch["target"].to(self.device)

                    outputs, _ = self.model(big5, cmi, rppg, voice)
                    loss = criterion(outputs.squeeze(), targets)
                    val_loss += loss.item()

                    val_predictions.extend(outputs.squeeze().cpu().numpy())
                    val_targets.extend(targets.cpu().numpy())

            # í‰ê·  ì†ì‹¤ ê³„ì‚°
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)

            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            scheduler.step(val_loss)

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            train_rmse = np.sqrt(
                np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)
            )
            val_rmse = np.sqrt(
                np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)
            )

            # RÂ² ê³„ì‚°
            train_r2 = r2_score(train_targets, train_predictions)
            val_r2 = r2_score(val_targets, val_predictions)

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.training_history.append(
                {
                    "epoch": epoch + 1,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "train_rmse": train_rmse,
                    "val_rmse": val_rmse,
                    "train_r2": train_r2,
                    "val_r2": val_r2,
                    "learning_rate": optimizer.param_groups[0]["lr"],
                }
            )

            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                torch.save(self.model.state_dict(), "best_optimized_model.pth")
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                print(f"â¹ï¸ ì¡°ê¸° ì¢…ë£Œ (Epoch {epoch+1})")
                break

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (epoch + 1) % 20 == 0:
                print(
                    f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}"
                )
                print(f"  Train RÂ²: {train_r2:.4f}, Val RÂ²: {val_r2:.4f}")
                print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")

        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        self.model.load_state_dict(torch.load("best_optimized_model.pth"))

        print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")

        return {
            "best_val_loss": best_val_loss,
            "total_epochs": epoch + 1,
            "training_history": self.training_history,
        }

    def evaluate_model(self, test_loader: DataLoader) -> Dict:
        """ëª¨ë¸ í‰ê°€ (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")

        self.model.eval()
        test_predictions = []
        test_targets = []

        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Evaluating"):
                big5 = batch["big5"].to(self.device)
                cmi = batch["cmi"].to(self.device)
                rppg = batch["rppg"].to(self.device)
                voice = batch["voice"].to(self.device)
                targets = batch["target"].to(self.device)

                outputs, _ = self.model(big5, cmi, rppg, voice)

                test_predictions.extend(outputs.squeeze().cpu().numpy())
                test_targets.extend(targets.cpu().numpy())

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        test_predictions = np.array(test_predictions)
        test_targets = np.array(test_targets)

        mse = mean_squared_error(test_targets, test_predictions)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(test_predictions - test_targets))
        r2 = r2_score(test_targets, test_predictions)
        correlation = np.corrcoef(test_predictions, test_targets)[0, 1]

        evaluation_results = {
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "r2": r2,
            "correlation": correlation,
            "predictions": test_predictions,
            "targets": test_targets,
        }

        print(f"âœ… í‰ê°€ ì™„ë£Œ:")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return evaluation_results

    def create_visualizations(
        self, evaluation_results: Dict, save_dir: str = "optimized_results"
    ):
        """ì‹œê°í™” ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

        os.makedirs(save_dir, exist_ok=True)

        # 1. í›ˆë ¨ íˆìŠ¤í† ë¦¬
        history_df = pd.DataFrame(self.training_history)

        plt.figure(figsize=(20, 12))

        # ì†ì‹¤ ê·¸ë˜í”„
        plt.subplot(2, 4, 1)
        plt.plot(history_df["epoch"], history_df["train_loss"], label="Train Loss")
        plt.plot(history_df["epoch"], history_df["val_loss"], label="Validation Loss")
        plt.title("Training History - Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

        # RÂ² ê·¸ë˜í”„
        plt.subplot(2, 4, 2)
        plt.plot(history_df["epoch"], history_df["train_r2"], label="Train RÂ²")
        plt.plot(history_df["epoch"], history_df["val_r2"], label="Validation RÂ²")
        plt.title("Training History - RÂ²")
        plt.xlabel("Epoch")
        plt.ylabel("RÂ²")
        plt.legend()
        plt.grid(True)

        # RMSE ê·¸ë˜í”„
        plt.subplot(2, 4, 3)
        plt.plot(history_df["epoch"], history_df["train_rmse"], label="Train RMSE")
        plt.plot(history_df["epoch"], history_df["val_rmse"], label="Validation RMSE")
        plt.title("Training History - RMSE")
        plt.xlabel("Epoch")
        plt.ylabel("RMSE")
        plt.legend()
        plt.grid(True)

        # í•™ìŠµë¥  ê·¸ë˜í”„
        plt.subplot(2, 4, 4)
        plt.plot(history_df["epoch"], history_df["learning_rate"])
        plt.title("Learning Rate Schedule")
        plt.xlabel("Epoch")
        plt.ylabel("Learning Rate")
        plt.yscale("log")
        plt.grid(True)

        # ì˜ˆì¸¡ vs ì‹¤ì œ
        plt.subplot(2, 4, 5)
        plt.scatter(
            evaluation_results["targets"], evaluation_results["predictions"], alpha=0.6
        )
        plt.plot(
            [evaluation_results["targets"].min(), evaluation_results["targets"].max()],
            [evaluation_results["targets"].min(), evaluation_results["targets"].max()],
            "r--",
        )
        plt.title(f'Predictions vs Targets (RÂ² = {evaluation_results["r2"]:.3f})')
        plt.xlabel("Actual")
        plt.ylabel("Predicted")
        plt.grid(True)

        # ì”ì°¨ ë¶„í¬
        plt.subplot(2, 4, 6)
        residuals = evaluation_results["predictions"] - evaluation_results["targets"]
        plt.hist(residuals, bins=30, alpha=0.7)
        plt.title("Residual Distribution")
        plt.xlabel("Residuals")
        plt.ylabel("Frequency")
        plt.grid(True)

        # ì˜ˆì¸¡ ë¶„í¬
        plt.subplot(2, 4, 7)
        plt.hist(
            evaluation_results["predictions"],
            bins=30,
            alpha=0.7,
            label="Predictions",
            color="blue",
        )
        plt.hist(
            evaluation_results["targets"],
            bins=30,
            alpha=0.7,
            label="Targets",
            color="red",
        )
        plt.title("Prediction vs Target Distribution")
        plt.xlabel("Value")
        plt.ylabel("Frequency")
        plt.legend()
        plt.grid(True)

        # ì„±ëŠ¥ ìš”ì•½
        plt.subplot(2, 4, 8)
        plt.text(0.1, 0.8, f"Performance Summary:", fontsize=14, fontweight="bold")
        plt.text(0.1, 0.7, f"RÂ² = {evaluation_results['r2']:.4f}", fontsize=12)
        plt.text(0.1, 0.6, f"RMSE = {evaluation_results['rmse']:.4f}", fontsize=12)
        plt.text(0.1, 0.5, f"MAE = {evaluation_results['mae']:.4f}", fontsize=12)
        plt.text(
            0.1,
            0.4,
            f"Correlation = {evaluation_results['correlation']:.4f}",
            fontsize=12,
        )
        plt.axis("off")

        plt.tight_layout()
        plt.savefig(
            f"{save_dir}/optimized_training_analysis.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        print(f"âœ… ì‹œê°í™” ì €ì¥: {save_dir}/optimized_training_analysis.png")

    def run_comprehensive_training(
        self, n_samples: int = 10000, epochs: int = 200
    ) -> Dict:
        """ì¢…í•© í›ˆë ¨ ì‹¤í–‰ (BigQuery ì‹¤ì œ ë°ì´í„°)"""
        print("ğŸš€ ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.data_loader.load_competition_data(n_samples)

        # 2. ë°ì´í„° ì¤€ë¹„
        train_loader, val_loader, test_loader = self.prepare_data(
            multimodal_data, targets
        )

        # 3. ëª¨ë¸ í›ˆë ¨
        training_results = self.train_model(train_loader, val_loader, epochs)

        # 4. ëª¨ë¸ í‰ê°€
        evaluation_results = self.evaluate_model(test_loader)

        # 5. ì‹œê°í™” ìƒì„±
        self.create_visualizations(evaluation_results)

        # 6. ê²°ê³¼ ì €ì¥
        results = {
            "training_results": training_results,
            "evaluation_results": evaluation_results,
            "model_info": {
                "device": str(self.device),
                "n_samples": n_samples,
                "epochs_trained": training_results["total_epochs"],
                "project_id": self.project_id,
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("optimized_training_results.json", "w") as f:
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj
            
            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ìµœì¢… RÂ²: {evaluation_results['r2']:.4f}")
        print(f"   ìµœì¢… RMSE: {evaluation_results['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {evaluation_results['correlation']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í›ˆë ¨ ì‹œìŠ¤í…œ - BigQuery ì‹¤ì œ ë°ì´í„° ìµœì í™”")
    print("=" * 70)

    # í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    trainer = AdvancedMultimodalTrainer()

    # ì¢…í•© í›ˆë ¨ ì‹¤í–‰
    results = trainer.run_comprehensive_training(n_samples=10000, epochs=200)

    print("\nğŸ“Š í›ˆë ¨ ê²°ê³¼ ìš”ì•½:")
    print(f"   ë””ë°”ì´ìŠ¤: {results['model_info']['device']}")
    print(f"   ìƒ˜í”Œ ìˆ˜: {results['model_info']['n_samples']}")
    print(f"   í›ˆë ¨ ì—í¬í¬: {results['model_info']['epochs_trained']}")
    print(f"   ìµœì¢… ì„±ëŠ¥: RÂ² = {results['evaluation_results']['r2']:.4f}")
    print(f"   ìµœì¢… RMSE: {results['evaluation_results']['rmse']:.4f}")


if __name__ == "__main__":
    main()
