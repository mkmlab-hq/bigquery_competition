#!/usr/bin/env python3
"""
ê³¼ì í•© ë¶„ì„ ì‹œìŠ¤í…œ - RÂ² 0.9797 ëª¨ë¸ì˜ ê³¼ì í•© ìœ„í—˜ì„± ê²€ì¦
- ì‹¤ì œ BigQuery ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
- ê³¼ì í•© ë°©ì§€ ì „ëµ ê²€ì¦
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class OverfittingAnalyzer:
    """ê³¼ì í•© ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}
        self.ensemble_weights = None
        self.best_models = []
        self.cv_scores = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])

            # Big5 ë°ì´í„°ë„ ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•© (ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì‚¬ìš©)
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (Big5 ì ìˆ˜ ê¸°ë°˜)
            big5_scores = {
                "EXT": big5_df[["EXT1", "EXT2", "EXT3", "EXT4", "EXT5"]].mean(axis=1),
                "EST": big5_df[["EST1", "EST2", "EST3", "EST4", "EST5"]].mean(axis=1),
                "AGR": big5_df[["AGR1", "AGR2", "AGR3", "AGR4", "AGR5"]].mean(axis=1),
                "CSN": big5_df[["CSN1", "CSN2", "CSN3", "CSN4", "CSN5"]].mean(axis=1),
                "OPN": big5_df[["OPN1", "OPN2", "OPN3", "OPN4", "OPN5"]].mean(axis=1),
            }

            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            targets = (
                big5_scores["EXT"] * 0.25
                + big5_scores["OPN"] * 0.20
                + (6 - big5_scores["EST"]) * 0.15
                + big5_scores["AGR"] * 0.15
                + big5_scores["CSN"] * 0.10
                + (cmi_numeric.mean(axis=1) / 6) * 0.10
                + (rppg_numeric.mean(axis=1) / 6) * 0.05
            )

            # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
            targets = (targets - targets.min()) / (
                targets.max() - targets.min()
            ) * 9 + 1

            print(f"âœ… ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_df.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def prepare_data(
        self, multimodal_data: Dict, targets: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ê²°í•©
        X = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # RobustScalerë¡œ ì •ê·œí™”
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["robust_ensemble"] = scaler

        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape}")
        return X_scaled, targets

    def create_models(self):
        """ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=150,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=5,
                min_samples_split=10,
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=5,
                subsample=0.7,
                colsample_bytree=0.7,
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=5,
                subsample=0.7,
                colsample_bytree=0.7,
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=5.0),
            "elastic_net": ElasticNet(alpha=0.05, l1_ratio=0.7),
            "svr": SVR(kernel="rbf", C=0.5, gamma="scale"),
        }

        print(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def analyze_overfitting(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ê³¼ì í•© ë¶„ì„"""
        print("ğŸ” ê³¼ì í•© ë¶„ì„ ì‹œì‘...")

        # 1. í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.4, random_state=42
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42
        )

        print(f"   í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
        print(f"   ê²€ì¦ ë°ì´í„°: {X_val.shape}")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

        # 2. êµì°¨ ê²€ì¦ìœ¼ë¡œ ê³¼ì í•© ë¶„ì„
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        overfitting_analysis = {}

        for name, model in self.models.items():
            print(f"   ë¶„ì„ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦ ì ìˆ˜
                cv_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )

                # í›ˆë ¨ ë°ì´í„°ë¡œ í›ˆë ¨
                model.fit(X_train, y_train)

                # í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡
                train_pred = model.predict(X_train)
                train_r2 = r2_score(y_train, train_pred)

                # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
                val_pred = model.predict(X_val)
                val_r2 = r2_score(y_val, val_pred)

                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)

                # ê³¼ì í•© ì§€í‘œ ê³„ì‚°
                overfitting_gap = train_r2 - val_r2
                generalization_gap = val_r2 - test_r2

                overfitting_analysis[name] = {
                    "cv_mean_r2": cv_scores.mean(),
                    "cv_std_r2": cv_scores.std(),
                    "train_r2": train_r2,
                    "val_r2": val_r2,
                    "test_r2": test_r2,
                    "overfitting_gap": overfitting_gap,
                    "generalization_gap": generalization_gap,
                    "is_overfitting": overfitting_gap > 0.1,  # 10% ì´ìƒ ì°¨ì´ë©´ ê³¼ì í•©
                    "is_generalizing": abs(generalization_gap)
                    < 0.05,  # 5% ì´ë‚´ë©´ ì¼ë°˜í™” ì–‘í˜¸
                }

                print(f"     í›ˆë ¨ RÂ²: {train_r2:.4f}")
                print(f"     ê²€ì¦ RÂ²: {val_r2:.4f}")
                print(f"     í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {overfitting_gap:.4f}")
                print(f"     ì¼ë°˜í™” ê°„ê²©: {generalization_gap:.4f}")

            except Exception as e:
                print(f"     âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                overfitting_analysis[name] = None

        return overfitting_analysis

    def analyze_data_quality(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì¤‘...")

        # 1. ë°ì´í„° ë¶„í¬ ë¶„ì„
        data_quality = {
            "n_samples": len(y),
            "n_features": X.shape[1],
            "feature_ratio": X.shape[1] / len(y),
            "target_stats": {
                "mean": float(y.mean()),
                "std": float(y.std()),
                "min": float(y.min()),
                "max": float(y.max()),
                "range": float(y.max() - y.min()),
            },
            "feature_stats": {
                "mean_std": float(X.std().mean()),
                "max_std": float(X.std().max()),
                "min_std": float(X.std().min()),
            },
        }

        # 2. ê³¼ì í•© ìœ„í—˜ë„ í‰ê°€
        if data_quality["feature_ratio"] > 0.1:
            data_quality["overfitting_risk"] = "HIGH"
        elif data_quality["feature_ratio"] > 0.05:
            data_quality["overfitting_risk"] = "MEDIUM"
        else:
            data_quality["overfitting_risk"] = "LOW"

        print(f"   ìƒ˜í”Œ ìˆ˜: {data_quality['n_samples']}")
        print(f"   íŠ¹ì„± ìˆ˜: {data_quality['n_features']}")
        print(f"   íŠ¹ì„±/ìƒ˜í”Œ ë¹„ìœ¨: {data_quality['feature_ratio']:.4f}")
        print(f"   ê³¼ì í•© ìœ„í—˜ë„: {data_quality['overfitting_risk']}")

        return data_quality

    def run_overfitting_analysis(self, limit: int = 10000) -> Dict:
        """ê³¼ì í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ê³¼ì í•© ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_real_bigquery_data(limit)

        # 2. ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_data(multimodal_data, targets)

        # 3. ëª¨ë¸ë“¤ ìƒì„±
        self.create_models()

        # 4. ê³¼ì í•© ë¶„ì„
        overfitting_analysis = self.analyze_overfitting(X, y)

        # 5. ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        data_quality = self.analyze_data_quality(X, y)

        # 6. ê²°ê³¼ ì¢…í•©
        results = {
            "overfitting_analysis": overfitting_analysis,
            "data_quality": data_quality,
            "summary": {
                "total_models": len(
                    [m for m in overfitting_analysis.values() if m is not None]
                ),
                "overfitting_models": len(
                    [
                        m
                        for m in overfitting_analysis.values()
                        if m and m.get("is_overfitting", False)
                    ]
                ),
                "generalizing_models": len(
                    [
                        m
                        for m in overfitting_analysis.values()
                        if m and m.get("is_generalizing", False)
                    ]
                ),
                "avg_overfitting_gap": np.mean(
                    [m["overfitting_gap"] for m in overfitting_analysis.values() if m]
                ),
                "avg_generalization_gap": np.mean(
                    [
                        m["generalization_gap"]
                        for m in overfitting_analysis.values()
                        if m
                    ]
                ),
            },
        }

        # 7. ê²°ê³¼ ì €ì¥
        with open("overfitting_analysis_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ê³¼ì í•© ë¶„ì„ ì™„ë£Œ!")
        print(f"   ì´ ëª¨ë¸ ìˆ˜: {results['summary']['total_models']}")
        print(f"   ê³¼ì í•© ëª¨ë¸ ìˆ˜: {results['summary']['overfitting_models']}")
        print(f"   ì¼ë°˜í™” ì–‘í˜¸ ëª¨ë¸ ìˆ˜: {results['summary']['generalizing_models']}")
        print(f"   í‰ê·  ê³¼ì í•© ê°„ê²©: {results['summary']['avg_overfitting_gap']:.4f}")
        print(
            f"   í‰ê·  ì¼ë°˜í™” ê°„ê²©: {results['summary']['avg_generalization_gap']:.4f}"
        )

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³¼ì í•© ë¶„ì„ ì‹œìŠ¤í…œ - RÂ² 0.9797 ëª¨ë¸ ê²€ì¦")
    print("=" * 60)

    analyzer = OverfittingAnalyzer()
    results = analyzer.run_overfitting_analysis(limit=10000)

    print("\nğŸ“Š ê³¼ì í•© ë¶„ì„ ê²°ê³¼:")
    print(f"   ê³¼ì í•© ëª¨ë¸ ìˆ˜: {results['summary']['overfitting_models']}")
    print(f"   ì¼ë°˜í™” ì–‘í˜¸ ëª¨ë¸ ìˆ˜: {results['summary']['generalizing_models']}")
    print(f"   í‰ê·  ê³¼ì í•© ê°„ê²©: {results['summary']['avg_overfitting_gap']:.4f}")
    print(f"   í‰ê·  ì¼ë°˜í™” ê°„ê²©: {results['summary']['avg_generalization_gap']:.4f}")


if __name__ == "__main__":
    main()
