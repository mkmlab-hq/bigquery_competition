#!/usr/bin/env python3
"""
ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì‹œìŠ¤í…œ - ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨
- ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
- ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ë°©ì§€
- í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class ColdVerifier:
    """ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©"""
        print("ðŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            print(f"âœ… ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")

            return multimodal_data

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_completely_independent_target(self, n_samples: int) -> np.ndarray:
        """ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ìž…ë ¥ ë°ì´í„°ì™€ ì™„ì „ížˆ ë¬´ê´€)"""
        print("ðŸ” ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

        # ë°©ë²• 1: ì™„ì „ížˆ ëžœë¤í•œ íƒ€ê²Ÿ ë³€ìˆ˜
        np.random.seed(42)
        random_targets = np.random.uniform(1, 10, n_samples)

        print(f"âœ… ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ:")
        print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
        print(f"     í‰ê· : {random_targets.mean():.4f}")
        print(f"     í‘œì¤€íŽ¸ì°¨: {random_targets.std():.4f}")
        print(f"     ìµœì†Œê°’: {random_targets.min():.4f}")
        print(f"     ìµœëŒ€ê°’: {random_targets.max():.4f}")
        print(f"   ìž…ë ¥ ë°ì´í„°ì™€ ì™„ì „ížˆ ë…ë¦½ì !")

        return random_targets

    def create_models(self):
        """ëª¨ë¸ë“¤ ìƒì„±"""
        print("ðŸ”„ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=10,
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=1.0),
            "elastic_net": ElasticNet(alpha=0.1, l1_ratio=0.5),
            "svr": SVR(kernel="rbf", C=1.0, gamma="scale"),
        }

        print(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_data(
        self, multimodal_data: Dict, targets: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë°ì´í„° ì¤€ë¹„"""
        print("ðŸ”„ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ê²°í•©
        X = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # RobustScalerë¡œ ì •ê·œí™”
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["robust_ensemble"] = scaler

        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape}")
        return X_scaled, targets

    def test_models_with_independent_target(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        print("ðŸš€ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ ì‹œìž‘...")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        model_results = {}

        for name, model in self.models.items():
            print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {name}")

            try:
                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ì˜ˆì¸¡
                train_pred = model.predict(X_train)
                test_pred = model.predict(X_test)

                # ì„±ëŠ¥ í‰ê°€
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                model_results[name] = {
                    "train_r2": train_r2,
                    "test_r2": test_r2,
                    "train_rmse": train_rmse,
                    "test_rmse": test_rmse,
                    "overfitting_gap": train_r2 - test_r2,
                }

                print(f"     í›ˆë ¨ RÂ²: {train_r2:.4f}, í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
                print(f"     í›ˆë ¨ RMSE: {train_rmse:.4f}, í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {train_r2 - test_r2:.4f}")

            except Exception as e:
                print(f"     âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        return model_results

    def test_models_with_correlated_target(
        self, X: np.ndarray, multimodal_data: Dict
    ) -> Dict:
        """ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        print("ðŸš€ ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ ì‹œìž‘...")

        # ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (í•˜ì§€ë§Œ ì™„ì „ížˆ ë™ì¼í•˜ì§€ëŠ” ì•ŠìŒ)
        print("ðŸ” ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

        # Big5 ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ì—¬ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        big5_scores = {
            "EXT": multimodal_data["big5"][:, 0:5].mean(axis=1),  # ì²˜ìŒ 5ê°œë§Œ
            "OPN": multimodal_data["big5"][:, 5:10].mean(axis=1),  # ë‹¤ìŒ 5ê°œë§Œ
        }

        # ìƒê´€ê´€ê³„ê°€ ìžˆì§€ë§Œ ì™„ì „ížˆ ë™ì¼í•˜ì§€ ì•Šì€ íƒ€ê²Ÿ ë³€ìˆ˜
        correlated_target = (
            big5_scores["EXT"] * 0.6
            + big5_scores["OPN"] * 0.4
            + np.random.normal(0, 0.5, len(big5_scores["EXT"]))  # ë…¸ì´ì¦ˆ ì¶”ê°€
        )

        # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        y = (correlated_target - correlated_target.min()) / (
            correlated_target.max() - correlated_target.min()
        ) * 9 + 1

        print(f"âœ… ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ:")
        print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
        print(f"     í‰ê· : {y.mean():.4f}")
        print(f"     í‘œì¤€íŽ¸ì°¨: {y.std():.4f}")
        print(f"     ìµœì†Œê°’: {y.min():.4f}")
        print(f"     ìµœëŒ€ê°’: {y.max():.4f}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        model_results = {}

        for name, model in self.models.items():
            print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {name}")

            try:
                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ì˜ˆì¸¡
                train_pred = model.predict(X_train)
                test_pred = model.predict(X_test)

                # ì„±ëŠ¥ í‰ê°€
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                model_results[name] = {
                    "train_r2": train_r2,
                    "test_r2": test_r2,
                    "train_rmse": train_rmse,
                    "test_rmse": test_rmse,
                    "overfitting_gap": train_r2 - test_r2,
                }

                print(f"     í›ˆë ¨ RÂ²: {train_r2:.4f}, í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
                print(f"     í›ˆë ¨ RMSE: {train_rmse:.4f}, í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.4f}")
                print(f"     ê³¼ì í•© ê°„ê²©: {train_r2 - test_r2:.4f}")

            except Exception as e:
                print(f"     âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        return model_results

    def run_cold_verification(self, limit: int = 10000) -> Dict:
        """ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì‹¤í–‰"""
        print("ðŸš€ ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì‹œìŠ¤í…œ ì‹œìž‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data = self.load_real_bigquery_data(limit)

        # 2. ëª¨ë¸ë“¤ ìƒì„±
        self.create_models()

        # 3. ë°ì´í„° ì¤€ë¹„
        X, _ = self.prepare_data(multimodal_data, np.zeros(limit))

        # 4. ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 50)
        print("ðŸ” í…ŒìŠ¤íŠ¸ 1: ì™„ì „ížˆ ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜")
        print("=" * 50)
        independent_targets = self.create_completely_independent_target(limit)
        independent_results = self.test_models_with_independent_target(
            X, independent_targets
        )

        # 5. ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 50)
        print("ðŸ” í…ŒìŠ¤íŠ¸ 2: ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜")
        print("=" * 50)
        correlated_results = self.test_models_with_correlated_target(X, multimodal_data)

        # 6. ê²°ê³¼ ë¶„ì„
        print("\n" + "=" * 50)
        print("ðŸ“Š ëƒ‰ì² í•œ ìž¬ê²€ì¦ ê²°ê³¼ ë¶„ì„")
        print("=" * 50)

        # ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ê²°ê³¼ ë¶„ì„
        valid_independent = {
            k: v for k, v in independent_results.items() if v is not None
        }
        if valid_independent:
            avg_independent_r2 = np.mean(
                [r["test_r2"] for r in valid_independent.values()]
            )
            max_independent_r2 = np.max(
                [r["test_r2"] for r in valid_independent.values()]
            )
            min_independent_r2 = np.min(
                [r["test_r2"] for r in valid_independent.values()]
            )

            print(f"ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸:")
            print(f"   í‰ê·  í…ŒìŠ¤íŠ¸ RÂ²: {avg_independent_r2:.4f}")
            print(f"   ìµœëŒ€ í…ŒìŠ¤íŠ¸ RÂ²: {max_independent_r2:.4f}")
            print(f"   ìµœì†Œ í…ŒìŠ¤íŠ¸ RÂ²: {min_independent_r2:.4f}")

            if avg_independent_r2 > 0.5:
                print("   ðŸš¨ ê²½ê³ : ë…ë¦½ì ì¸ íƒ€ê²Ÿì—ì„œë„ ë†’ì€ ì„±ëŠ¥! ë°ì´í„° ëˆ„ì¶œ ì˜ì‹¬!")
            elif avg_independent_r2 > 0.2:
                print(
                    "   âš ï¸ ì£¼ì˜: ë…ë¦½ì ì¸ íƒ€ê²Ÿì—ì„œ ì¤‘ê°„ ì„±ëŠ¥. ì¼ë¶€ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„±."
                )
            else:
                print("   âœ… ì–‘í˜¸: ë…ë¦½ì ì¸ íƒ€ê²Ÿì—ì„œ ë‚®ì€ ì„±ëŠ¥. ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ.")

        # ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ ê²°ê³¼ ë¶„ì„
        valid_correlated = {
            k: v for k, v in correlated_results.items() if v is not None
        }
        if valid_correlated:
            avg_correlated_r2 = np.mean(
                [r["test_r2"] for r in valid_correlated.values()]
            )
            max_correlated_r2 = np.max(
                [r["test_r2"] for r in valid_correlated.values()]
            )
            min_correlated_r2 = np.min(
                [r["test_r2"] for r in valid_correlated.values()]
            )

            print(f"\nìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸:")
            print(f"   í‰ê·  í…ŒìŠ¤íŠ¸ RÂ²: {avg_correlated_r2:.4f}")
            print(f"   ìµœëŒ€ í…ŒìŠ¤íŠ¸ RÂ²: {max_correlated_r2:.4f}")
            print(f"   ìµœì†Œ í…ŒìŠ¤íŠ¸ RÂ²: {min_correlated_r2:.4f}")

            if avg_correlated_r2 > 0.8:
                print(
                    "   ðŸš¨ ê²½ê³ : ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿì—ì„œ ë§¤ìš° ë†’ì€ ì„±ëŠ¥! ë°ì´í„° ëˆ„ì¶œ ì˜ì‹¬!"
                )
            elif avg_correlated_r2 > 0.5:
                print(
                    "   âš ï¸ ì£¼ì˜: ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿì—ì„œ ë†’ì€ ì„±ëŠ¥. ì¼ë¶€ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„±."
                )
            else:
                print("   âœ… ì–‘í˜¸: ìƒê´€ê´€ê³„ê°€ ìžˆëŠ” íƒ€ê²Ÿì—ì„œ í˜„ì‹¤ì ì¸ ì„±ëŠ¥.")

        # 7. ê²°ê³¼ ì €ìž¥
        results = {
            "independent_target_results": independent_results,
            "correlated_target_results": correlated_results,
            "summary": {
                "avg_independent_r2": (
                    float(avg_independent_r2) if valid_independent else 0
                ),
                "max_independent_r2": (
                    float(max_independent_r2) if valid_independent else 0
                ),
                "min_independent_r2": (
                    float(min_independent_r2) if valid_independent else 0
                ),
                "avg_correlated_r2": (
                    float(avg_correlated_r2) if valid_correlated else 0
                ),
                "max_correlated_r2": (
                    float(max_correlated_r2) if valid_correlated else 0
                ),
                "min_correlated_r2": (
                    float(min_correlated_r2) if valid_correlated else 0
                ),
                "data_leakage_suspected": (
                    avg_independent_r2 > 0.5 if valid_independent else False
                ),
                "correlated_performance_realistic": (
                    avg_correlated_r2 < 0.8 if valid_correlated else False
                ),
            },
        }

        with open("cold_verification_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"\nâœ… ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì™„ë£Œ!")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ðŸš€ ëƒ‰ì² í•œ ìž¬ê²€ì¦ ì‹œìŠ¤í…œ - ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨")
    print("=" * 60)

    verifier = ColdVerifier()
    results = verifier.run_cold_verification(limit=10000)

    print("\nðŸ“Š ëƒ‰ì² í•œ ìž¬ê²€ì¦ ìµœì¢… ê²°ê³¼:")
    print(f"   ë…ë¦½ì ì¸ íƒ€ê²Ÿ í‰ê·  RÂ²: {results['summary']['avg_independent_r2']:.4f}")
    print(f"   ìƒê´€ê´€ê³„ íƒ€ê²Ÿ í‰ê·  RÂ²: {results['summary']['avg_correlated_r2']:.4f}")
    print(f"   ë°ì´í„° ëˆ„ì¶œ ì˜ì‹¬: {results['summary']['data_leakage_suspected']}")
    print(
        f"   ìƒê´€ê´€ê³„ ì„±ëŠ¥ í˜„ì‹¤ì : {results['summary']['correlated_performance_realistic']}"
    )


if __name__ == "__main__":
    main()
