#!/usr/bin/env python3
"""
BigQuery ÏµúÏ†ÅÌôî Ï†ÑÎûµ ÏãúÏä§ÌÖú
- Partitioning Î∞è Clustering ÏµúÏ†ÅÌôî
- Materialized Views ÏÉùÏÑ±
- ÏøºÎ¶¨ ÏÑ±Îä• Î∂ÑÏÑù
- ÎπÑÏö© ÏµúÏ†ÅÌôî
"""

import json
import time
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import google.cloud.bigquery as bigquery
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore", category=UserWarning)


class BigQueryOptimizationStrategy:
    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.client = bigquery.Client(project=project_id)
        self.results_dir = "bigquery_optimization_results"
        import os

        os.makedirs(self.results_dir, exist_ok=True)

    def analyze_current_performance(self):
        """ÌòÑÏû¨ ÏøºÎ¶¨ ÏÑ±Îä• Î∂ÑÏÑù"""
        print("üìä ÌòÑÏû¨ BigQuery ÏÑ±Îä• Î∂ÑÏÑù Ï§ë...")

        # Í∏∞Î≥∏ Big5 ÏøºÎ¶¨ ÏÑ±Îä• ÌÖåÏä§Ìä∏
        queries = {
            "basic_big5": """
                SELECT * FROM `{project_id}.big5_dataset.big5_preprocessed` 
                LIMIT 1000
            """,
            "big5_statistics": """
                SELECT 
                    COUNT(*) as total_records,
                    AVG(EXT1) as avg_ext1,
                    AVG(EST1) as avg_est1,
                    AVG(AGR1) as avg_agr1,
                    AVG(CSN1) as avg_csn1,
                    AVG(OPN1) as avg_opn1
                FROM `{project_id}.big5_dataset.big5_preprocessed`
            """,
            "country_analysis": """
                SELECT 
                    country,
                    COUNT(*) as count,
                    AVG(EXT1) as avg_ext1,
                    AVG(EST1) as avg_est1,
                    AVG(AGR1) as avg_agr1,
                    AVG(CSN1) as avg_csn1,
                    AVG(OPN1) as avg_opn1
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                GROUP BY country
                ORDER BY count DESC
                LIMIT 20
            """,
            "correlation_analysis": """
                SELECT 
                    CORR(EXT1, EST1) as ext_est_corr,
                    CORR(EXT1, AGR1) as ext_agr_corr,
                    CORR(EST1, AGR1) as est_agr_corr,
                    CORR(CSN1, OPN1) as csn_opn_corr
                FROM `{project_id}.big5_dataset.big5_preprocessed`
            """,
        }

        performance_results = {}

        for query_name, query_template in queries.items():
            query = query_template.format(project_id=self.project_id)

            try:
                start_time = time.time()
                query_job = self.client.query(query)
                results = query_job.result()

                # ÏøºÎ¶¨ ÌÜµÍ≥Ñ ÏàòÏßë
                job_stats = query_job.dry_run
                bytes_processed = (
                    query_job.total_bytes_processed
                    if hasattr(query_job, "total_bytes_processed")
                    else 0
                )
                bytes_billed = (
                    query_job.total_bytes_billed
                    if hasattr(query_job, "total_bytes_billed")
                    else 0
                )

                execution_time = time.time() - start_time

                performance_results[query_name] = {
                    "execution_time": execution_time,
                    "bytes_processed": bytes_processed,
                    "bytes_billed": bytes_billed,
                    "estimated_cost": bytes_billed / (1024**4) * 5,  # $5 per TB
                    "status": "success",
                }

                print(
                    f"  {query_name}: {execution_time:.2f}Ï¥à, {bytes_processed:,} bytes"
                )

            except Exception as e:
                performance_results[query_name] = {
                    "execution_time": 0,
                    "bytes_processed": 0,
                    "bytes_billed": 0,
                    "estimated_cost": 0,
                    "status": f"error: {str(e)}",
                }
                print(f"  {query_name}: Ïò§Î•ò - {e}")

        return performance_results

    def create_optimized_queries(self):
        """ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÉùÏÑ±"""
        print("\nüîß ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÉùÏÑ± Ï§ë...")

        optimized_queries = {
            "sampled_big5": """
                SELECT * FROM `{project_id}.big5_dataset.big5_preprocessed` 
                TABLESAMPLE SYSTEM (10 PERCENT)
                LIMIT 1000
            """,
            "partitioned_big5": """
                SELECT 
                    *,
                    DATE('2024-01-01') as partition_date
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IN ('US', 'GB', 'CA', 'AU')
                LIMIT 1000
            """,
            "clustered_big5": """
                SELECT 
                    country,
                    EXT1, EST1, AGR1, CSN1, OPN1,
                    (EXT1 + EST1 + AGR1 + CSN1 + OPN1) / 5 as avg_personality
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IS NOT NULL
                ORDER BY country, avg_personality DESC
                LIMIT 1000
            """,
            "window_functions": """
                SELECT 
                    country,
                    EXT1, EST1, AGR1, CSN1, OPN1,
                    ROW_NUMBER() OVER (PARTITION BY country ORDER BY EXT1 DESC) as ext_rank,
                    AVG(EXT1) OVER (PARTITION BY country) as country_avg_ext,
                    PERCENTILE_CONT(EXT1, 0.5) OVER (PARTITION BY country) as country_median_ext
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IS NOT NULL
                LIMIT 1000
            """,
            "aggregated_features": """
                SELECT 
                    country,
                    COUNT(*) as total_count,
                    AVG(EXT1) as avg_ext,
                    STDDEV(EXT1) as std_ext,
                    AVG(EST1) as avg_est,
                    STDDEV(EST1) as std_est,
                    AVG(AGR1) as avg_agr,
                    STDDEV(AGR1) as std_agr,
                    AVG(CSN1) as avg_csn,
                    STDDEV(CSN1) as std_csn,
                    AVG(OPN1) as avg_opn,
                    STDDEV(OPN1) as std_opn
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IS NOT NULL
                GROUP BY country
                HAVING COUNT(*) > 10
                ORDER BY total_count DESC
            """,
        }

        return optimized_queries

    def create_materialized_views(self):
        """Materialized Views ÏÉùÏÑ±"""
        print("\nüìä Materialized Views ÏÉùÏÑ± Ï§ë...")

        materialized_views = {
            "big5_country_stats": """
                CREATE OR REPLACE MATERIALIZED VIEW `{project_id}.big5_dataset.big5_country_stats`
                PARTITION BY DATE('2024-01-01')
                CLUSTER BY country
                AS
                SELECT 
                    country,
                    COUNT(*) as total_count,
                    AVG(EXT1) as avg_ext,
                    STDDEV(EXT1) as std_ext,
                    AVG(EST1) as avg_est,
                    STDDEV(EST1) as std_est,
                    AVG(AGR1) as avg_agr,
                    STDDEV(AGR1) as std_agr,
                    AVG(CSN1) as avg_csn,
                    STDDEV(CSN1) as std_csn,
                    AVG(OPN1) as avg_opn,
                    STDDEV(OPN1) as std_opn,
                    CURRENT_TIMESTAMP() as created_at
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IS NOT NULL
                GROUP BY country
            """,
            "big5_personality_clusters": """
                CREATE OR REPLACE MATERIALIZED VIEW `{project_id}.big5_dataset.big5_personality_clusters`
                PARTITION BY DATE('2024-01-01')
                CLUSTER BY personality_type
                AS
                SELECT 
                    *,
                    CASE 
                        WHEN EXT1 > 4 AND OPN1 > 4 THEN 'High_Ext_Opn'
                        WHEN EST1 < 2 AND AGR1 > 4 THEN 'Low_Est_High_Agr'
                        WHEN CSN1 > 4 AND OPN1 > 4 THEN 'High_Csn_Opn'
                        WHEN EXT1 < 2 AND EST1 < 2 THEN 'Low_Ext_Est'
                        ELSE 'Mixed'
                    END as personality_type
                FROM `{project_id}.big5_dataset.big5_preprocessed`
                WHERE country IS NOT NULL
            """,
            "big5_correlations": """
                CREATE OR REPLACE MATERIALIZED VIEW `{project_id}.big5_dataset.big5_correlations`
                PARTITION BY DATE('2024-01-01')
                AS
                SELECT 
                    CORR(EXT1, EST1) as ext_est_corr,
                    CORR(EXT1, AGR1) as ext_agr_corr,
                    CORR(EXT1, CSN1) as ext_csn_corr,
                    CORR(EXT1, OPN1) as ext_opn_corr,
                    CORR(EST1, AGR1) as est_agr_corr,
                    CORR(EST1, CSN1) as est_csn_corr,
                    CORR(EST1, OPN1) as est_opn_corr,
                    CORR(AGR1, CSN1) as agr_csn_corr,
                    CORR(AGR1, OPN1) as agr_opn_corr,
                    CORR(CSN1, OPN1) as csn_opn_corr,
                    CURRENT_TIMESTAMP() as created_at
                FROM `{project_id}.big5_dataset.big5_preprocessed`
            """,
        }

        return materialized_views

    def test_optimized_queries(self, optimized_queries):
        """ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÑ±Îä• ÌÖåÏä§Ìä∏"""
        print("\nüß™ ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÑ±Îä• ÌÖåÏä§Ìä∏ Ï§ë...")

        test_results = {}

        for query_name, query_template in optimized_queries.items():
            query = query_template.format(project_id=self.project_id)

            try:
                start_time = time.time()
                query_job = self.client.query(query)
                results = query_job.result()

                bytes_processed = (
                    query_job.total_bytes_processed
                    if hasattr(query_job, "total_bytes_processed")
                    else 0
                )
                bytes_billed = (
                    query_job.total_bytes_billed
                    if hasattr(query_job, "total_bytes_billed")
                    else 0
                )

                execution_time = time.time() - start_time

                test_results[query_name] = {
                    "execution_time": execution_time,
                    "bytes_processed": bytes_processed,
                    "bytes_billed": bytes_billed,
                    "estimated_cost": bytes_billed / (1024**4) * 5,
                    "status": "success",
                }

                print(
                    f"  {query_name}: {execution_time:.2f}Ï¥à, {bytes_processed:,} bytes"
                )

            except Exception as e:
                test_results[query_name] = {
                    "execution_time": 0,
                    "bytes_processed": 0,
                    "bytes_billed": 0,
                    "estimated_cost": 0,
                    "status": f"error: {str(e)}",
                }
                print(f"  {query_name}: Ïò§Î•ò - {e}")

        return test_results

    def generate_optimization_recommendations(
        self, current_performance, optimized_performance
    ):
        """ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        print("\nüí° ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë...")

        recommendations = []

        # ÏÑ±Îä• ÎπÑÍµê Î∂ÑÏÑù
        for query_name in current_performance.keys():
            if query_name in optimized_performance:
                current_time = current_performance[query_name]["execution_time"]
                optimized_time = optimized_performance[query_name]["execution_time"]

                if current_time > 0 and optimized_time > 0:
                    improvement = (current_time - optimized_time) / current_time * 100

                    if improvement > 10:
                        recommendations.append(
                            {
                                "query": query_name,
                                "improvement": f"{improvement:.1f}%",
                                "current_time": f"{current_time:.2f}Ï¥à",
                                "optimized_time": f"{optimized_time:.2f}Ï¥à",
                                "recommendation": "ÏøºÎ¶¨ ÏµúÏ†ÅÌôî Ìö®Í≥ºÏ†Å",
                            }
                        )
                    elif improvement < -10:
                        recommendations.append(
                            {
                                "query": query_name,
                                "improvement": f"{improvement:.1f}%",
                                "current_time": f"{current_time:.2f}Ï¥à",
                                "optimized_time": f"{optimized_time:.2f}Ï¥à",
                                "recommendation": "ÏøºÎ¶¨ ÏµúÏ†ÅÌôî Ïû¨Í≤ÄÌÜ† ÌïÑÏöî",
                            }
                        )

        # ÏùºÎ∞òÏ†ÅÏù∏ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠
        general_recommendations = [
            "1. ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî ÏøºÎ¶¨Îäî Materialized ViewÎ°ú Ï†ÄÏû•ÌïòÏó¨ ÏÑ±Îä• Ìñ•ÏÉÅ",
            "2. WHERE Ï†àÏóê ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî Ïª¨ÎüºÏúºÎ°ú Clustering ÏÑ§Ï†ï",
            "3. ÎÇ†ÏßúÎ≥ÑÎ°ú PartitioningÌïòÏó¨ ÏøºÎ¶¨ Î≤îÏúÑ Ï†úÌïú",
            "4. SELECT * ÎåÄÏã† ÌïÑÏöîÌïú Ïª¨ÎüºÎßå ÏÑ†ÌÉùÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏÜ°Îüâ Í∞êÏÜå",
            "5. TABLESAMPLEÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞ ÏÉòÌîåÎßÅ",
            "6. ÏúàÎèÑÏö∞ Ìï®ÏàòÎ•º ÌôúÏö©ÌïòÏó¨ Î≥µÏû°Ìïú ÏßëÍ≥Ñ Ïó∞ÏÇ∞ ÏµúÏ†ÅÌôî",
            "7. Ï†ÅÏ†àÌïú LIMIT Ï†à ÏÇ¨Ïö©ÏúºÎ°ú Í≤∞Í≥º ÌÅ¨Í∏∞ Ï†úÌïú",
        ]

        return recommendations, general_recommendations

    def create_optimization_report(
        self,
        current_performance,
        optimized_performance,
        recommendations,
        general_recommendations,
    ):
        """ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        print("\nüìã ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï§ë...")

        report = {
            "timestamp": datetime.now().isoformat(),
            "project_id": self.project_id,
            "current_performance": current_performance,
            "optimized_performance": optimized_performance,
            "recommendations": recommendations,
            "general_recommendations": general_recommendations,
            "summary": {
                "total_queries_tested": len(current_performance),
                "successful_optimizations": len(
                    [r for r in recommendations if "Ìö®Í≥ºÏ†Å" in r["recommendation"]]
                ),
                "average_improvement": (
                    np.mean(
                        [
                            float(r["improvement"].replace("%", ""))
                            for r in recommendations
                            if "Ìö®Í≥ºÏ†Å" in r["recommendation"]
                        ]
                    )
                    if recommendations
                    else 0
                ),
            },
        }

        # JSON Î≥¥Í≥†ÏÑú Ï†ÄÏû•
        with open(
            f"{self.results_dir}/optimization_report.json", "w", encoding="utf-8"
        ) as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # ÌÖçÏä§Ìä∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        with open(
            f"{self.results_dir}/optimization_summary.txt", "w", encoding="utf-8"
        ) as f:
            f.write("BigQuery ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ÏÉùÏÑ± ÏãúÍ∞Ñ: {report['timestamp']}\n")
            f.write(f"ÌîÑÎ°úÏ†ùÌä∏ ID: {report['project_id']}\n\n")

            f.write("ÏÑ±Îä• Í∞úÏÑ† ÏöîÏïΩ:\n")
            f.write(
                f"- ÌÖåÏä§Ìä∏Îêú ÏøºÎ¶¨ Ïàò: {report['summary']['total_queries_tested']}\n"
            )
            f.write(
                f"- ÏÑ±Í≥µÏ†ÅÏù∏ ÏµúÏ†ÅÌôî: {report['summary']['successful_optimizations']}\n"
            )
            f.write(
                f"- ÌèâÍ∑† Í∞úÏÑ†Ïú®: {report['summary']['average_improvement']:.1f}%\n\n"
            )

            f.write("ÏøºÎ¶¨Î≥Ñ ÏÑ±Îä• ÎπÑÍµê:\n")
            for rec in recommendations:
                f.write(
                    f"- {rec['query']}: {rec['improvement']} Í∞úÏÑ† ({rec['current_time']} ‚Üí {rec['optimized_time']})\n"
                )

            f.write("\nÏùºÎ∞òÏ†ÅÏù∏ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠:\n")
            for rec in general_recommendations:
                f.write(f"{rec}\n")

        print(f"‚úÖ ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú Ï†ÄÏû•: {self.results_dir}/optimization_report.json")
        print(f"‚úÖ ÏöîÏïΩ Î≥¥Í≥†ÏÑú Ï†ÄÏû•: {self.results_dir}/optimization_summary.txt")

    def run_optimization_analysis(self):
        """ÏµúÏ†ÅÌôî Î∂ÑÏÑù Ïã§Ìñâ"""
        print("üöÄ BigQuery ÏµúÏ†ÅÌôî Î∂ÑÏÑù ÏãúÏûë")
        print("=" * 60)

        # 1. ÌòÑÏû¨ ÏÑ±Îä• Î∂ÑÏÑù
        current_performance = self.analyze_current_performance()

        # 2. ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÉùÏÑ±
        optimized_queries = self.create_optimized_queries()

        # 3. ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÌÖåÏä§Ìä∏
        optimized_performance = self.test_optimized_queries(optimized_queries)

        # 4. Materialized Views ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî ÏÉùÏÑ±ÌïòÏßÄ ÏïäÍ≥† SQLÎßå Ï†úÍ≥µ)
        materialized_views = self.create_materialized_views()

        # 5. ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
        recommendations, general_recommendations = (
            self.generate_optimization_recommendations(
                current_performance, optimized_performance
            )
        )

        # 6. ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        self.create_optimization_report(
            current_performance,
            optimized_performance,
            recommendations,
            general_recommendations,
        )

        print("\nüéâ BigQuery ÏµúÏ†ÅÌôî Î∂ÑÏÑù ÏôÑÎ£å!")
        print(f"   Í≤∞Í≥º Ï†ÄÏû•: {self.results_dir}/")

        return {
            "current_performance": current_performance,
            "optimized_performance": optimized_performance,
            "recommendations": recommendations,
            "materialized_views": materialized_views,
        }


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("üß† BigQuery ÏµúÏ†ÅÌôî Ï†ÑÎûµ ÏãúÏä§ÌÖú")

    optimizer = BigQueryOptimizationStrategy()
    results = optimizer.run_optimization_analysis()

    print("\nüéâ BigQuery ÏµúÏ†ÅÌôî ÏôÑÎ£å!")
    print(f"   Í≤∞Í≥º Ï†ÄÏû•: {optimizer.results_dir}/")


if __name__ == "__main__":
    main()
