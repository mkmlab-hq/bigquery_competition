import json
import os
import warnings

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from google.cloud import bigquery
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader

from transfer_learning_multimodal import (
    TransferLearningMultimodalDataset,
    TransferLearningMultimodalNet,
    TransferLearningTrainer,
)

warnings.filterwarnings("ignore")


class BigQueryDataLoader:
    """BigQuery ë°ì´í„° ë¡œë”"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âš ï¸ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            print("ëŒ€ì²´ ë°ì´í„° ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.client = None

    def load_competition_data(self, limit: int = 10000) -> dict:
        """ëŒ€íšŒ ë°ì´í„° ë¡œë“œ"""
        if self.client is None:
            print("BigQuery í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_fallback_data(limit)

        print(f"BigQueryì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘... (ì œí•œ: {limit}ê°œ)")

        try:
            # Big5 ë°ì´í„°
            big5_query = f"""
            SELECT 
                openness, conscientiousness, extraversion, agreeableness, neuroticism
            FROM `{self.project_id}.persona_diary.big5_scores`
            LIMIT {limit}
            """

            # CMI ë°ì´í„°
            cmi_query = f"""
            SELECT 
                cmi_1, cmi_2, cmi_3, cmi_4, cmi_5, cmi_6, cmi_7, cmi_8, cmi_9, cmi_10,
                cmi_11, cmi_12, cmi_13, cmi_14, cmi_15, cmi_16, cmi_17, cmi_18, cmi_19, cmi_20
            FROM `{self.project_id}.persona_diary.cmi_scores`
            LIMIT {limit}
            """

            # RPPG ë°ì´í„°
            rppg_query = f"""
            SELECT 
                rppg_1, rppg_2, rppg_3, rppg_4, rppg_5, rppg_6, rppg_7, rppg_8, rppg_9, rppg_10
            FROM `{self.project_id}.persona_diary.rppg_features`
            LIMIT {limit}
            """

            # Voice ë°ì´í„°
            voice_query = f"""
            SELECT 
                voice_1, voice_2, voice_3, voice_4, voice_5, voice_6, voice_7, voice_8, voice_9, voice_10,
                voice_11, voice_12, voice_13, voice_14, voice_15, voice_16, voice_17, voice_18, voice_19, voice_20
            FROM `{self.project_id}.persona_diary.voice_features`
            LIMIT {limit}
            """

            # íƒ€ê²Ÿ ë°ì´í„°
            target_query = f"""
            SELECT 
                target_score
            FROM `{self.project_id}.persona_diary.target_scores`
            LIMIT {limit}
            """

            # ë°ì´í„° ë¡œë“œ
            big5_df = self.client.query(big5_query).to_dataframe()
            cmi_df = self.client.query(cmi_query).to_dataframe()
            rppg_df = self.client.query(rppg_query).to_dataframe()
            voice_df = self.client.query(voice_query).to_dataframe()
            target_df = self.client.query(target_query).to_dataframe()

            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
            print(f"  Big5: {len(big5_df)}ê°œ")
            print(f"  CMI: {len(cmi_df)}ê°œ")
            print(f"  RPPG: {len(rppg_df)}ê°œ")
            print(f"  Voice: {len(voice_df)}ê°œ")
            print(f"  Target: {len(target_df)}ê°œ")

            return {
                "big5": big5_df.values,
                "cmi": cmi_df.values,
                "rppg": rppg_df.values,
                "voice": voice_df.values,
                "targets": target_df.values.flatten(),
            }

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_fallback_data(limit)

    def _generate_fallback_data(self, limit: int) -> dict:
        """ëŒ€ì²´ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜)"""
        print("ğŸ“Š ëŒ€ì²´ ë°ì´í„° ìƒì„± ì¤‘...")

        # ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ìƒì„±
        np.random.seed(42)

        # Big5 ë°ì´í„° (0-1 ë²”ìœ„)
        big5_data = np.random.beta(2, 2, (limit, 5))

        # CMI ë°ì´í„° (0-1 ë²”ìœ„)
        cmi_data = np.random.beta(1.5, 1.5, (limit, 20))

        # RPPG ë°ì´í„° (ì •ê·œí™”ëœ ìƒì²´ì‹ í˜¸)
        rppg_data = np.random.normal(0, 1, (limit, 10))

        # Voice ë°ì´í„° (ì •ê·œí™”ëœ ìŒì„± íŠ¹ì„±)
        voice_data = np.random.normal(0, 1, (limit, 20))

        # íƒ€ê²Ÿ ìƒì„± (ë” ë³µì¡í•œ ê´€ê³„)
        targets = (
            0.25 * big5_data[:, 0]  # Openness
            + 0.20 * big5_data[:, 1]  # Conscientiousness
            + 0.15 * big5_data[:, 2]  # Extraversion
            + 0.20 * big5_data[:, 3]  # Agreeableness
            + 0.10 * big5_data[:, 4]  # Neuroticism
            + 0.05 * np.mean(cmi_data, axis=1)
            + 0.03 * np.mean(rppg_data, axis=1)
            + 0.02 * np.mean(voice_data, axis=1)
            + np.random.normal(0, 0.15, limit)  # ë” ë§ì€ ë…¸ì´ì¦ˆ
        )

        # íƒ€ê²Ÿ ì •ê·œí™” (0-1 ë²”ìœ„)
        targets = (targets - targets.min()) / (targets.max() - targets.min())

        print(f"âœ… ëŒ€ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ: {limit}ê°œ ìƒ˜í”Œ")

        return {
            "big5": big5_data,
            "cmi": cmi_data,
            "rppg": rppg_data,
            "voice": voice_data,
            "targets": targets,
        }


def test_transfer_model_on_real_data():
    """ì‹¤ì œ ë°ì´í„°ì—ì„œ ì „ì´ í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    print("ğŸ§ª ì‹¤ì œ ë°ì´í„°ì—ì„œ ì „ì´ í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘!")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # BigQuery ë°ì´í„° ë¡œë“œ
    data_loader = BigQueryDataLoader()
    data = data_loader.load_competition_data(limit=5000)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5000ê°œ

    # ë°ì´í„° ë¶„í• 
    from sklearn.model_selection import train_test_split

    # ì¸ë±ìŠ¤ë¡œ ë¶„í• 
    train_idx, test_idx = train_test_split(
        range(len(data["targets"])), test_size=0.3, random_state=42
    )
    train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42)

    # ë°ì´í„° ë¶„í• 
    X_train = {
        modality: data[modality][train_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }
    X_val = {
        modality: data[modality][val_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }
    X_test = {
        modality: data[modality][test_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }

    y_train = data["targets"][train_idx]
    y_val = data["targets"][val_idx]
    y_test = data["targets"][test_idx]

    print(f"í›ˆë ¨ ë°ì´í„°: {len(y_train)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(y_val)}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(y_test)}ê°œ")

    # ë°ì´í„° ì •ê·œí™”
    scalers = {}
    for modality in ["big5", "cmi", "rppg", "voice"]:
        scaler = StandardScaler()
        X_train[modality] = scaler.fit_transform(X_train[modality])
        X_val[modality] = scaler.transform(X_val[modality])
        X_test[modality] = scaler.transform(X_test[modality])
        scalers[modality] = scaler

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = TransferLearningMultimodalDataset(
        X_train["big5"],
        X_train["cmi"],
        X_train["rppg"],
        X_train["voice"],
        y_train,
        augment=True,
    )
    val_dataset = TransferLearningMultimodalDataset(
        X_val["big5"], X_val["cmi"], X_val["rppg"], X_val["voice"], y_val, augment=False
    )
    test_dataset = TransferLearningMultimodalDataset(
        X_test["big5"],
        X_test["cmi"],
        X_test["rppg"],
        X_test["voice"],
        y_test,
        augment=False,
    )

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")

    try:
        checkpoint = torch.load("transfer_learning_model.pth", map_location=device)
        best_params = checkpoint["best_params"]

        # ëª¨ë¸ ìƒì„±
        model = TransferLearningMultimodalNet(
            hidden_dim=best_params["hidden_dim"],
            dropout_rate=best_params["dropout_rate"],
            use_pretrained=False,  # ì´ë¯¸ ë¡œë“œí•  ì˜ˆì •
        )

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        print("âœ… ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    except Exception as e:
        print(f"âš ï¸ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        print("ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ìƒˆ ëª¨ë¸ ìƒì„±
        model = TransferLearningMultimodalNet(
            hidden_dim=256, dropout_rate=0.3, use_pretrained=False
        )

    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = TransferLearningTrainer(model, device)

    # íŒŒì¸íŠœë‹ (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •)
    print("ğŸ”§ ì‹¤ì œ ë°ì´í„°ì— íŒŒì¸íŠœë‹ ì¤‘...")

    final_r2 = trainer.train(
        train_loader,
        val_loader,
        epochs=20,  # ë¹ ë¥¸ íŒŒì¸íŠœë‹
        lr=0.0001,  # ë‚®ì€ í•™ìŠµë¥ 
        weight_decay=1e-4,
        patience=5,
    )

    # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€...")
    test_metrics = trainer.evaluate(test_loader, nn.MSELoss())

    print("\nğŸ“Š ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  ê²€ì¦ RÂ²: {final_r2:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ RÂ²: {test_metrics['r2']:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ RMSE: {test_metrics['rmse']:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ MAE: {test_metrics['mae']:.4f}")

    # ê²°ê³¼ ì €ì¥
    results = {
        "real_data_test": {
            "val_r2": final_r2,
            "test_r2": test_metrics["r2"],
            "test_rmse": test_metrics["rmse"],
            "test_mae": test_metrics["mae"],
            "data_source": "BigQuery" if data_loader.client else "Simulated",
            "sample_count": len(data["targets"]),
        }
    }

    with open("real_data_test_results.json", "w") as f:
        json.dump(results, f, indent=2)

    print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
    print("ğŸ“ real_data_test_results.json")

    return results


if __name__ == "__main__":
    results = test_transfer_model_on_real_data()


