#!/usr/bin/env python3
"""
ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ - RÂ² 0.999+ ìˆ˜ì¹˜ì˜ ì‹ ë¢°ì„± ê²€ì¦
- íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ê³¼ì •ì—ì„œì˜ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„± ê²€ì¦
- ì‹¤ì œ ì˜ˆì¸¡ ê°€ëŠ¥ì„± vs ë°ì´í„° ëˆ„ì¶œ êµ¬ë¶„
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class SuspiciousPerformanceAnalyzer:
    """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(
        self, limit: int = 10000
    ) -> Tuple[Dict, np.ndarray, Dict]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© (íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ê³¼ì • í¬í•¨)"""
        print("ğŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ê³¼ì • ë¶„ì„
            print("ğŸ” íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ê³¼ì • ë¶„ì„ ì¤‘...")

            # Big5 ì ìˆ˜ ê³„ì‚°
            big5_scores = {
                "EXT": big5_df[["EXT1", "EXT2", "EXT3", "EXT4", "EXT5"]].mean(axis=1),
                "EST": big5_df[["EST1", "EST2", "EST3", "EST4", "EST5"]].mean(axis=1),
                "AGR": big5_df[["AGR1", "AGR2", "AGR3", "AGR4", "AGR5"]].mean(axis=1),
                "CSN": big5_df[["CSN1", "CSN2", "CSN3", "CSN4", "CSN5"]].mean(axis=1),
                "OPN": big5_df[["OPN1", "OPN2", "OPN3", "OPN4", "OPN5"]].mean(axis=1),
            }

            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„!)
            targets = (
                big5_scores["EXT"] * 0.25
                + big5_scores["OPN"] * 0.20
                + (6 - big5_scores["EST"]) * 0.15
                + big5_scores["AGR"] * 0.15
                + big5_scores["CSN"] * 0.10
                + (cmi_numeric.mean(axis=1) / 6) * 0.10
                + (rppg_numeric.mean(axis=1) / 6) * 0.05
            )

            # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
            targets = (targets - targets.min()) / (
                targets.max() - targets.min()
            ) * 9 + 1

            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì •ë³´
            target_info = {
                "big5_scores": {k: v.tolist() for k, v in big5_scores.items()},
                "cmi_mean": cmi_numeric.mean(axis=1).tolist(),
                "rppg_mean": rppg_numeric.mean(axis=1).tolist(),
                "targets_raw": targets.tolist(),
                "target_stats": {
                    "mean": float(targets.mean()),
                    "std": float(targets.std()),
                    "min": float(targets.min()),
                    "max": float(targets.max()),
                },
            }

            print(f"âœ… ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")

            return multimodal_data, targets, target_info

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def analyze_target_leakage(
        self, multimodal_data: Dict, targets: np.ndarray, target_info: Dict
    ) -> Dict:
        """íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ë¶„ì„"""
        print("ğŸ” íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ë¶„ì„ ì¤‘...")

        # 1. íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ì…ë ¥ íŠ¹ì„± ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
        X = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # 2. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ (ê°„ë‹¨í•œ ë°©ë²•)
        leakage_analysis = {}

        # Big5ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„
        big5_corrs = []
        for i in range(multimodal_data["big5"].shape[1]):
            corr = np.corrcoef(multimodal_data["big5"][:, i], targets)[0, 1]
            if not np.isnan(corr):
                big5_corrs.append(abs(corr))
        big5_max_corr = max(big5_corrs) if big5_corrs else 0

        # CMIì™€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„
        cmi_corrs = []
        for i in range(multimodal_data["cmi"].shape[1]):
            corr = np.corrcoef(multimodal_data["cmi"][:, i], targets)[0, 1]
            if not np.isnan(corr):
                cmi_corrs.append(abs(corr))
        cmi_max_corr = max(cmi_corrs) if cmi_corrs else 0

        # RPPGì™€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„
        rppg_corrs = []
        for i in range(multimodal_data["rppg"].shape[1]):
            corr = np.corrcoef(multimodal_data["rppg"][:, i], targets)[0, 1]
            if not np.isnan(corr):
                rppg_corrs.append(abs(corr))
        rppg_max_corr = max(rppg_corrs) if rppg_corrs else 0

        # Voiceì™€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„
        voice_corrs = []
        for i in range(multimodal_data["voice"].shape[1]):
            corr = np.corrcoef(multimodal_data["voice"][:, i], targets)[0, 1]
            if not np.isnan(corr):
                voice_corrs.append(abs(corr))
        voice_max_corr = max(voice_corrs) if voice_corrs else 0

        leakage_analysis = {
            "big5_max_correlation": float(big5_max_corr),
            "cmi_max_correlation": float(cmi_max_corr),
            "rppg_max_correlation": float(rppg_max_corr),
            "voice_max_correlation": float(voice_max_corr),
            "overall_max_correlation": float(
                np.max([big5_max_corr, cmi_max_corr, rppg_max_corr, voice_max_corr])
            ),
            "leakage_risk": (
                "HIGH"
                if np.max([big5_max_corr, cmi_max_corr, rppg_max_corr, voice_max_corr])
                > 0.9
                else (
                    "MEDIUM"
                    if np.max(
                        [big5_max_corr, cmi_max_corr, rppg_max_corr, voice_max_corr]
                    )
                    > 0.7
                    else "LOW"
                )
            ),
        }

        print(f"   Big5 ìµœëŒ€ ìƒê´€ê´€ê³„: {big5_max_corr:.4f}")
        print(f"   CMI ìµœëŒ€ ìƒê´€ê´€ê³„: {cmi_max_corr:.4f}")
        print(f"   RPPG ìµœëŒ€ ìƒê´€ê´€ê³„: {rppg_max_corr:.4f}")
        print(f"   Voice ìµœëŒ€ ìƒê´€ê´€ê³„: {voice_max_corr:.4f}")
        print(
            f"   ì „ì²´ ìµœëŒ€ ìƒê´€ê´€ê³„: {leakage_analysis['overall_max_correlation']:.4f}"
        )
        print(f"   ëˆ„ì¶œ ìœ„í—˜ë„: {leakage_analysis['leakage_risk']}")

        return leakage_analysis

    def test_realistic_prediction(
        self, multimodal_data: Dict, targets: np.ndarray
    ) -> Dict:
        """í˜„ì‹¤ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” í˜„ì‹¤ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")

        # 1. ë°ì´í„° ì¤€ë¹„
        X = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # 2. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, targets, test_size=0.3, random_state=42
        )

        # 3. ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LinearRegression

        # Linear Regression (ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸)
        lr = LinearRegression()
        lr.fit(X_train, y_train)
        lr_pred = lr.predict(X_test)
        lr_r2 = r2_score(y_test, lr_pred)

        # Random Forest (ê³¼ì í•© ë°©ì§€)
        rf = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42,
        )
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        rf_r2 = r2_score(y_test, rf_pred)

        realistic_test = {
            "linear_regression_r2": float(lr_r2),
            "random_forest_r2": float(rf_r2),
            "is_realistic": lr_r2 < 0.8 and rf_r2 < 0.9,  # í˜„ì‹¤ì ì¸ ë²”ìœ„
            "suspicious": lr_r2 > 0.95 or rf_r2 > 0.95,  # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë²”ìœ„
        }

        print(f"   Linear Regression RÂ²: {lr_r2:.4f}")
        print(f"   Random Forest RÂ²: {rf_r2:.4f}")
        print(f"   í˜„ì‹¤ì  ì„±ëŠ¥: {realistic_test['is_realistic']}")
        print(f"   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥: {realistic_test['suspicious']}")

        return realistic_test

    def analyze_target_construction(self, target_info: Dict) -> Dict:
        """íƒ€ê²Ÿ ë³€ìˆ˜ êµ¬ì„± ë¶„ì„"""
        print("ğŸ” íƒ€ê²Ÿ ë³€ìˆ˜ êµ¬ì„± ë¶„ì„ ì¤‘...")

        # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ ë¶„ì„
        construction_analysis = {
            "big5_contribution": 0.25 + 0.20 + 0.15 + 0.15 + 0.10,  # 0.85 (85%)
            "cmi_contribution": 0.10,  # 10%
            "rppg_contribution": 0.05,  # 5%
            "big5_dominant": True,  # Big5ê°€ 85% ì°¨ì§€
            "construction_method": "weighted_average",
            "potential_leakage": "Big5 ë°ì´í„°ê°€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ 85%ë¥¼ ì°¨ì§€í•˜ì—¬ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„± ë†’ìŒ",
        }

        print(f"   Big5 ê¸°ì—¬ë„: {construction_analysis['big5_contribution']:.1%}")
        print(f"   CMI ê¸°ì—¬ë„: {construction_analysis['cmi_contribution']:.1%}")
        print(f"   RPPG ê¸°ì—¬ë„: {construction_analysis['rppg_contribution']:.1%}")
        print(f"   Big5 ì§€ë°°ì : {construction_analysis['big5_dominant']}")
        print(f"   ì ì¬ì  ëˆ„ì¶œ: {construction_analysis['potential_leakage']}")

        return construction_analysis

    def run_suspicious_analysis(self, limit: int = 10000) -> Dict:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets, target_info = self.load_real_bigquery_data(limit)

        # 2. íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ë¶„ì„
        leakage_analysis = self.analyze_target_leakage(
            multimodal_data, targets, target_info
        )

        # 3. í˜„ì‹¤ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        realistic_test = self.test_realistic_prediction(multimodal_data, targets)

        # 4. íƒ€ê²Ÿ ë³€ìˆ˜ êµ¬ì„± ë¶„ì„
        construction_analysis = self.analyze_target_construction(target_info)

        # 5. ê²°ê³¼ ì¢…í•©
        results = {
            "leakage_analysis": leakage_analysis,
            "realistic_test": realistic_test,
            "construction_analysis": construction_analysis,
            "target_info": target_info,
            "conclusion": {
                "is_suspicious": leakage_analysis["leakage_risk"] == "HIGH"
                or realistic_test["suspicious"],
                "main_issue": "íƒ€ê²Ÿ ë³€ìˆ˜ê°€ Big5 ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ ê³„ì‚°ë˜ì–´ ë°ì´í„° ëˆ„ì¶œ ë°œìƒ",
                "recommendation": "ë…ë¦½ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ì‚¬ìš© í•„ìš”",
            },
        }

        # 6. ê²°ê³¼ ì €ì¥
        with open("suspicious_performance_analysis.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
        print(f"   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥: {results['conclusion']['is_suspicious']}")
        print(f"   ì£¼ìš” ë¬¸ì œ: {results['conclusion']['main_issue']}")
        print(f"   ê¶Œì¥ì‚¬í•­: {results['conclusion']['recommendation']}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ - RÂ² 0.999+ ê²€ì¦")
    print("=" * 60)

    analyzer = SuspiciousPerformanceAnalyzer()
    results = analyzer.run_suspicious_analysis(limit=10000)

    print("\nğŸ“Š ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼:")
    print(f"   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥: {results['conclusion']['is_suspicious']}")
    print(f"   ì£¼ìš” ë¬¸ì œ: {results['conclusion']['main_issue']}")
    print(f"   ê¶Œì¥ì‚¬í•­: {results['conclusion']['recommendation']}")


if __name__ == "__main__":
    main()
