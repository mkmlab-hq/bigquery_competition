#!/usr/bin/env python3
"""
ìµœì¢… ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ - BigQuery ëŒ€íšŒ ìƒìœ„ê¶Œ ì§„ì…
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
- í”¼ì²˜ ì„ íƒ ìµœì í™”
- ì•™ìƒë¸” ì„±ëŠ¥ ê·¹ëŒ€í™”
- ëŒ€íšŒ ì œì¶œ ì¤€ë¹„
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class FinalOptimizer:
    """ìµœì¢… ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}
        self.ensemble_weights = None
        self.best_models = []
        self.cv_scores = {}
        self.feature_selector = None

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            print("ğŸ” ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # Big5 ì ìˆ˜ ê³„ì‚°
            big5_scores = {
                "EXT": big5_numeric[["EXT1", "EXT2", "EXT3", "EXT4", "EXT5"]].mean(
                    axis=1
                ),
                "EST": big5_numeric[["EST1", "EST2", "EST3", "EST4", "EST5"]].mean(
                    axis=1
                ),
                "AGR": big5_numeric[["AGR1", "AGR2", "AGR3", "AGR4", "AGR5"]].mean(
                    axis=1
                ),
                "CSN": big5_numeric[["CSN1", "CSN2", "CSN3", "CSN4", "CSN5"]].mean(
                    axis=1
                ),
                "OPN": big5_numeric[["OPN1", "OPN2", "OPN3", "OPN4", "OPN5"]].mean(
                    axis=1
                ),
            }

            # ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ (ê°€ì¤‘ì¹˜ ì¡°ì •)
            targets = (
                big5_scores["EXT"] * 0.30  # ê°€ì¤‘ì¹˜ ì¦ê°€
                + big5_scores["OPN"] * 0.25  # ê°€ì¤‘ì¹˜ ì¦ê°€
                + (6 - big5_scores["EST"]) * 0.20  # ESTëŠ” ì—­ì½”ë”©, ê°€ì¤‘ì¹˜ ì¦ê°€
                + big5_scores["AGR"] * 0.15
                + big5_scores["CSN"] * 0.10
                + (cmi_numeric.mean(axis=1) / 6) * 0.05  # CMI ê°€ì¤‘ì¹˜ ê°ì†Œ
                + (rppg_numeric.mean(axis=1) / 6) * 0.03  # RPPG ê°€ì¤‘ì¹˜ ê°ì†Œ
                + (voice_numeric.mean(axis=1) / 6) * 0.02  # Voice ê°€ì¤‘ì¹˜ ê°ì†Œ
            )

            print(f"âœ… ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")
            print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
            print(f"     í‰ê· : {targets.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {targets.std():.4f}")
            print(f"     ìµœì†Œê°’: {targets.min():.4f}")
            print(f"     ìµœëŒ€ê°’: {targets.max():.4f}")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_optimized_models(self):
        """ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=200,  # íŠ¸ë¦¬ ìˆ˜ ì¦ê°€
                max_depth=12,  # ê¹Šì´ ì¦ê°€
                min_samples_split=15,  # ìµœì í™”
                min_samples_leaf=8,  # ìµœì í™”
                max_features="sqrt",  # í”¼ì²˜ ì„ íƒ ìµœì í™”
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=200,  # íŠ¸ë¦¬ ìˆ˜ ì¦ê°€
                learning_rate=0.05,  # í•™ìŠµë¥  ê°ì†Œ
                max_depth=6,  # ê¹Šì´ ì¦ê°€
                min_samples_split=15,
                subsample=0.9,  # ì„œë¸Œìƒ˜í”Œë§ ì¶”ê°€
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.9,
                colsample_bytree=0.9,
                reg_alpha=0.1,  # L1 ì •ê·œí™” ì¶”ê°€
                reg_lambda=0.1,  # L2 ì •ê·œí™” ì¶”ê°€
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.9,
                colsample_bytree=0.9,
                reg_alpha=0.1,  # L1 ì •ê·œí™” ì¶”ê°€
                reg_lambda=0.1,  # L2 ì •ê·œí™” ì¶”ê°€
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=1.0),  # ì •ê·œí™” ê°•ë„ ì¡°ì •
            "elastic_net": ElasticNet(alpha=0.01, l1_ratio=0.7),  # ì •ê·œí™” ê°•ë„ ì¡°ì •
            "svr": SVR(kernel="rbf", C=1.0, gamma="auto"),  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        }

        print(f"âœ… {len(self.models)}ê°œ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_optimized_data(
        self, multimodal_data: Dict, targets: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ê²°í•©
        X = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        # RobustScalerë¡œ ì •ê·œí™”
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["robust_ensemble"] = scaler

        # í”¼ì²˜ ì„ íƒ (ìƒìœ„ 150ê°œ íŠ¹ì„± ì„ íƒ)
        print("ğŸ” í”¼ì²˜ ì„ íƒ ì¤‘...")
        self.feature_selector = SelectKBest(score_func=f_regression, k=150)
        X_selected = self.feature_selector.fit_transform(X_scaled, targets)

        print(f"âœ… ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_selected.shape}")
        print(f"   ì›ë³¸ íŠ¹ì„± ìˆ˜: {X_scaled.shape[1]}")
        print(f"   ì„ íƒëœ íŠ¹ì„± ìˆ˜: {X_selected.shape[1]}")

        return X_selected, targets

    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")

        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        from sklearn.model_selection import train_test_split

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        optimized_models = {}

        # Random Forest ìµœì í™”
        print("   Random Forest ìµœì í™” ì¤‘...")
        rf_param_grid = {
            "n_estimators": [150, 200, 250],
            "max_depth": [10, 12, 15],
            "min_samples_split": [10, 15, 20],
        }
        rf_grid = GridSearchCV(
            RandomForestRegressor(random_state=42, n_jobs=-1),
            rf_param_grid,
            cv=3,
            scoring="r2",
            n_jobs=-1,
        )
        rf_grid.fit(X_train, y_train)
        optimized_models["random_forest"] = rf_grid.best_estimator_
        print(f"     ìµœì  íŒŒë¼ë¯¸í„°: {rf_grid.best_params_}")
        print(f"     ìµœì  ì ìˆ˜: {rf_grid.best_score_:.4f}")

        # XGBoost ìµœì í™”
        print("   XGBoost ìµœì í™” ì¤‘...")
        xgb_param_grid = {
            "n_estimators": [150, 200, 250],
            "learning_rate": [0.03, 0.05, 0.07],
            "max_depth": [5, 6, 7],
        }
        xgb_grid = GridSearchCV(
            XGBRegressor(random_state=42, n_jobs=-1),
            xgb_param_grid,
            cv=3,
            scoring="r2",
            n_jobs=-1,
        )
        xgb_grid.fit(X_train, y_train)
        optimized_models["xgboost"] = xgb_grid.best_estimator_
        print(f"     ìµœì  íŒŒë¼ë¯¸í„°: {xgb_grid.best_params_}")
        print(f"     ìµœì  ì ìˆ˜: {xgb_grid.best_score_:.4f}")

        # LightGBM ìµœì í™”
        print("   LightGBM ìµœì í™” ì¤‘...")
        lgb_param_grid = {
            "n_estimators": [150, 200, 250],
            "learning_rate": [0.03, 0.05, 0.07],
            "max_depth": [5, 6, 7],
        }
        lgb_grid = GridSearchCV(
            LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
            lgb_param_grid,
            cv=3,
            scoring="r2",
            n_jobs=-1,
        )
        lgb_grid.fit(X_train, y_train)
        optimized_models["lightgbm"] = lgb_grid.best_estimator_
        print(f"     ìµœì  íŒŒë¼ë¯¸í„°: {lgb_grid.best_params_}")
        print(f"     ìµœì  ì ìˆ˜: {lgb_grid.best_score_:.4f}")

        # Ridge ìµœì í™”
        print("   Ridge ìµœì í™” ì¤‘...")
        ridge_param_grid = {
            "alpha": [0.1, 1.0, 10.0, 100.0],
        }
        ridge_grid = GridSearchCV(
            Ridge(), ridge_param_grid, cv=3, scoring="r2", n_jobs=-1
        )
        ridge_grid.fit(X_train, y_train)
        optimized_models["ridge"] = ridge_grid.best_estimator_
        print(f"     ìµœì  íŒŒë¼ë¯¸í„°: {ridge_grid.best_params_}")
        print(f"     ìµœì  ì ìˆ˜: {ridge_grid.best_score_:.4f}")

        print(f"âœ… {len(optimized_models)}ê°œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ")
        return optimized_models

    def train_optimized_models(
        self, X: np.ndarray, y: np.ndarray, optimized_models: Dict
    ) -> Dict:
        """ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸš€ ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        model_scores = {}
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        for name, model in optimized_models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                cv_r2_scores = cross_val_score(
                    model, X, y, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model, X, y, cv=kf, scoring="neg_root_mean_squared_error", n_jobs=-1
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ ë°ì´í„°)
                model.fit(X, y)

                model_scores[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "model": model,
                }

                print(f"     RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f}), RMSE: {avg_rmse:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_scores[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
        valid_models = {k: v for k, v in model_scores.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(), key=lambda x: x[1]["cv_mean_r2"], reverse=True
        )

        print(f"âœ… {len(valid_models)}ê°œ ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ìµœì í™”ëœ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (êµì°¨ ê²€ì¦ RÂ² ê¸°ì¤€):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(
                f"   {i}. {name}: RÂ² = {scores['cv_mean_r2']:.4f} (Â±{scores['cv_std_r2']:.4f})"
            )

        self.cv_scores = model_scores
        return model_scores

    def create_final_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_scores: Dict
    ) -> Dict:
        """ìµœì¢… ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ”„ ìµœì¢… ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 5ê°œ ëª¨ë¸ ì„ íƒ
        valid_models = {k: v for k, v in model_scores.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(), key=lambda x: x[1]["cv_mean_r2"], reverse=True
        )
        top_models = [name for name, _ in sorted_models[:5]]

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        predictions = []
        weights = []

        for name in top_models:
            if name in model_scores and model_scores[name] is not None:
                model = model_scores[name]["model"]
                pred = model.predict(X)
                predictions.append(pred)
                # RÂ² ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
                weights.append(model_scores[name]["cv_mean_r2"])

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()
        self.ensemble_weights = dict(zip(top_models, weights))

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y, ensemble_pred)
        mse = mean_squared_error(y, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y))
        correlation = np.corrcoef(ensemble_pred, y)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y,
            "ensemble_weights": self.ensemble_weights,
            "selected_models": top_models,
        }

        print(f"âœ… ìµœì¢… ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_final_optimization(self, limit: int = 10000) -> Dict:
        """ìµœì¢… ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ìµœì¢… ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_real_bigquery_data(limit)

        # 2. ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±
        self.create_optimized_models()

        # 3. ìµœì í™”ëœ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_optimized_data(multimodal_data, targets)

        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        optimized_models = self.optimize_hyperparameters(X, y)

        # 5. ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨
        model_scores = self.train_optimized_models(X, y, optimized_models)

        # 6. ìµœì¢… ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_final_ensemble(X, y, model_scores)

        # 7. ê²°ê³¼ ì €ì¥
        results = {
            "individual_models_cv_scores": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "cv_mean_rmse": scores["cv_mean_rmse"] if scores else None,
                }
                for name, scores in model_scores.items()
            },
            "ensemble_results": ensemble_results,
            "ensemble_weights": self.ensemble_weights,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": multimodal_data["big5"].shape[1]
                + multimodal_data["cmi"].shape[1]
                + multimodal_data["rppg"].shape[1]
                + multimodal_data["voice"].shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_scores.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "optimization_status": "ì™„ë£Œ",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("final_optimization_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ìµœì¢… ëª¨ë¸ ìµœì í™” ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì¢… ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ - BigQuery ëŒ€íšŒ ìƒìœ„ê¶Œ ì§„ì…")
    print("=" * 60)

    optimizer = FinalOptimizer()
    results = optimizer.run_final_optimization(limit=10000)

    print("\nğŸ“Š ìµœì¢… ìµœì í™” ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")


if __name__ == "__main__":
    main()
