#!/usr/bin/env python3
"""
í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ (Persona Discovery Operation)
- ì˜ˆì¸¡ ëª¨ë¸ â†’ ë¹„ì§€ë„í•™ìŠµ ì „í™˜
- íšŒê·€ â†’ í´ëŸ¬ìŠ¤í„°ë§ ì „í™˜
- ì„±ëŠ¥ â†’ ì¸ì‚¬ì´íŠ¸ ì „í™˜
- BigQuery ëŒ€íšŒ ê·œì • ì™„ì „ ì¤€ìˆ˜
"""

import json
import os
import warnings
from typing import Any, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
from google.cloud import bigquery
from plotly.subplots import make_subplots
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class PersonaDiscoveryOperation:
    """í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ - í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì‚¬ìš©ì ê·¸ë£¹ ë¶„ì„"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_multimodal_data(self, limit: int = 10000) -> Dict[str, np.ndarray]:
        """ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”© ì¤‘...")

        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()
            voice_numeric = voice_df.select_dtypes(include=[np.number])

            multimodal_data = {
                "big5": big5_numeric.values.astype(np.float64),
                "cmi": cmi_numeric.values.astype(np.float64),
                "rppg": rppg_numeric.values.astype(np.float64),
                "voice": voice_numeric.values.astype(np.float64),
            }

            print(f"âœ… ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")

            return multimodal_data

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_unified_latent_space(
        self, multimodal_data: Dict[str, np.ndarray]
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """í†µí•© ì ì¬ ê³µê°„ ìƒì„±"""
        print("ğŸ”§ í†µí•© ì ì¬ ê³µê°„ ìƒì„± ì¤‘...")

        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í†µê³„ íŠ¹ì„± ì¶”ì¶œ
        features = []
        modality_info = []

        for modality_name, data in multimodal_data.items():
            print(f"   íŠ¹ì„± ì¶”ì¶œ ì¤‘: {modality_name}")

            # ê¸°ë³¸ í†µê³„ íŠ¹ì„±
            mean_features = np.mean(data, axis=1, keepdims=True)
            std_features = np.std(data, axis=1, keepdims=True)
            max_features = np.max(data, axis=1, keepdims=True)
            min_features = np.min(data, axis=1, keepdims=True)
            median_features = np.median(data, axis=1, keepdims=True)

            # ê³ ê¸‰ í†µê³„ íŠ¹ì„±
            q25_features = np.percentile(data, 25, axis=1, keepdims=True)
            q75_features = np.percentile(data, 75, axis=1, keepdims=True)
            range_features = max_features - min_features
            iqr_features = q75_features - q25_features

            # ëª¨ë‹¬ë¦¬í‹°ë³„ íŠ¹ì„± ê²°í•©
            modality_features = np.concatenate(
                [
                    mean_features,
                    std_features,
                    max_features,
                    min_features,
                    median_features,
                    q25_features,
                    q75_features,
                    range_features,
                    iqr_features,
                ],
                axis=1,
            )

            features.append(modality_features)
            modality_info.extend([modality_name] * modality_features.shape[1])

            print(f"     {modality_name} íŠ¹ì„± ìˆ˜: {modality_features.shape[1]}")

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° íŠ¹ì„± ê²°í•©
        X_combined = np.concatenate(features, axis=1)

        # ë°ì´í„° ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_combined)

        # PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(50, X_scaled.shape[1]))
        X_pca = pca.fit_transform(X_scaled)

        # t-SNEë¥¼ í†µí•œ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ
        if X_scaled.shape[0] > 1000:
            # í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ìƒ˜í”Œë§
            sample_indices = np.random.choice(X_scaled.shape[0], 1000, replace=False)
            X_sample = X_scaled[sample_indices]
        else:
            X_sample = X_scaled
            sample_indices = np.arange(X_scaled.shape[0])

        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        X_tsne = tsne.fit_transform(X_sample)

        latent_space_info = {
            "original_features": X_combined.shape[1],
            "pca_components": X_pca.shape[1],
            "pca_explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
            "pca_cumulative_variance": np.cumsum(
                pca.explained_variance_ratio_
            ).tolist(),
            "tsne_components": X_tsne.shape[0],
            "sample_indices": sample_indices.tolist(),
            "modality_info": modality_info,
            "scaler": scaler,
            "pca": pca,
            "tsne": tsne,
        }

        print(f"âœ… í†µí•© ì ì¬ ê³µê°„ ìƒì„± ì™„ë£Œ:")
        print(f"   ì›ë³¸ íŠ¹ì„± ìˆ˜: {X_combined.shape[1]}")
        print(f"   PCA ì°¨ì›: {X_pca.shape[1]}")
        print(f"   PCA ì„¤ëª… ë¶„ì‚°: {pca.explained_variance_ratio_[:5]}")
        print(f"   t-SNE ìƒ˜í”Œ ìˆ˜: {X_tsne.shape[0]}")

        return X_pca, latent_space_info

    def discover_personas(
        self, X_latent: np.ndarray, latent_space_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ ë°œê²¬ (í´ëŸ¬ìŠ¤í„°ë§)"""
        print("ğŸ¯ í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì¤‘...")

        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (K-Means)
        k_range = range(2, 11)
        silhouette_scores = []
        inertias = []

        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_latent)
            silhouette_avg = silhouette_score(X_latent, cluster_labels)
            silhouette_scores.append(silhouette_avg)
            inertias.append(kmeans.inertia_)

        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ
        optimal_k = k_range[np.argmax(silhouette_scores)]
        print(
            f"   ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k} (ì‹¤ë£¨ì—£ ì ìˆ˜: {max(silhouette_scores):.4f})"
        )

        # K-Means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        kmeans_labels = kmeans.fit_predict(X_latent)

        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ë¹„ì„ í˜• í´ëŸ¬ìŠ¤í„° íƒì§€)
        # ìµœì  eps ì°¾ê¸°
        neighbors = NearestNeighbors(n_neighbors=min(20, X_latent.shape[0] // 10))
        neighbors_fit = neighbors.fit(X_latent)
        distances, indices = neighbors_fit.kneighbors(X_latent)
        distances = np.sort(distances[:, -1], axis=0)

        # ì—˜ë³´ìš° ë°©ë²•ìœ¼ë¡œ ìµœì  eps ì°¾ê¸°
        eps_candidates = np.linspace(distances[0], distances[-1], 20)
        dbscan_scores = []

        for eps in eps_candidates:
            dbscan = DBSCAN(eps=eps, min_samples=5)
            dbscan_labels = dbscan.fit_predict(X_latent)
            n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
            if n_clusters > 1:
                silhouette_avg = silhouette_score(X_latent, dbscan_labels)
                dbscan_scores.append((eps, silhouette_avg, n_clusters))
            else:
                dbscan_scores.append((eps, 0, n_clusters))

        # ìµœì  DBSCAN íŒŒë¼ë¯¸í„°
        if dbscan_scores:
            best_eps, best_score, best_n_clusters = max(
                dbscan_scores, key=lambda x: x[1]
            )
            dbscan = DBSCAN(eps=best_eps, min_samples=5)
            dbscan_labels = dbscan.fit_predict(X_latent)
            print(
                f"   DBSCAN ìµœì  eps: {best_eps:.4f} (í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_n_clusters}, ì‹¤ë£¨ì—£ ì ìˆ˜: {best_score:.4f})"
            )
        else:
            dbscan_labels = np.zeros(X_latent.shape[0])
            print("   DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ - ëª¨ë“  ì ì„ ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜")

        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í†µí•©
        clustering_results = {
            "kmeans": {
                "labels": kmeans_labels.tolist(),
                "n_clusters": optimal_k,
                "silhouette_score": max(silhouette_scores),
                "inertia": kmeans.inertia_,
                "centers": kmeans.cluster_centers_.tolist(),
            },
            "dbscan": {
                "labels": dbscan_labels.tolist(),
                "n_clusters": len(set(dbscan_labels))
                - (1 if -1 in dbscan_labels else 0),
                "silhouette_score": best_score if dbscan_scores else 0,
                "eps": best_eps if dbscan_scores else 0,
                "noise_points": int(np.sum(dbscan_labels == -1)),
            },
            "optimal_k": optimal_k,
            "silhouette_scores": silhouette_scores,
            "inertias": inertias,
            "k_range": list(k_range),
        }

        print(f"âœ… í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì™„ë£Œ:")
        print(f"   K-Means í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}")
        print(
            f"   DBSCAN í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}"
        )
        print(f"   ë…¸ì´ì¦ˆ ì  ìˆ˜: {int(np.sum(dbscan_labels == -1))}")

        return clustering_results

    def profile_personas(
        self, multimodal_data: Dict[str, np.ndarray], clustering_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§"""
        print("ğŸ‘¥ í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§ ì¤‘...")

        kmeans_labels = np.array(clustering_results["kmeans"]["labels"])
        dbscan_labels = np.array(clustering_results["dbscan"]["labels"])

        persona_profiles = {}

        # K-Means ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§
        for cluster_id in range(clustering_results["kmeans"]["n_clusters"]):
            cluster_mask = kmeans_labels == cluster_id
            cluster_size = np.sum(cluster_mask)

            if cluster_size == 0:
                continue

            persona_profile = {
                "cluster_id": cluster_id,
                "size": int(cluster_size),
                "percentage": float(cluster_size / len(kmeans_labels) * 100),
                "modality_profiles": {},
            }

            # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ íŠ¹ì„± ë¶„ì„
            for modality_name, data in multimodal_data.items():
                cluster_data = data[cluster_mask]

                modality_profile = {
                    "mean": np.mean(cluster_data, axis=0).tolist(),
                    "std": np.std(cluster_data, axis=0).tolist(),
                    "median": np.median(cluster_data, axis=0).tolist(),
                    "min": np.min(cluster_data, axis=0).tolist(),
                    "max": np.max(cluster_data, axis=0).tolist(),
                    "shape": cluster_data.shape,
                }

                # ëª¨ë‹¬ë¦¬í‹°ë³„ ëŒ€í‘œ íŠ¹ì„± (í‰ê· )
                modality_mean = np.mean(cluster_data, axis=1)
                modality_profile["overall_mean"] = float(np.mean(modality_mean))
                modality_profile["overall_std"] = float(np.std(modality_mean))

                persona_profile["modality_profiles"][modality_name] = modality_profile

            # í˜ë¥´ì†Œë‚˜ íŠ¹ì„± ì •ì˜
            persona_characteristics = self._define_persona_characteristics(
                persona_profile
            )
            persona_profile["characteristics"] = persona_characteristics

            persona_profiles[f"persona_{cluster_id}"] = persona_profile

        # DBSCAN ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§ (ë…¸ì´ì¦ˆ ì œì™¸)
        unique_dbscan_labels = [label for label in set(dbscan_labels) if label != -1]

        for cluster_id in unique_dbscan_labels:
            cluster_mask = dbscan_labels == cluster_id
            cluster_size = np.sum(cluster_mask)

            if cluster_size < 10:  # ë„ˆë¬´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ì œì™¸
                continue

            persona_profile = {
                "cluster_id": int(cluster_id),
                "size": int(cluster_size),
                "percentage": float(cluster_size / len(dbscan_labels) * 100),
                "modality_profiles": {},
                "method": "dbscan",
            }

            # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ íŠ¹ì„± ë¶„ì„
            for modality_name, data in multimodal_data.items():
                cluster_data = data[cluster_mask]

                modality_profile = {
                    "mean": np.mean(cluster_data, axis=0).tolist(),
                    "std": np.std(cluster_data, axis=0).tolist(),
                    "median": np.median(cluster_data, axis=0).tolist(),
                    "min": np.min(cluster_data, axis=0).tolist(),
                    "max": np.max(cluster_data, axis=0).tolist(),
                    "shape": cluster_data.shape,
                }

                # ëª¨ë‹¬ë¦¬í‹°ë³„ ëŒ€í‘œ íŠ¹ì„± (í‰ê· )
                modality_mean = np.mean(cluster_data, axis=1)
                modality_profile["overall_mean"] = float(np.mean(modality_mean))
                modality_profile["overall_std"] = float(np.std(modality_mean))

                persona_profile["modality_profiles"][modality_name] = modality_profile

            # í˜ë¥´ì†Œë‚˜ íŠ¹ì„± ì •ì˜
            persona_characteristics = self._define_persona_characteristics(
                persona_profile
            )
            persona_profile["characteristics"] = persona_characteristics

            persona_profiles[f"dbscan_persona_{cluster_id}"] = persona_profile

        print(f"âœ… í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ:")
        print(
            f"   K-Means í˜ë¥´ì†Œë‚˜ ìˆ˜: {len([k for k in persona_profiles.keys() if k.startswith('persona_')])}"
        )
        print(
            f"   DBSCAN í˜ë¥´ì†Œë‚˜ ìˆ˜: {len([k for k in persona_profiles.keys() if k.startswith('dbscan_persona_')])}"
        )

        return persona_profiles

    def _define_persona_characteristics(
        self, persona_profile: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ íŠ¹ì„± ì •ì˜"""
        characteristics = {
            "persona_name": "",
            "description": "",
            "key_traits": [],
            "risk_level": "unknown",
            "health_status": "unknown",
            "personality_type": "unknown",
        }

        # Big5 íŠ¹ì„± ë¶„ì„
        if "big5" in persona_profile["modality_profiles"]:
            big5_profile = persona_profile["modality_profiles"]["big5"]
            big5_mean = big5_profile["overall_mean"]

            # Big5 ê¸°ë°˜ ì„±ê²© ìœ í˜• ì¶”ì •
            if big5_mean > 0.6:
                characteristics["personality_type"] = "high_engagement"
            elif big5_mean < 0.4:
                characteristics["personality_type"] = "low_engagement"
            else:
                characteristics["personality_type"] = "moderate_engagement"

        # CMI íŠ¹ì„± ë¶„ì„
        if "cmi" in persona_profile["modality_profiles"]:
            cmi_profile = persona_profile["modality_profiles"]["cmi"]
            cmi_mean = cmi_profile["overall_mean"]

            if cmi_mean > 0.7:
                characteristics["health_status"] = "high_risk"
                characteristics["key_traits"].append("high_stress")
            elif cmi_mean < 0.3:
                characteristics["health_status"] = "low_risk"
                characteristics["key_traits"].append("low_stress")
            else:
                characteristics["health_status"] = "moderate_risk"

        # RPPG íŠ¹ì„± ë¶„ì„
        if "rppg" in persona_profile["modality_profiles"]:
            rppg_profile = persona_profile["modality_profiles"]["rppg"]
            rppg_mean = rppg_profile["overall_mean"]

            if rppg_mean > 0.6:
                characteristics["key_traits"].append("high_physiological_activity")
            elif rppg_mean < 0.4:
                characteristics["key_traits"].append("low_physiological_activity")

        # Voice íŠ¹ì„± ë¶„ì„
        if "voice" in persona_profile["modality_profiles"]:
            voice_profile = persona_profile["modality_profiles"]["voice"]
            voice_mean = voice_profile["overall_mean"]

            if voice_mean > 0.6:
                characteristics["key_traits"].append("high_vocal_activity")
            elif voice_mean < 0.4:
                characteristics["key_traits"].append("low_vocal_activity")

        # í˜ë¥´ì†Œë‚˜ ì´ë¦„ ìƒì„±
        if (
            characteristics["health_status"] == "high_risk"
            and characteristics["personality_type"] == "high_engagement"
        ):
            characteristics["persona_name"] = "ê³ ìœ„í—˜ ê³ í™œë™í˜•"
            characteristics["description"] = (
                "ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ë†’ì€ ì°¸ì—¬ë„ë¥¼ ë³´ì´ëŠ” ì‚¬ìš©ì ê·¸ë£¹"
            )
        elif (
            characteristics["health_status"] == "low_risk"
            and characteristics["personality_type"] == "low_engagement"
        ):
            characteristics["persona_name"] = "ì €ìœ„í—˜ ì €í™œë™í˜•"
            characteristics["description"] = (
                "ë‚®ì€ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ë‚®ì€ ì°¸ì—¬ë„ë¥¼ ë³´ì´ëŠ” ì‚¬ìš©ì ê·¸ë£¹"
            )
        elif (
            characteristics["health_status"] == "moderate_risk"
            and characteristics["personality_type"] == "moderate_engagement"
        ):
            characteristics["persona_name"] = "ê· í˜•í˜•"
            characteristics["description"] = (
                "ì¤‘ê°„ ìˆ˜ì¤€ì˜ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ì°¸ì—¬ë„ë¥¼ ë³´ì´ëŠ” ì‚¬ìš©ì ê·¸ë£¹"
            )
        else:
            characteristics["persona_name"] = "í˜¼í•©í˜•"
            characteristics["description"] = "ë³µí•©ì  íŠ¹ì„±ì„ ë³´ì´ëŠ” ì‚¬ìš©ì ê·¸ë£¹"

        return characteristics

    def create_visualizations(
        self,
        X_latent: np.ndarray,
        latent_space_info: Dict[str, Any],
        clustering_results: Dict[str, Any],
        persona_profiles: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì‹œê°í™” ìë£Œ ìƒì„±"""
        print("ğŸ“Š ì‹œê°í™” ìë£Œ ìƒì„± ì¤‘...")

        visualizations = {}

        # 1. PCA 2D ì‹œê°í™”
        if X_latent.shape[1] >= 2:
            plt.figure(figsize=(12, 8))

            # K-Means í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
            plt.subplot(2, 2, 1)
            kmeans_labels = np.array(clustering_results["kmeans"]["labels"])
            scatter = plt.scatter(
                X_latent[:, 0],
                X_latent[:, 1],
                c=kmeans_labels,
                cmap="viridis",
                alpha=0.6,
            )
            plt.title("K-Means í´ëŸ¬ìŠ¤í„°ë§ (PCA)")
            plt.xlabel("PC1")
            plt.ylabel("PC2")
            plt.colorbar(scatter)

            # DBSCAN í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
            plt.subplot(2, 2, 2)
            dbscan_labels = np.array(clustering_results["dbscan"]["labels"])
            scatter = plt.scatter(
                X_latent[:, 0],
                X_latent[:, 1],
                c=dbscan_labels,
                cmap="viridis",
                alpha=0.6,
            )
            plt.title("DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (PCA)")
            plt.xlabel("PC1")
            plt.ylabel("PC2")
            plt.colorbar(scatter)

            # ì‹¤ë£¨ì—£ ì ìˆ˜ ë¹„êµ
            plt.subplot(2, 2, 3)
            k_range = clustering_results["k_range"]
            silhouette_scores = clustering_results["silhouette_scores"]
            plt.plot(k_range, silhouette_scores, "bo-")
            plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
            plt.ylabel("ì‹¤ë£¨ì—£ ì ìˆ˜")
            plt.title("K-Means ì‹¤ë£¨ì—£ ì ìˆ˜")
            plt.grid(True)

            # ì—˜ë³´ìš° ê³¡ì„ 
            plt.subplot(2, 2, 4)
            inertias = clustering_results["inertias"]
            plt.plot(k_range, inertias, "ro-")
            plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
            plt.ylabel("Inertia")
            plt.title("K-Means ì—˜ë³´ìš° ê³¡ì„ ")
            plt.grid(True)

            plt.tight_layout()
            plt.savefig(
                "persona_discovery_clustering.png", dpi=300, bbox_inches="tight"
            )
            plt.close()

            visualizations["clustering_plot"] = "persona_discovery_clustering.png"

        # 2. t-SNE ì‹œê°í™”
        if "sample_indices" in latent_space_info:
            sample_indices = latent_space_info["sample_indices"]
            X_tsne = latent_space_info["tsne"].embedding_

            plt.figure(figsize=(15, 5))

            # t-SNE + K-Means
            plt.subplot(1, 3, 1)
            sample_kmeans_labels = kmeans_labels[sample_indices]
            scatter = plt.scatter(
                X_tsne[:, 0],
                X_tsne[:, 1],
                c=sample_kmeans_labels,
                cmap="viridis",
                alpha=0.6,
            )
            plt.title("t-SNE + K-Means í´ëŸ¬ìŠ¤í„°ë§")
            plt.xlabel("t-SNE 1")
            plt.ylabel("t-SNE 2")
            plt.colorbar(scatter)

            # t-SNE + DBSCAN
            plt.subplot(1, 3, 2)
            sample_dbscan_labels = dbscan_labels[sample_indices]
            scatter = plt.scatter(
                X_tsne[:, 0],
                X_tsne[:, 1],
                c=sample_dbscan_labels,
                cmap="viridis",
                alpha=0.6,
            )
            plt.title("t-SNE + DBSCAN í´ëŸ¬ìŠ¤í„°ë§")
            plt.xlabel("t-SNE 1")
            plt.ylabel("t-SNE 2")
            plt.colorbar(scatter)

            # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            plt.subplot(1, 3, 3)
            kmeans_counts = np.bincount(kmeans_labels)
            dbscan_counts = np.bincount(dbscan_labels[dbscan_labels >= 0])

            x_pos = np.arange(len(kmeans_counts))
            plt.bar(x_pos - 0.2, kmeans_counts, 0.4, label="K-Means", alpha=0.7)
            if len(dbscan_counts) > 0:
                plt.bar(
                    x_pos[: len(dbscan_counts)] + 0.2,
                    dbscan_counts,
                    0.4,
                    label="DBSCAN",
                    alpha=0.7,
                )
            plt.xlabel("í´ëŸ¬ìŠ¤í„° ID")
            plt.ylabel("í´ëŸ¬ìŠ¤í„° í¬ê¸°")
            plt.title("í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬")
            plt.legend()
            plt.grid(True, alpha=0.3)

            plt.tight_layout()
            plt.savefig("persona_discovery_tsne.png", dpi=300, bbox_inches="tight")
            plt.close()

            visualizations["tsne_plot"] = "persona_discovery_tsne.png"

        # 3. í˜ë¥´ì†Œë‚˜ íŠ¹ì„± íˆíŠ¸ë§µ
        if persona_profiles:
            plt.figure(figsize=(15, 10))

            # K-Means í˜ë¥´ì†Œë‚˜ë“¤ë§Œ ì„ íƒ
            kmeans_personas = {
                k: v for k, v in persona_profiles.items() if k.startswith("persona_")
            }

            if kmeans_personas:
                # ê° í˜ë¥´ì†Œë‚˜ì˜ ëª¨ë‹¬ë¦¬í‹°ë³„ í‰ê· ê°’ ì¶”ì¶œ
                persona_matrix = []
                persona_names = []

                for persona_name, persona_data in kmeans_personas.items():
                    persona_names.append(
                        persona_data["characteristics"]["persona_name"]
                    )
                    persona_row = []

                    for modality in ["big5", "cmi", "rppg", "voice"]:
                        if modality in persona_data["modality_profiles"]:
                            persona_row.append(
                                persona_data["modality_profiles"][modality][
                                    "overall_mean"
                                ]
                            )
                        else:
                            persona_row.append(0)

                    persona_matrix.append(persona_row)

                persona_matrix = np.array(persona_matrix)

                # íˆíŠ¸ë§µ ìƒì„±
                plt.subplot(2, 2, 1)
                sns.heatmap(
                    persona_matrix,
                    xticklabels=["Big5", "CMI", "RPPG", "Voice"],
                    yticklabels=persona_names,
                    annot=True,
                    fmt=".3f",
                    cmap="RdYlBu_r",
                )
                plt.title("í˜ë¥´ì†Œë‚˜ë³„ ëª¨ë‹¬ë¦¬í‹° íŠ¹ì„± íˆíŠ¸ë§µ")
                plt.xlabel("ëª¨ë‹¬ë¦¬í‹°")
                plt.ylabel("í˜ë¥´ì†Œë‚˜")

                # í˜ë¥´ì†Œë‚˜ í¬ê¸° ë¶„í¬
                plt.subplot(2, 2, 2)
                persona_sizes = [
                    persona_data["size"] for persona_data in kmeans_personas.values()
                ]
                persona_labels = [
                    persona_data["characteristics"]["persona_name"]
                    for persona_data in kmeans_personas.values()
                ]
                plt.pie(
                    persona_sizes,
                    labels=persona_labels,
                    autopct="%1.1f%%",
                    startangle=90,
                )
                plt.title("í˜ë¥´ì†Œë‚˜ í¬ê¸° ë¶„í¬")

                # ëª¨ë‹¬ë¦¬í‹°ë³„ ë¶„ì‚°
                plt.subplot(2, 2, 3)
                modality_vars = np.var(persona_matrix, axis=0)
                plt.bar(["Big5", "CMI", "RPPG", "Voice"], modality_vars)
                plt.title("ëª¨ë‹¬ë¦¬í‹°ë³„ ë¶„ì‚°")
                plt.ylabel("ë¶„ì‚°")
                plt.xticks(rotation=45)

                # í˜ë¥´ì†Œë‚˜ë³„ íŠ¹ì„± ìš”ì•½
                plt.subplot(2, 2, 4)
                persona_summary = []
                for persona_data in kmeans_personas.values():
                    summary = f"{persona_data['characteristics']['persona_name']}\n"
                    summary += f"í¬ê¸°: {persona_data['size']} ({persona_data['percentage']:.1f}%)\n"
                    summary += f"íŠ¹ì„±: {', '.join(persona_data['characteristics']['key_traits'])}"
                    persona_summary.append(summary)

                plt.text(
                    0.1,
                    0.9,
                    "\n\n".join(persona_summary),
                    transform=plt.gca().transAxes,
                    fontsize=10,
                    verticalalignment="top",
                    bbox=dict(boxstyle="round", facecolor="lightblue", alpha=0.5),
                )
                plt.axis("off")
                plt.title("í˜ë¥´ì†Œë‚˜ ìš”ì•½")

            plt.tight_layout()
            plt.savefig("persona_discovery_analysis.png", dpi=300, bbox_inches="tight")
            plt.close()

            visualizations["analysis_plot"] = "persona_discovery_analysis.png"

        print(f"âœ… ì‹œê°í™” ìë£Œ ìƒì„± ì™„ë£Œ:")
        for plot_name, plot_file in visualizations.items():
            print(f"   {plot_name}: {plot_file}")

        return visualizations

    def run_persona_discovery_operation(self, limit: int = 10000) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ ì‹¤í–‰"""
        print("ğŸš€ í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ ì‹œì‘")
        print("=" * 60)
        print("ğŸ¯ ëª©í‘œ: ì˜ˆì¸¡ â†’ ë°œê²¬, íšŒê·€ â†’ í´ëŸ¬ìŠ¤í„°ë§, ì„±ëŠ¥ â†’ ì¸ì‚¬ì´íŠ¸")

        # 1. ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ë¡œë”©
        multimodal_data = self.load_multimodal_data(limit)

        # 2. í†µí•© ì ì¬ ê³µê°„ ìƒì„±
        X_latent, latent_space_info = self.create_unified_latent_space(multimodal_data)

        # 3. í˜ë¥´ì†Œë‚˜ ë°œê²¬ (í´ëŸ¬ìŠ¤í„°ë§)
        clustering_results = self.discover_personas(X_latent, latent_space_info)

        # 4. í˜ë¥´ì†Œë‚˜ í”„ë¡œíŒŒì¼ë§
        persona_profiles = self.profile_personas(multimodal_data, clustering_results)

        # 5. ì‹œê°í™” ìë£Œ ìƒì„±
        visualizations = self.create_visualizations(
            X_latent, latent_space_info, clustering_results, persona_profiles
        )

        # 6. ê²°ê³¼ í†µí•©
        results = {
            "operation_summary": {
                "total_samples": len(multimodal_data["big5"]),
                "n_features_original": latent_space_info["original_features"],
                "n_features_pca": latent_space_info["pca_components"],
                "kmeans_clusters": clustering_results["kmeans"]["n_clusters"],
                "dbscan_clusters": clustering_results["dbscan"]["n_clusters"],
                "n_personas_discovered": len(persona_profiles),
                "visualizations_created": len(visualizations),
            },
            "latent_space_info": latent_space_info,
            "clustering_results": clustering_results,
            "persona_profiles": persona_profiles,
            "visualizations": visualizations,
            "insights": self._generate_insights(persona_profiles, clustering_results),
        }

        # 7. ê²°ê³¼ ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´ë§Œ ì €ì¥)
        json_safe_results = self._convert_to_json_serializable(results)
        with open("persona_discovery_operation_results.json", "w") as f:
            json.dump(json_safe_results, f, indent=2)

        print("âœ… í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ ì™„ë£Œ!")
        print(f"   ë°œê²¬ëœ í˜ë¥´ì†Œë‚˜ ìˆ˜: {len(persona_profiles)}")
        print(f"   ìƒì„±ëœ ì‹œê°í™”: {len(visualizations)}")
        print(f"   ê²°ê³¼ íŒŒì¼: persona_discovery_operation_results.json")

        return results

    def _generate_insights(
        self, persona_profiles: Dict[str, Any], clustering_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {"key_findings": [], "persona_summary": {}, "recommendations": []}

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        kmeans_personas = {
            k: v for k, v in persona_profiles.items() if k.startswith("persona_")
        }
        dbscan_personas = {
            k: v for k, v in persona_profiles.items() if k.startswith("dbscan_persona_")
        }

        insights["key_findings"].append(f"ì´ {len(persona_profiles)}ê°œì˜ í˜ë¥´ì†Œë‚˜ ë°œê²¬")
        insights["key_findings"].append(
            f"K-Means: {len(kmeans_personas)}ê°œ, DBSCAN: {len(dbscan_personas)}ê°œ"
        )

        # ê°€ì¥ í° í˜ë¥´ì†Œë‚˜ ì‹ë³„
        if kmeans_personas:
            largest_persona = max(kmeans_personas.values(), key=lambda x: x["size"])
            insights["key_findings"].append(
                f"ê°€ì¥ í° í˜ë¥´ì†Œë‚˜: {largest_persona['characteristics']['persona_name']} ({largest_persona['size']}ëª…, {largest_persona['percentage']:.1f}%)"
            )

        # í˜ë¥´ì†Œë‚˜ ìš”ì•½
        for persona_name, persona_data in kmeans_personas.items():
            insights["persona_summary"][
                persona_data["characteristics"]["persona_name"]
            ] = {
                "size": persona_data["size"],
                "percentage": persona_data["percentage"],
                "key_traits": persona_data["characteristics"]["key_traits"],
                "health_status": persona_data["characteristics"]["health_status"],
                "personality_type": persona_data["characteristics"]["personality_type"],
            }

        # ê¶Œì¥ì‚¬í•­
        insights["recommendations"].append("ê° í˜ë¥´ì†Œë‚˜ë³„ ë§ì¶¤í˜• ê°œì… ì „ëµ ìˆ˜ë¦½ í•„ìš”")
        insights["recommendations"].append(
            "ê³ ìœ„í—˜ í˜ë¥´ì†Œë‚˜ì— ëŒ€í•œ ìš°ì„ ì  ëª¨ë‹ˆí„°ë§ ê°•í™”"
        )
        insights["recommendations"].append(
            "í˜ë¥´ì†Œë‚˜ ê°„ ì „í™˜ íŒ¨í„´ ë¶„ì„ì„ í†µí•œ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ"
        )

        return insights

    def _convert_to_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´ë¡œ ë³€í™˜"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.float32):
            return float(obj)
        elif isinstance(obj, np.float64):
            return float(obj)
        elif isinstance(obj, np.int32):
            return int(obj)
        elif isinstance(obj, np.int64):
            return int(obj)
        elif isinstance(obj, bool):
            return bool(obj)
        elif isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif hasattr(obj, "__dict__"):
            # ê°ì²´ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(obj)
        else:
            return obj


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „")
    print("=" * 60)

    operation = PersonaDiscoveryOperation()
    results = operation.run_persona_discovery_operation(limit=10000)

    print("\nğŸ“Š í˜ë¥´ì†Œë‚˜ ë°œê²¬ ì‘ì „ ê²°ê³¼:")
    print(
        f"   ë°œê²¬ëœ í˜ë¥´ì†Œë‚˜ ìˆ˜: {results['operation_summary']['n_personas_discovered']}"
    )
    print(f"   K-Means í´ëŸ¬ìŠ¤í„°: {results['operation_summary']['kmeans_clusters']}")
    print(f"   DBSCAN í´ëŸ¬ìŠ¤í„°: {results['operation_summary']['dbscan_clusters']}")
    print(f"   ìƒì„±ëœ ì‹œê°í™”: {results['operation_summary']['visualizations_created']}")

    print("\nğŸ¯ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for finding in results["insights"]["key_findings"]:
        print(f"   â€¢ {finding}")


if __name__ == "__main__":
    main()
