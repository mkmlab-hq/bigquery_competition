#!/usr/bin/env python3
"""
ê³ ê¸‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„
- ìë™ ì•Œë¦¼ ë° ë³µêµ¬ ì‹œìŠ¤í…œ
- ëŒ€ì‹œë³´ë“œ ë° ì‹œê°í™”
"""

import json
import os
import time
import warnings
from collections import deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings("ignore", category=UserWarning)


class PerformanceMonitor:
    """ê³ ê¸‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        monitoring_window=1000,
        alert_threshold=0.1,
        recovery_threshold=0.2,
        save_interval=100,
    ):
        self.monitoring_window = monitoring_window
        self.alert_threshold = alert_threshold
        self.recovery_threshold = recovery_threshold
        self.save_interval = save_interval

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.metrics_history = deque(maxlen=monitoring_window)
        self.alerts_history = deque(maxlen=1000)
        self.recovery_attempts = deque(maxlen=100)

        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitoring_start_time = None
        self.last_save_time = time.time()

        # ì„±ëŠ¥ ê¸°ì¤€ì„ 
        self.baseline_metrics = {}
        self.performance_trends = {}

        print("ğŸ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.is_monitoring = True
        self.monitoring_start_time = time.time()
        print("ğŸš€ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")

    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        print("â¹ï¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")

    def record_metrics(
        self,
        model_name: str,
        metrics: Dict[str, float],
        additional_info: Optional[Dict] = None,
    ):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if not self.is_monitoring:
            return

        timestamp = datetime.now()
        record = {
            "timestamp": timestamp,
            "model_name": model_name,
            "metrics": metrics,
            "additional_info": additional_info or {},
        }

        self.metrics_history.append(record)

        # ì„±ëŠ¥ ë¶„ì„ ë° ì•Œë¦¼
        self._analyze_performance(record)

        # ì£¼ê¸°ì  ì €ì¥
        if len(self.metrics_history) % self.save_interval == 0:
            self._save_metrics()

    def _analyze_performance(self, record: Dict):
        """ì„±ëŠ¥ ë¶„ì„ ë° ì•Œë¦¼ ìƒì„±"""
        if len(self.metrics_history) < 10:
            return

        # ìµœê·¼ ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„
        recent_metrics = [r["metrics"] for r in list(self.metrics_history)[-10:]]
        self._analyze_trends(recent_metrics)

        # ì„±ëŠ¥ ì €í•˜ íƒì§€
        if self._detect_performance_degradation(record):
            self._generate_alert("PERFORMANCE_DEGRADATION", record)

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        if self._detect_memory_issues(record):
            self._generate_alert("MEMORY_ISSUE", record)

        # í•™ìŠµë¥  ëª¨ë‹ˆí„°ë§
        if self._detect_learning_rate_issues(record):
            self._generate_alert("LEARNING_RATE_ISSUE", record)

    def _analyze_trends(self, recent_metrics: List[Dict]):
        """ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„"""
        if len(recent_metrics) < 5:
            return

        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        mae_values = [m.get("mae", 0) for m in recent_metrics]
        mse_values = [m.get("mse", 0) for m in recent_metrics]
        loss_values = [m.get("loss", 0) for m in recent_metrics]

        # ì¶”ì„¸ ê³„ì‚°
        mae_trend = np.polyfit(range(len(mae_values)), mae_values, 1)[0]
        mse_trend = np.polyfit(range(len(mse_values)), mse_values, 1)[0]
        loss_trend = np.polyfit(range(len(loss_values)), loss_values, 1)[0]

        self.performance_trends = {
            "mae_trend": mae_trend,
            "mse_trend": mse_trend,
            "loss_trend": loss_trend,
            "overall_trend": (mae_trend + mse_trend + loss_trend) / 3,
        }

    def _detect_performance_degradation(self, record: Dict) -> bool:
        """ì„±ëŠ¥ ì €í•˜ íƒì§€"""
        if len(self.metrics_history) < 20:
            return False

        current_mae = record["metrics"].get("mae", 0)
        historical_mae = [r["metrics"].get("mae", 0) for r in list(self.metrics_history)[-20:]]

        # ê¸°ì¤€ì„  ì„¤ì • (ì²˜ìŒ 10ê°œ ë°ì´í„°ì˜ í‰ê· )
        baseline = np.mean(historical_mae[:10])
        current_avg = np.mean(historical_mae[-5:])

        # ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’ ì²´í¬
        degradation_ratio = current_avg / baseline if baseline > 0 else 1.0
        return degradation_ratio > (1 + self.alert_threshold)

    def _detect_memory_issues(self, record: Dict) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¬¸ì œ íƒì§€"""
        memory_usage = record.get("additional_info", {}).get("memory_usage", 0)
        return memory_usage > 0.9  # 90% ì´ìƒ ì‚¬ìš© ì‹œ ì•Œë¦¼

    def _detect_learning_rate_issues(self, record: Dict) -> bool:
        """í•™ìŠµë¥  ë¬¸ì œ íƒì§€"""
        learning_rate = record.get("additional_info", {}).get("learning_rate", 0)
        return learning_rate < 1e-6 or learning_rate > 1.0

    def _generate_alert(self, alert_type: str, record: Dict):
        """ì•Œë¦¼ ìƒì„±"""
        alert = {
            "timestamp": datetime.now(),
            "alert_type": alert_type,
            "severity": self._get_alert_severity(alert_type),
            "record": record,
            "recommendations": self._get_recommendations(alert_type),
        }

        self.alerts_history.append(alert)
        self._log_alert(alert)

    def _get_alert_severity(self, alert_type: str) -> str:
        """ì•Œë¦¼ ì‹¬ê°ë„ ê²°ì •"""
        severity_map = {
            "PERFORMANCE_DEGRADATION": "HIGH",
            "MEMORY_ISSUE": "CRITICAL",
            "LEARNING_RATE_ISSUE": "MEDIUM",
        }
        return severity_map.get(alert_type, "LOW")

    def _get_recommendations(self, alert_type: str) -> List[str]:
        """ì•Œë¦¼ë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = {
            "PERFORMANCE_DEGRADATION": [
                "í•™ìŠµë¥  ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”",
                "ëª¨ë¸ ë³µì¡ë„ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤",
                "ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•˜ì„¸ìš”",
                "ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš”",
            ],
            "MEMORY_ISSUE": [
                "ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”",
                "ëª¨ë¸ í¬ê¸°ë¥¼ ì¶•ì†Œí•˜ì„¸ìš”",
                "ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”",
                "GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”",
            ],
            "LEARNING_RATE_ISSUE": [
                "í•™ìŠµë¥ ì„ ì ì ˆí•œ ë²”ìœ„ë¡œ ì¡°ì •í•˜ì„¸ìš”",
                "ì ì‘í˜• í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
                "ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ì„ ì ìš©í•˜ì„¸ìš”",
            ],
        }
        return recommendations.get(alert_type, ["ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”"])

    def _log_alert(self, alert: Dict):
        """ì•Œë¦¼ ë¡œê¹…"""
        severity_emoji = {
            "CRITICAL": "ğŸš¨",
            "HIGH": "âš ï¸",
            "MEDIUM": "ğŸ”¶",
            "LOW": "â„¹ï¸",
        }

        emoji = severity_emoji.get(alert["severity"], "â„¹ï¸")
        print(f"{emoji} {alert['alert_type']} - {alert['severity']}")
        print(f"   ì‹œê°„: {alert['timestamp']}")
        print(f"   ê¶Œì¥ì‚¬í•­: {', '.join(alert['recommendations'])}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if not self.metrics_history:
            return {"status": "no_data"}

        recent_metrics = [r["metrics"] for r in list(self.metrics_history)[-10:]]
        all_metrics = [r["metrics"] for r in self.metrics_history]

        # ê¸°ë³¸ í†µê³„
        mae_values = [m.get("mae", 0) for m in all_metrics]
        mse_values = [m.get("mse", 0) for m in all_metrics]
        loss_values = [m.get("loss", 0) for m in all_metrics]

        summary = {
            "monitoring_duration": time.time() - self.monitoring_start_time if self.monitoring_start_time else 0,
            "total_records": len(self.metrics_history),
            "recent_performance": {
                "mae": {
                    "current": mae_values[-1] if mae_values else 0,
                    "average": np.mean(mae_values[-10:]) if len(mae_values) >= 10 else np.mean(mae_values),
                    "trend": self.performance_trends.get("mae_trend", 0),
                },
                "mse": {
                    "current": mse_values[-1] if mse_values else 0,
                    "average": np.mean(mse_values[-10:]) if len(mse_values) >= 10 else np.mean(mse_values),
                    "trend": self.performance_trends.get("mse_trend", 0),
                },
                "loss": {
                    "current": loss_values[-1] if loss_values else 0,
                    "average": np.mean(loss_values[-10:]) if len(loss_values) >= 10 else np.mean(loss_values),
                    "trend": self.performance_trends.get("loss_trend", 0),
                },
            },
            "alerts": {
                "total": len(self.alerts_history),
                "recent": len([a for a in self.alerts_history if (datetime.now() - a["timestamp"]).seconds < 3600]),
                "critical": len([a for a in self.alerts_history if a["severity"] == "CRITICAL"]),
            },
            "overall_trend": self.performance_trends.get("overall_trend", 0),
        }

        return summary

    def create_performance_dashboard(self, save_path: str = "performance_dashboard.png"):
        """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        if not self.metrics_history:
            print("âŒ í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # ë°ì´í„° ì¤€ë¹„
        timestamps = [r["timestamp"] for r in self.metrics_history]
        mae_values = [r["metrics"].get("mae", 0) for r in self.metrics_history]
        mse_values = [r["metrics"].get("mse", 0) for r in self.metrics_history]
        loss_values = [r["metrics"].get("loss", 0) for r in self.metrics_history]

        # ëŒ€ì‹œë³´ë“œ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle("ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ", fontsize=16, fontweight="bold")

        # 1. MAE ì¶”ì´
        axes[0, 0].plot(timestamps, mae_values, "b-", linewidth=2, label="MAE")
        axes[0, 0].set_title("Mean Absolute Error (MAE)")
        axes[0, 0].set_ylabel("MAE")
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].tick_params(axis="x", rotation=45)

        # 2. MSE ì¶”ì´
        axes[0, 1].plot(timestamps, mse_values, "r-", linewidth=2, label="MSE")
        axes[0, 1].set_title("Mean Squared Error (MSE)")
        axes[0, 1].set_ylabel("MSE")
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].tick_params(axis="x", rotation=45)

        # 3. Loss ì¶”ì´
        axes[1, 0].plot(timestamps, loss_values, "g-", linewidth=2, label="Loss")
        axes[1, 0].set_title("Training Loss")
        axes[1, 0].set_ylabel("Loss")
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].tick_params(axis="x", rotation=45)

        # 4. ì„±ëŠ¥ ìš”ì•½
        summary = self.get_performance_summary()
        axes[1, 1].axis("off")
        
        summary_text = f"""
ì„±ëŠ¥ ìš”ì•½:
â€¢ ëª¨ë‹ˆí„°ë§ ì‹œê°„: {summary['monitoring_duration']:.1f}ì´ˆ
â€¢ ì´ ê¸°ë¡ ìˆ˜: {summary['total_records']}
â€¢ í˜„ì¬ MAE: {summary['recent_performance']['mae']['current']:.4f}
â€¢ í˜„ì¬ MSE: {summary['recent_performance']['mse']['current']:.4f}
â€¢ í˜„ì¬ Loss: {summary['recent_performance']['loss']['current']:.4f}
â€¢ ì „ì²´ ì¶”ì„¸: {summary['overall_trend']:.4f}
â€¢ ì•Œë¦¼ ìˆ˜: {summary['alerts']['total']}
â€¢ ìµœê·¼ ì•Œë¦¼: {summary['alerts']['recent']}
â€¢ ì‹¬ê°í•œ ì•Œë¦¼: {summary['alerts']['critical']}
        """
        
        axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='top', fontfamily='monospace')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"ğŸ“Š ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ì €ì¥: {save_path}")

    def _save_metrics(self):
        """ë©”íŠ¸ë¦­ ì €ì¥"""
        if not self.metrics_history:
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_metrics_{timestamp}.json"

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        save_data = {
            "monitoring_info": {
                "start_time": self.monitoring_start_time,
                "total_records": len(self.metrics_history),
                "monitoring_window": self.monitoring_window,
            },
            "metrics_history": [
                {
                    "timestamp": r["timestamp"].isoformat(),
                    "model_name": r["model_name"],
                    "metrics": r["metrics"],
                    "additional_info": r["additional_info"],
                }
                for r in self.metrics_history
            ],
            "alerts_history": [
                {
                    "timestamp": a["timestamp"].isoformat(),
                    "alert_type": a["alert_type"],
                    "severity": a["severity"],
                    "recommendations": a["recommendations"],
                }
                for a in self.alerts_history
            ],
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ë©”íŠ¸ë¦­ ì €ì¥: {filename}")

    def export_performance_report(self, save_path: str = "performance_report.html"):
        """ì„±ëŠ¥ ë³´ê³ ì„œ HTML ë‚´ë³´ë‚´ê¸°"""
        if not self.metrics_history:
            print("âŒ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        summary = self.get_performance_summary()
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .metric {{ margin: 10px 0; padding: 10px; border-left: 4px solid #007acc; }}
                .alert {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ff6b6b; }}
                .summary {{ background-color: #e8f4fd; padding: 15px; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ</h1>
                <p>ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="summary">
                <h2>ğŸ“Š ì„±ëŠ¥ ìš”ì•½</h2>
                <p><strong>ëª¨ë‹ˆí„°ë§ ì‹œê°„:</strong> {summary['monitoring_duration']:.1f}ì´ˆ</p>
                <p><strong>ì´ ê¸°ë¡ ìˆ˜:</strong> {summary['total_records']}</p>
                <p><strong>í˜„ì¬ MAE:</strong> {summary['recent_performance']['mae']['current']:.4f}</p>
                <p><strong>í˜„ì¬ MSE:</strong> {summary['recent_performance']['mse']['current']:.4f}</p>
                <p><strong>ì „ì²´ ì¶”ì„¸:</strong> {summary['overall_trend']:.4f}</p>
            </div>
            
            <div class="metric">
                <h2>âš ï¸ ì•Œë¦¼ í˜„í™©</h2>
                <p><strong>ì´ ì•Œë¦¼ ìˆ˜:</strong> {summary['alerts']['total']}</p>
                <p><strong>ìµœê·¼ ì•Œë¦¼:</strong> {summary['alerts']['recent']}</p>
                <p><strong>ì‹¬ê°í•œ ì•Œë¦¼:</strong> {summary['alerts']['critical']}</p>
            </div>
        </body>
        </html>
        """

        with open(save_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        print(f"ğŸ“„ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥: {save_path}")


class ModelPerformanceTracker:
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ì¶”ì  ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.model_metrics = {}
        self.comparison_results = {}

    def track_model(self, model_name: str, metrics: Dict[str, float]):
        """ëª¨ë¸ ì„±ëŠ¥ ì¶”ì """
        if model_name not in self.model_metrics:
            self.model_metrics[model_name] = []

        self.model_metrics[model_name].append({
            "timestamp": datetime.now(),
            "metrics": metrics,
        })

    def compare_models(self, model_names: List[str]) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        if len(model_names) < 2:
            return {"error": "ìµœì†Œ 2ê°œ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤"}

        comparison = {}
        for model_name in model_names:
            if model_name not in self.model_metrics:
                continue

            model_data = self.model_metrics[model_name]
            if not model_data:
                continue

            # ìµœê·¼ ì„±ëŠ¥ ê³„ì‚°
            recent_metrics = model_data[-10:] if len(model_data) >= 10 else model_data
            mae_values = [m["metrics"].get("mae", 0) for m in recent_metrics]
            mse_values = [m["metrics"].get("mse", 0) for m in recent_metrics]

            comparison[model_name] = {
                "avg_mae": np.mean(mae_values),
                "avg_mse": np.mean(mse_values),
                "std_mae": np.std(mae_values),
                "std_mse": np.std(mse_values),
                "total_updates": len(model_data),
            }

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        if comparison:
            best_mae_model = min(comparison.keys(), key=lambda x: comparison[x]["avg_mae"])
            best_mse_model = min(comparison.keys(), key=lambda x: comparison[x]["avg_mse"])

            comparison["best_models"] = {
                "mae": best_mae_model,
                "mse": best_mse_model,
            }

        self.comparison_results = comparison
        return comparison

    def get_model_ranking(self) -> List[Tuple[str, float]]:
        """ëª¨ë¸ ìˆœìœ„ ë°˜í™˜ (MAE ê¸°ì¤€)"""
        if not self.comparison_results:
            return []

        rankings = []
        for model_name, metrics in self.comparison_results.items():
            if model_name == "best_models":
                continue
            rankings.append((model_name, metrics["avg_mae"]))

        return sorted(rankings, key=lambda x: x[1])


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” ê³ ê¸‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ")
    print("=" * 50)

    # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    monitor = PerformanceMonitor()
    tracker = ModelPerformanceTracker()

    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor.start_monitoring()

    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    print("ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    for i in range(50):
        # ê°€ìƒì˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
        mae = 0.5 + 0.1 * np.sin(i * 0.1) + np.random.normal(0, 0.05)
        mse = mae ** 2 + np.random.normal(0, 0.01)
        loss = mse + np.random.normal(0, 0.02)

        metrics = {
            "mae": max(0, mae),
            "mse": max(0, mse),
            "loss": max(0, loss),
        }

        additional_info = {
            "learning_rate": 0.001 * (0.95 ** (i // 10)),
            "memory_usage": 0.3 + 0.1 * np.random.random(),
            "batch_size": 32,
        }

        # ë©”íŠ¸ë¦­ ê¸°ë¡
        monitor.record_metrics("test_model", metrics, additional_info)
        tracker.track_model("test_model", metrics)

        time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ ì§€ì—°

    # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
    summary = monitor.get_performance_summary()
    print("\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
    print(f"   ëª¨ë‹ˆí„°ë§ ì‹œê°„: {summary['monitoring_duration']:.1f}ì´ˆ")
    print(f"   ì´ ê¸°ë¡ ìˆ˜: {summary['total_records']}")
    print(f"   í˜„ì¬ MAE: {summary['recent_performance']['mae']['current']:.4f}")
    print(f"   ì „ì²´ ì¶”ì„¸: {summary['overall_trend']:.4f}")

    # ëŒ€ì‹œë³´ë“œ ìƒì„±
    monitor.create_performance_dashboard()
    monitor.export_performance_report()

    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    monitor.stop_monitoring()

    print("\nâœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
