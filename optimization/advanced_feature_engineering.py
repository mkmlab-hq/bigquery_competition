#!/usr/bin/env python3
"""
ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ - ì„±ëŠ¥ ê·¹ëŒ€í™”
- ë¹„ì„ í˜• ë³€í™˜ íƒìƒ‰
- ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
- ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ì¶”ì¶œ
- ê³ ê¸‰ í”¼ì²˜ ì„ íƒ
"""

import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from google.cloud import bigquery
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import PolynomialFeatures, RobustScaler, StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor

warnings.filterwarnings("ignore", category=UserWarning)


class AdvancedFeatureEngineer:
    """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scalers = {}
        self.models = {}
        self.feature_selector = None
        self.polynomial_features = None

        # ì¸ì¦ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
            "F:/workspace/bigquery_competition/optimization/gcs-key.json"
        )

        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âŒ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            raise e

    def load_real_bigquery_data(self, limit: int = 10000) -> Tuple[Dict, np.ndarray]:
        """ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©"""
        print("ğŸ”„ ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            # Big5 ë°ì´í„° ë¡œë”©
            big5_query = f"""
            SELECT * FROM `persona-diary-service.big5_dataset.big5_preprocessed` LIMIT {limit}
            """
            big5_df = self.client.query(big5_query).to_dataframe()

            # CMI ë°ì´í„° ë¡œë”©
            cmi_query = f"""
            SELECT * FROM `persona-diary-service.cmi_dataset.cmi_preprocessed` LIMIT {limit}
            """
            cmi_df = self.client.query(cmi_query).to_dataframe()

            # RPPG ë°ì´í„° ë¡œë”©
            rppg_query = f"""
            SELECT * FROM `persona-diary-service.rppg_dataset.rppg_preprocessed` LIMIT {limit}
            """
            rppg_df = self.client.query(rppg_query).to_dataframe()

            # Voice ë°ì´í„° ë¡œë”©
            voice_query = f"""
            SELECT * FROM `persona-diary-service.voice_dataset.voice_preprocessed` LIMIT {limit}
            """
            voice_df = self.client.query(voice_query).to_dataframe()

            # ìˆ˜ì¹˜ ë°ì´í„°ë§Œ ì„ íƒ
            cmi_numeric = cmi_df.select_dtypes(include=[np.number])
            rppg_numeric = rppg_df.select_dtypes(include=[np.number])
            voice_numeric = voice_df.select_dtypes(include=[np.number])
            big5_numeric = big5_df.select_dtypes(include=[np.number])

            # ë°ì´í„° ê²°í•©
            multimodal_data = {
                "big5": big5_numeric.values,
                "cmi": cmi_numeric.values,
                "rppg": rppg_numeric.values,
                "voice": voice_numeric.values,
            }

            # ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
            print("ğŸ” ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

            # Big5 ì ìˆ˜ ê³„ì‚°
            big5_scores = {
                "EXT": big5_numeric[["EXT1", "EXT2", "EXT3", "EXT4", "EXT5"]].mean(
                    axis=1
                ),
                "EST": big5_numeric[["EST1", "EST2", "EST3", "EST4", "EST5"]].mean(
                    axis=1
                ),
                "AGR": big5_numeric[["AGR1", "AGR2", "AGR3", "AGR4", "AGR5"]].mean(
                    axis=1
                ),
                "CSN": big5_numeric[["CSN1", "CSN2", "CSN3", "CSN4", "CSN5"]].mean(
                    axis=1
                ),
                "OPN": big5_numeric[["OPN1", "OPN2", "OPN3", "OPN4", "OPN5"]].mean(
                    axis=1
                ),
            }

            # ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ (ê°€ì¤‘ì¹˜ ì¡°ì •)
            targets = (
                big5_scores["EXT"] * 0.30
                + big5_scores["OPN"] * 0.25
                + (6 - big5_scores["EST"]) * 0.20
                + big5_scores["AGR"] * 0.15
                + big5_scores["CSN"] * 0.10
                + (cmi_numeric.mean(axis=1) / 6) * 0.05
                + (rppg_numeric.mean(axis=1) / 6) * 0.03
                + (voice_numeric.mean(axis=1) / 6) * 0.02
            )

            print(f"âœ… ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
            print(f"   Big5: {big5_numeric.shape}")
            print(f"   CMI: {cmi_numeric.shape}")
            print(f"   RPPG: {rppg_numeric.shape}")
            print(f"   Voice: {voice_numeric.shape}")
            print(f"   Targets: {targets.shape}")

            return multimodal_data, targets

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e

    def create_advanced_features(self, multimodal_data: Dict) -> np.ndarray:
        """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("ğŸ”§ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...")

        # 1. ê¸°ë³¸ í”¼ì²˜ ê²°í•©
        X_basic = np.concatenate(
            [
                multimodal_data["big5"],
                multimodal_data["cmi"],
                multimodal_data["rppg"],
                multimodal_data["voice"],
            ],
            axis=1,
        )

        print(f"   ê¸°ë³¸ í”¼ì²˜ ìˆ˜: {X_basic.shape[1]}")

        # 2. í†µê³„ì  í”¼ì²˜ ìƒì„±
        print("   í†µê³„ì  í”¼ì²˜ ìƒì„± ì¤‘...")
        statistical_features = []

        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í†µê³„ í”¼ì²˜
        for modality_name, data in multimodal_data.items():
            # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
            if data.dtype != np.float64:
                data = data.astype(np.float64)

            # í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’, ìµœì†Œê°’
            mean_features = np.mean(data, axis=1, keepdims=True)
            std_features = np.std(data, axis=1, keepdims=True)
            max_features = np.max(data, axis=1, keepdims=True)
            min_features = np.min(data, axis=1, keepdims=True)

            # ë¶„ìœ„ìˆ˜ í”¼ì²˜
            q25_features = np.percentile(data, 25, axis=1, keepdims=True)
            q75_features = np.percentile(data, 75, axis=1, keepdims=True)

            # ë²”ìœ„ í”¼ì²˜
            range_features = max_features - min_features

            # ë³€ë™ê³„ìˆ˜ í”¼ì²˜
            cv_features = std_features / (mean_features + 1e-8)

            statistical_features.append(
                np.concatenate(
                    [
                        mean_features,
                        std_features,
                        max_features,
                        min_features,
                        q25_features,
                        q75_features,
                        range_features,
                        cv_features,
                    ],
                    axis=1,
                )
            )

        X_statistical = np.concatenate(statistical_features, axis=1)
        print(f"   í†µê³„ì  í”¼ì²˜ ìˆ˜: {X_statistical.shape[1]}")

        # 3. ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
        print("   ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì¤‘...")
        interaction_features = []

        # Big5ì™€ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ê°„ ìƒí˜¸ì‘ìš©
        big5_data = multimodal_data["big5"]
        for other_name, other_data in [
            ("cmi", multimodal_data["cmi"]),
            ("rppg", multimodal_data["rppg"]),
            ("voice", multimodal_data["voice"]),
        ]:
            # Big5ì˜ ì£¼ìš” íŠ¹ì„±ê³¼ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì˜ í‰ê·  ê°„ ìƒí˜¸ì‘ìš©
            big5_ext = big5_data[:, 0:5].mean(axis=1, keepdims=True)  # EXT
            big5_opn = big5_data[:, 5:10].mean(axis=1, keepdims=True)  # OPN
            other_mean = np.mean(other_data, axis=1, keepdims=True)

            interaction_features.append(big5_ext * other_mean)
            interaction_features.append(big5_opn * other_mean)

        X_interaction = np.concatenate(interaction_features, axis=1)
        print(f"   ìƒí˜¸ì‘ìš© í”¼ì²˜ ìˆ˜: {X_interaction.shape[1]}")

        # 4. ë¹„ì„ í˜• ë³€í™˜ í”¼ì²˜
        print("   ë¹„ì„ í˜• ë³€í™˜ í”¼ì²˜ ìƒì„± ì¤‘...")
        nonlinear_features = []

        # ë¡œê·¸ ë³€í™˜ (ì–‘ìˆ˜ ê°’ë§Œ)
        for data in [
            multimodal_data["big5"],
            multimodal_data["cmi"],
            multimodal_data["rppg"],
            multimodal_data["voice"],
        ]:
            data_positive = np.maximum(data, 0.1)  # 0ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ë³€í™˜
            log_features = np.log(data_positive)
            nonlinear_features.append(log_features)

        # ì œê³±ê·¼ ë³€í™˜
        for data in [
            multimodal_data["big5"],
            multimodal_data["cmi"],
            multimodal_data["rppg"],
            multimodal_data["voice"],
        ]:
            data_positive = np.maximum(data, 0)  # ìŒìˆ˜ ê°’ ì²˜ë¦¬
            sqrt_features = np.sqrt(data_positive)
            nonlinear_features.append(sqrt_features)

        X_nonlinear = np.concatenate(nonlinear_features, axis=1)
        print(f"   ë¹„ì„ í˜• ë³€í™˜ í”¼ì²˜ ìˆ˜: {X_nonlinear.shape[1]}")

        # 5. ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜
        print("   ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ìƒì„± ì¤‘...")
        domain_features = []

        # Big5 ê·¹ê°’ í”¼ì²˜ (ë§¤ìš° ë†’ê±°ë‚˜ ë‚®ì€ ê°’)
        big5_data = multimodal_data["big5"]
        big5_extreme_high = np.sum(big5_data > 5, axis=1, keepdims=True)
        big5_extreme_low = np.sum(big5_data < 2, axis=1, keepdims=True)
        domain_features.append(big5_extreme_high)
        domain_features.append(big5_extreme_low)

        # CMI ì¼ê´€ì„± í”¼ì²˜ (í‘œì¤€í¸ì°¨ê°€ ë‚®ì€ ê²½ìš°)
        cmi_data = multimodal_data["cmi"]
        cmi_consistency = 1 / (np.std(cmi_data, axis=1, keepdims=True) + 1e-8)
        domain_features.append(cmi_consistency)

        # RPPG ì•ˆì •ì„± í”¼ì²˜ (ë³€ë™ê³„ìˆ˜)
        rppg_data = multimodal_data["rppg"]
        rppg_mean = np.mean(rppg_data, axis=1, keepdims=True)
        rppg_std = np.std(rppg_data, axis=1, keepdims=True)
        rppg_stability = 1 / (rppg_std / (rppg_mean + 1e-8) + 1e-8)
        domain_features.append(rppg_stability)

        # Voice ë³µì¡ì„± í”¼ì²˜ (ê³ ìœ ê°’ì˜ ìˆ˜)
        voice_data = multimodal_data["voice"]
        voice_complexity = np.sum(
            voice_data > np.mean(voice_data, axis=1, keepdims=True),
            axis=1,
            keepdims=True,
        )
        domain_features.append(voice_complexity)

        X_domain = np.concatenate(domain_features, axis=1)
        print(f"   ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ìˆ˜: {X_domain.shape[1]}")

        # 6. ëª¨ë“  í”¼ì²˜ ê²°í•©
        X_combined = np.concatenate(
            [X_basic, X_statistical, X_interaction, X_nonlinear, X_domain], axis=1
        )

        print(f"âœ… ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ:")
        print(f"   ì´ í”¼ì²˜ ìˆ˜: {X_combined.shape[1]}")
        print(f"   ê¸°ë³¸: {X_basic.shape[1]}, í†µê³„: {X_statistical.shape[1]}")
        print(f"   ìƒí˜¸ì‘ìš©: {X_interaction.shape[1]}, ë¹„ì„ í˜•: {X_nonlinear.shape[1]}")
        print(f"   ë„ë©”ì¸: {X_domain.shape[1]}")

        return X_combined

    def create_optimized_models(self):
        """ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ”„ ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")

        self.models = {
            "random_forest": RandomForestRegressor(
                n_estimators=300,  # ë” ë§ì€ íŠ¸ë¦¬
                max_depth=15,  # ë” ê¹Šì€ ê¹Šì´
                min_samples_split=5,  # ë” ì ì€ ìƒ˜í”Œ í•„ìš”
                min_samples_leaf=2,  # ë” ì ì€ ë¦¬í”„ ìƒ˜í”Œ
                max_features="sqrt",  # í”¼ì²˜ ì„ íƒ ìµœì í™”
                random_state=42,
                n_jobs=-1,
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=300,  # ë” ë§ì€ íŠ¸ë¦¬
                learning_rate=0.03,  # ë” ë‚®ì€ í•™ìŠµë¥ 
                max_depth=8,  # ë” ê¹Šì€ ê¹Šì´
                min_samples_split=5,
                subsample=0.8,  # ì„œë¸Œìƒ˜í”Œë§
                random_state=42,
            ),
            "xgboost": XGBRegressor(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=8,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.01,  # L1 ì •ê·œí™”
                reg_lambda=0.01,  # L2 ì •ê·œí™”
                random_state=42,
                n_jobs=-1,
            ),
            "lightgbm": LGBMRegressor(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=8,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.01,  # L1 ì •ê·œí™”
                reg_lambda=0.01,  # L2 ì •ê·œí™”
                random_state=42,
                n_jobs=-1,
                verbose=-1,
            ),
            "ridge": Ridge(alpha=0.1),  # ì •ê·œí™” ê°•ë„ ì¡°ì •
            "elastic_net": ElasticNet(alpha=0.001, l1_ratio=0.5),  # ì •ê·œí™” ê°•ë„ ì¡°ì •
            "svr": SVR(kernel="rbf", C=10.0, gamma="auto"),  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        }

        print(f"âœ… {len(self.models)}ê°œ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    def prepare_advanced_data(
        self, X: np.ndarray, y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ê³ ê¸‰ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ”„ ê³ ê¸‰ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # RobustScalerë¡œ ì •ê·œí™”
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers["robust_ensemble"] = scaler

        # ë‹¤í•­ì‹ í”¼ì²˜ ìƒì„± (2ì°¨ê¹Œì§€)
        print("   ë‹¤í•­ì‹ í”¼ì²˜ ìƒì„± ì¤‘...")
        poly_features = PolynomialFeatures(
            degree=2, include_bias=False, interaction_only=True
        )
        X_poly = poly_features.fit_transform(X_scaled)
        self.polynomial_features = poly_features

        print(f"   ë‹¤í•­ì‹ í”¼ì²˜ ìˆ˜: {X_poly.shape[1]}")

        # ê³ ê¸‰ í”¼ì²˜ ì„ íƒ (ìƒìœ„ 200ê°œ íŠ¹ì„± ì„ íƒ)
        print("   ê³ ê¸‰ í”¼ì²˜ ì„ íƒ ì¤‘...")
        self.feature_selector = SelectKBest(score_func=mutual_info_regression, k=200)
        X_selected = self.feature_selector.fit_transform(X_poly, y)

        print(f"âœ… ê³ ê¸‰ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_selected.shape}")
        print(f"   ì›ë³¸ í”¼ì²˜ ìˆ˜: {X.shape[1]}")
        print(f"   ë‹¤í•­ì‹ í”¼ì²˜ ìˆ˜: {X_poly.shape[1]}")
        print(f"   ì„ íƒëœ í”¼ì²˜ ìˆ˜: {X_selected.shape[1]}")

        return X_selected, y

    def train_advanced_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ê³ ê¸‰ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸš€ ê³ ê¸‰ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        model_results = {}
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        for name, model in self.models.items():
            print(f"   í›ˆë ¨ ì¤‘: {name}")

            try:
                # êµì°¨ ê²€ì¦
                cv_r2_scores = cross_val_score(
                    model, X_train, y_train, cv=kf, scoring="r2", n_jobs=-1
                )
                cv_rmse_scores = -cross_val_score(
                    model,
                    X_train,
                    y_train,
                    cv=kf,
                    scoring="neg_root_mean_squared_error",
                    n_jobs=-1,
                )

                avg_r2 = cv_r2_scores.mean()
                std_r2 = cv_r2_scores.std()
                avg_rmse = cv_rmse_scores.mean()

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
                test_pred = model.predict(X_test)
                test_r2 = r2_score(y_test, test_pred)
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

                model_results[name] = {
                    "cv_mean_r2": avg_r2,
                    "cv_std_r2": std_r2,
                    "cv_mean_rmse": avg_rmse,
                    "test_r2": test_r2,
                    "test_rmse": test_rmse,
                    "model": model,
                }

                print(f"     CV RÂ²: {avg_r2:.4f} (Â±{std_r2:.4f})")
                print(f"     Test RÂ²: {test_r2:.4f}")
                print(f"     Test RMSE: {test_rmse:.4f}")

            except Exception as e:
                print(f"     âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
                model_results[name] = None

        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(), key=lambda x: x[1]["test_r2"], reverse=True
        )

        print(f"âœ… {len(valid_models)}ê°œ ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        print("ğŸ“Š ê³ ê¸‰ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (í…ŒìŠ¤íŠ¸ RÂ² ê¸°ì¤€):")
        for i, (name, scores) in enumerate(sorted_models, 1):
            print(f"   {i}. {name}: RÂ² = {scores['test_r2']:.4f}")

        return model_results

    def create_advanced_ensemble(
        self, X: np.ndarray, y: np.ndarray, model_results: Dict
    ) -> Dict:
        """ê³ ê¸‰ ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ”„ ê³ ê¸‰ ì•™ìƒë¸” ìƒì„± ì¤‘...")

        # ìƒìœ„ 5ê°œ ëª¨ë¸ ì„ íƒ
        valid_models = {k: v for k, v in model_results.items() if v is not None}
        sorted_models = sorted(
            valid_models.items(), key=lambda x: x[1]["test_r2"], reverse=True
        )
        top_models = [name for name, _ in sorted_models[:5]]

        print(f"   ì„ íƒëœ ìƒìœ„ ëª¨ë¸ë“¤: {top_models}")

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        predictions = []
        weights = []

        for name in top_models:
            if name in model_results and model_results[name] is not None:
                model = model_results[name]["model"]
                pred = model.predict(X_test)
                predictions.append(pred)
                # í…ŒìŠ¤íŠ¸ RÂ² ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
                weights.append(model_results[name]["test_r2"])

        if not predictions:
            print("âŒ ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = np.array(predictions)
        ensemble_pred = np.average(predictions, axis=0, weights=weights)

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        r2 = r2_score(y_test, ensemble_pred)
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(ensemble_pred - y_test))
        correlation = np.corrcoef(ensemble_pred, y_test)[0, 1]

        results = {
            "r2": r2,
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "correlation": correlation,
            "predictions": ensemble_pred,
            "targets": y_test,
            "selected_models": top_models,
            "model_weights": dict(zip(top_models, weights)),
        }

        print(f"âœ… ê³ ê¸‰ ì•™ìƒë¸” ìƒì„± ë° í‰ê°€ ì™„ë£Œ:")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")

        return results

    def run_advanced_feature_engineering(self, limit: int = 10000) -> Dict:
        """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ì‹¤ì œ BigQuery ë°ì´í„° ë¡œë”©
        multimodal_data, targets = self.load_real_bigquery_data(limit)

        # 2. ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        X_engineered = self.create_advanced_features(multimodal_data)

        # 3. ìµœì í™”ëœ ëª¨ë¸ë“¤ ìƒì„±
        self.create_optimized_models()

        # 4. ê³ ê¸‰ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_advanced_data(X_engineered, targets)

        # 5. ê³ ê¸‰ ëª¨ë¸ë“¤ í›ˆë ¨
        model_results = self.train_advanced_models(X, y)

        # 6. ê³ ê¸‰ ì•™ìƒë¸” ìƒì„±
        ensemble_results = self.create_advanced_ensemble(X, y, model_results)

        # 7. ê²°ê³¼ ì €ì¥
        results = {
            "individual_models_results": {
                name: {
                    "cv_mean_r2": scores["cv_mean_r2"] if scores else None,
                    "cv_std_r2": scores["cv_std_r2"] if scores else None,
                    "test_r2": scores["test_r2"] if scores else None,
                    "test_rmse": scores["test_rmse"] if scores else None,
                }
                for name, scores in model_results.items()
            },
            "ensemble_results": ensemble_results,
            "data_info": {
                "n_samples": len(y),
                "n_features_original": X_engineered.shape[1],
                "n_features_selected": X.shape[1],
                "n_models_trained": len(
                    [m for m in model_results.values() if m is not None]
                ),
                "n_models_in_ensemble": (
                    len(ensemble_results["selected_models"]) if ensemble_results else 0
                ),
                "feature_engineering_status": "ì™„ë£Œ",
            },
        }

        # JSONìœ¼ë¡œ ì €ì¥
        with open("advanced_feature_engineering_results.json", "w") as f:

            def convert_to_json_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.float32):
                    return float(obj)
                elif isinstance(obj, np.float64):
                    return float(obj)
                elif isinstance(obj, np.int32):
                    return int(obj)
                elif isinstance(obj, np.int64):
                    return int(obj)
                elif isinstance(obj, pd.Series):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_json_serializable(item) for item in obj]
                else:
                    return obj

            json_results = convert_to_json_serializable(results)
            json.dump(json_results, f, indent=2)

        print(f"âœ… ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!")
        if ensemble_results:
            print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {ensemble_results['r2']:.4f}")
            print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {ensemble_results['rmse']:.4f}")

        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ - ì„±ëŠ¥ ê·¹ëŒ€í™”")
    print("=" * 60)

    engineer = AdvancedFeatureEngineer()
    results = engineer.run_advanced_feature_engineering(limit=10000)

    print("\nğŸ“Š ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:")
    if results["ensemble_results"]:
        print(f"   ìµœì¢… ì•™ìƒë¸” RÂ²: {results['ensemble_results']['r2']:.4f}")
        print(f"   ìµœì¢… ì•™ìƒë¸” RMSE: {results['ensemble_results']['rmse']:.4f}")
        print(f"   ìƒê´€ê³„ìˆ˜: {results['ensemble_results']['correlation']:.4f}")
        print(f"   ì„ íƒëœ ëª¨ë¸ë“¤: {results['ensemble_results']['selected_models']}")


if __name__ == "__main__":
    main()
