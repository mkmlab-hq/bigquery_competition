import json
import os
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from google.cloud import bigquery
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from torch.utils.data import DataLoader

from transfer_learning_multimodal import (
    TransferLearningMultimodalDataset,
    TransferLearningMultimodalNet,
    TransferLearningTrainer,
)

warnings.filterwarnings("ignore")


class ImprovedBigQueryDataLoader:
    """ê°œì„ ëœ BigQuery ë°ì´í„° ë¡œë”"""

    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        try:
            self.client = bigquery.Client(project=project_id)
            print(f"âœ… BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            print(f"âš ï¸ BigQuery ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
            print("ëŒ€ì²´ ë°ì´í„° ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.client = None

    def load_competition_data(self, limit: int = 50000) -> dict:
        """ëŒ€íšŒ ë°ì´í„° ë¡œë“œ (ë” ë§ì€ ìƒ˜í”Œ)"""
        if self.client is None:
            print("BigQuery í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_improved_fallback_data(limit)

        print(f"BigQueryì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘... (ì œí•œ: {limit}ê°œ)")

        try:
            # Big5 ë°ì´í„°
            big5_query = f"""
            SELECT 
                openness, conscientiousness, extraversion, agreeableness, neuroticism
            FROM `{self.project_id}.persona_diary.big5_scores`
            LIMIT {limit}
            """

            # CMI ë°ì´í„°
            cmi_query = f"""
            SELECT 
                cmi_1, cmi_2, cmi_3, cmi_4, cmi_5, cmi_6, cmi_7, cmi_8, cmi_9, cmi_10,
                cmi_11, cmi_12, cmi_13, cmi_14, cmi_15, cmi_16, cmi_17, cmi_18, cmi_19, cmi_20
            FROM `{self.project_id}.persona_diary.cmi_scores`
            LIMIT {limit}
            """

            # RPPG ë°ì´í„°
            rppg_query = f"""
            SELECT 
                rppg_1, rppg_2, rppg_3, rppg_4, rppg_5, rppg_6, rppg_7, rppg_8, rppg_9, rppg_10
            FROM `{self.project_id}.persona_diary.rppg_features`
            LIMIT {limit}
            """

            # Voice ë°ì´í„°
            voice_query = f"""
            SELECT 
                voice_1, voice_2, voice_3, voice_4, voice_5, voice_6, voice_7, voice_8, voice_9, voice_10,
                voice_11, voice_12, voice_13, voice_14, voice_15, voice_16, voice_17, voice_18, voice_19, voice_20
            FROM `{self.project_id}.persona_diary.voice_features`
            LIMIT {limit}
            """

            # íƒ€ê²Ÿ ë°ì´í„°
            target_query = f"""
            SELECT 
                target_score
            FROM `{self.project_id}.persona_diary.target_scores`
            LIMIT {limit}
            """

            # ë°ì´í„° ë¡œë“œ
            big5_df = self.client.query(big5_query).to_dataframe()
            cmi_df = self.client.query(cmi_query).to_dataframe()
            rppg_df = self.client.query(rppg_query).to_dataframe()
            voice_df = self.client.query(voice_query).to_dataframe()
            target_df = self.client.query(target_query).to_dataframe()

            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
            print(f"  Big5: {len(big5_df)}ê°œ")
            print(f"  CMI: {len(cmi_df)}ê°œ")
            print(f"  RPPG: {len(rppg_df)}ê°œ")
            print(f"  Voice: {len(voice_df)}ê°œ")
            print(f"  Target: {len(target_df)}ê°œ")

            return {
                "big5": big5_df.values,
                "cmi": cmi_df.values,
                "rppg": rppg_df.values,
                "voice": voice_df.values,
                "targets": target_df.values.flatten(),
            }

        except Exception as e:
            print(f"âŒ BigQuery ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_improved_fallback_data(limit)

    def _generate_improved_fallback_data(self, limit: int) -> dict:
        """ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„° ìƒì„± (ë” í˜„ì‹¤ì ì´ê³  ë‹¤ì–‘í•œ ë°ì´í„°)"""
        print("ğŸ“Š ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„° ìƒì„± ì¤‘...")

        # ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ìƒì„±
        np.random.seed(42)

        # Big5 ë°ì´í„° (ë” ë‹¤ì–‘í•œ ë¶„í¬)
        big5_data = np.random.beta(2, 2, (limit, 5))

        # CMI ë°ì´í„° (ë” ë³µì¡í•œ íŒ¨í„´)
        cmi_data = np.random.beta(1.5, 1.5, (limit, 20))

        # RPPG ë°ì´í„° (ìƒì²´ì‹ í˜¸ íŠ¹ì„± ë°˜ì˜)
        rppg_data = np.random.normal(0, 1, (limit, 10))

        # Voice ë°ì´í„° (ìŒì„± íŠ¹ì„± ë°˜ì˜)
        voice_data = np.random.normal(0, 1, (limit, 20))

        # ë” ë³µì¡í•œ íƒ€ê²Ÿ ìƒì„± (ë¹„ì„ í˜• ê´€ê³„ í¬í•¨)
        targets = (
            0.20 * big5_data[:, 0]  # Openness
            + 0.18 * big5_data[:, 1]  # Conscientiousness
            + 0.15 * big5_data[:, 2]  # Extraversion
            + 0.17 * big5_data[:, 3]  # Agreeableness
            + 0.12 * big5_data[:, 4]  # Neuroticism
            + 0.08 * np.mean(cmi_data, axis=1)
            + 0.05 * np.mean(rppg_data, axis=1)
            + 0.03 * np.mean(voice_data, axis=1)
            # ë¹„ì„ í˜• ìƒí˜¸ì‘ìš© ì¶”ê°€
            + 0.02 * (big5_data[:, 0] * big5_data[:, 1])  # Openness Ã— Conscientiousness
            + 0.01 * (big5_data[:, 2] * big5_data[:, 3])  # Extraversion Ã— Agreeableness
            + np.random.normal(0, 0.12, limit)  # ë…¸ì´ì¦ˆ
        )

        # íƒ€ê²Ÿ ì •ê·œí™” (0-1 ë²”ìœ„)
        targets = (targets - targets.min()) / (targets.max() - targets.min())

        print(f"âœ… ê°œì„ ëœ ëŒ€ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ: {limit}ê°œ ìƒ˜í”Œ")

        return {
            "big5": big5_data,
            "cmi": cmi_data,
            "rppg": rppg_data,
            "voice": voice_data,
            "targets": targets,
        }


class FeatureEngineer:
    """íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.poly_features = PolynomialFeatures(
            degree=2, include_bias=False, interaction_only=True
        )
        self.scalers = {}

    def create_advanced_features(self, data: dict) -> dict:
        """ê³ ê¸‰ íŠ¹ì§• ìƒì„±"""
        print("ğŸ”§ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...")

        # ê¸°ë³¸ íŠ¹ì§•
        big5 = data["big5"]
        cmi = data["cmi"]
        rppg = data["rppg"]
        voice = data["voice"]

        # 1. í†µê³„ì  íŠ¹ì§• ìƒì„±
        print("  ğŸ“Š í†µê³„ì  íŠ¹ì§• ìƒì„±...")
        big5_stats = self._create_statistical_features(big5, "big5")
        cmi_stats = self._create_statistical_features(cmi, "cmi")
        rppg_stats = self._create_statistical_features(rppg, "rppg")
        voice_stats = self._create_statistical_features(voice, "voice")

        # 2. ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„±
        print("  ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„±...")
        big5_interactions = self._create_interaction_features(big5, "big5")
        cmi_interactions = self._create_interaction_features(cmi, "cmi")

        # 3. ë‹¤í•­ì‹ íŠ¹ì§• ìƒì„±
        print("  ğŸ“ˆ ë‹¤í•­ì‹ íŠ¹ì§• ìƒì„±...")
        big5_poly = self.poly_features.fit_transform(big5)

        # 4. ëª¨ë“  íŠ¹ì§• ê²°í•©
        enhanced_data = {
            "big5": np.concatenate([big5, big5_stats, big5_interactions], axis=1),
            "cmi": np.concatenate([cmi, cmi_stats, cmi_interactions], axis=1),
            "rppg": np.concatenate([rppg, rppg_stats], axis=1),
            "voice": np.concatenate([voice, voice_stats], axis=1),
            "targets": data["targets"],
        }

        print(f"âœ… íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ:")
        print(f"  Big5: {big5.shape[1]} â†’ {enhanced_data['big5'].shape[1]} íŠ¹ì§•")
        print(f"  CMI: {cmi.shape[1]} â†’ {enhanced_data['cmi'].shape[1]} íŠ¹ì§•")
        print(f"  RPPG: {rppg.shape[1]} â†’ {enhanced_data['rppg'].shape[1]} íŠ¹ì§•")
        print(f"  Voice: {voice.shape[1]} â†’ {enhanced_data['voice'].shape[1]} íŠ¹ì§•")

        return enhanced_data

    def _create_statistical_features(self, data: np.ndarray, name: str) -> np.ndarray:
        """í†µê³„ì  íŠ¹ì§• ìƒì„±"""
        features = []
        features.append(np.mean(data, axis=1, keepdims=True))  # í‰ê· 
        features.append(np.std(data, axis=1, keepdims=True))  # í‘œì¤€í¸ì°¨
        features.append(np.var(data, axis=1, keepdims=True))  # ë¶„ì‚°
        features.append(np.max(data, axis=1, keepdims=True))  # ìµœëŒ€ê°’
        features.append(np.min(data, axis=1, keepdims=True))  # ìµœì†Œê°’
        features.append(np.median(data, axis=1, keepdims=True))  # ì¤‘ì•™ê°’

        return np.concatenate(features, axis=1)

    def _create_interaction_features(self, data: np.ndarray, name: str) -> np.ndarray:
        """ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„±"""
        if data.shape[1] < 2:
            return np.array([]).reshape(data.shape[0], 0)

        # ìƒìœ„ 3ê°œ íŠ¹ì§• ê°„ì˜ ìƒí˜¸ì‘ìš©
        n_features = min(3, data.shape[1])
        interactions = []
        for i in range(n_features):
            for j in range(i + 1, n_features):
                interactions.append((data[:, i] * data[:, j]).reshape(-1, 1))

        if interactions:
            return np.concatenate(interactions, axis=1)
        else:
            return np.array([]).reshape(data.shape[0], 0)


class ImprovedEnsembleModel:
    """ê°œì„ ëœ ì•™ìƒë¸” ëª¨ë¸"""

    def __init__(self, device="cpu"):
        self.device = device
        self.models = {}
        self.scalers = {}
        self.weights = {}

    def add_model(self, name: str, model, model_type: str, weight: float = 1.0):
        """ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = {"model": model, "type": model_type, "weight": weight}

    def train_models(
        self, X_train: Dict, y_train: np.ndarray, X_val: Dict, y_val: np.ndarray
    ):
        """ëª¨ë“  ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ ê°œì„ ëœ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")

        # ë°ì´í„° ì •ê·œí™”
        for modality in ["big5", "cmi", "rppg", "voice"]:
            scaler = StandardScaler()
            X_train[modality] = scaler.fit_transform(X_train[modality])
            X_val[modality] = scaler.transform(X_val[modality])
            self.scalers[modality] = scaler

        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ê²°í•©
        X_train_combined = np.concatenate(
            [X_train["big5"], X_train["cmi"], X_train["rppg"], X_train["voice"]], axis=1
        )
        X_val_combined = np.concatenate(
            [X_val["big5"], X_val["cmi"], X_val["rppg"], X_val["voice"]], axis=1
        )

        # ê° ëª¨ë¸ í›ˆë ¨
        for name, model_info in self.models.items():
            print(f"ğŸ“š {name} ëª¨ë¸ í›ˆë ¨ ì¤‘...")

            if model_info["type"] == "neural_network":
                # ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨
                self._train_neural_network(
                    name, model_info, X_train, y_train, X_val, y_val
                )
            else:
                # Scikit-learn ëª¨ë¸ í›ˆë ¨
                self._train_sklearn_model(
                    name, model_info, X_train_combined, y_train, X_val_combined, y_val
                )

        # ê°€ì¤‘ì¹˜ ìµœì í™”
        self._optimize_weights(X_val, y_val)

        print("âœ… ê°œì„ ëœ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

    def _train_neural_network(
        self,
        name: str,
        model_info: Dict,
        X_train: Dict,
        y_train: np.ndarray,
        X_val: Dict,
        y_val: np.ndarray,
    ):
        """ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨"""
        model = model_info["model"]
        model.to(self.device)

        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
        criterion = nn.MSELoss()

        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = TransferLearningMultimodalDataset(
            X_train["big5"],
            X_train["cmi"],
            X_train["rppg"],
            X_train["voice"],
            y_train,
            augment=True,  # ë°ì´í„° ì¦ê°• í™œì„±í™”
        )
        val_dataset = TransferLearningMultimodalDataset(
            X_val["big5"],
            X_val["cmi"],
            X_val["rppg"],
            X_val["voice"],
            y_val,
            augment=False,
        )

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

        # í›ˆë ¨
        best_val_r2 = -float("inf")
        patience = 10  # ë” ê¸´ patience
        patience_counter = 0

        for epoch in range(50):  # ë” ë§ì€ ì—í¬í¬
            # í›ˆë ¨
            model.train()
            train_loss = 0
            for batch in train_loader:
                optimizer.zero_grad()

                big5 = batch["big5"].to(self.device)
                cmi = batch["cmi"].to(self.device)
                rppg = batch["rppg"].to(self.device)
                voice = batch["voice"].to(self.device)
                targets = batch["target"].to(self.device)

                predictions = model(big5, cmi, rppg, voice)
                loss = criterion(predictions, targets)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                train_loss += loss.item()

            # ê²€ì¦
            model.eval()
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    big5 = batch["big5"].to(self.device)
                    cmi = batch["cmi"].to(self.device)
                    rppg = batch["rppg"].to(self.device)
                    voice = batch["voice"].to(self.device)
                    targets = batch["target"].to(self.device)

                    predictions = model(big5, cmi, rppg, voice)
                    val_predictions.extend(predictions.cpu().numpy().flatten())
                    val_targets.extend(targets.cpu().numpy().flatten())

            val_r2 = r2_score(val_targets, val_predictions)

            if val_r2 > best_val_r2:
                best_val_r2 = val_r2
                patience_counter = 0
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                model_info["best_state"] = model.state_dict().copy()
            else:
                patience_counter += 1

            if patience_counter >= patience:
                break

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³µì›
        if "best_state" in model_info:
            model.load_state_dict(model_info["best_state"])

        print(f"  {name} ìµœê³  RÂ²: {best_val_r2:.4f}")

    def _train_sklearn_model(
        self,
        name: str,
        model_info: Dict,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
    ):
        """Scikit-learn ëª¨ë¸ í›ˆë ¨"""
        model = model_info["model"]

        # í›ˆë ¨
        model.fit(X_train, y_train)

        # ê²€ì¦
        val_predictions = model.predict(X_val)
        val_r2 = r2_score(y_val, val_predictions)

        print(f"  {name} RÂ²: {val_r2:.4f}")

    def _optimize_weights(self, X_val: Dict, y_val: np.ndarray):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”"""
        print("âš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...")

        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
        predictions = {}

        for name, model_info in self.models.items():
            if model_info["type"] == "neural_network":
                # ì‹ ê²½ë§ ëª¨ë¸ ì˜ˆì¸¡
                model = model_info["model"]
                model.eval()

                val_dataset = TransferLearningMultimodalDataset(
                    X_val["big5"],
                    X_val["cmi"],
                    X_val["rppg"],
                    X_val["voice"],
                    y_val,
                    augment=False,
                )
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

                val_predictions = []
                with torch.no_grad():
                    for batch in val_loader:
                        big5 = batch["big5"].to(self.device)
                        cmi = batch["cmi"].to(self.device)
                        rppg = batch["rppg"].to(self.device)
                        voice = batch["voice"].to(self.device)

                        pred = model(big5, cmi, rppg, voice)
                        val_predictions.extend(pred.cpu().numpy().flatten())

                predictions[name] = np.array(val_predictions)
            else:
                # Scikit-learn ëª¨ë¸ ì˜ˆì¸¡
                X_val_combined = np.concatenate(
                    [X_val["big5"], X_val["cmi"], X_val["rppg"], X_val["voice"]], axis=1
                )

                predictions[name] = model_info["model"].predict(X_val_combined)

        # ê°€ì¤‘ì¹˜ ìµœì í™” (RÂ² ì ìˆ˜ ê¸°ë°˜, ìŒìˆ˜ ì œì™¸)
        weights = {}
        for name, pred in predictions.items():
            r2 = r2_score(y_val, pred)
            weights[name] = max(0, r2)  # ìŒìˆ˜ RÂ²ëŠ” 0ìœ¼ë¡œ ì„¤ì •

        # ì •ê·œí™”
        total_weight = sum(weights.values())
        if total_weight > 0:
            for name in weights:
                weights[name] /= total_weight
        else:
            # ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ 0ì´ë©´ ê· ë“± ë¶„ë°°
            for name in weights:
                weights[name] = 1.0 / len(weights)

        self.weights = weights
        print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: {weights}")

    def predict(self, X: Dict) -> np.ndarray:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        predictions = {}

        for name, model_info in self.models.items():
            if model_info["type"] == "neural_network":
                # ì‹ ê²½ë§ ëª¨ë¸ ì˜ˆì¸¡
                model = model_info["model"]
                model.eval()

                dataset = TransferLearningMultimodalDataset(
                    X["big5"],
                    X["cmi"],
                    X["rppg"],
                    X["voice"],
                    np.zeros(len(X["big5"])),
                    augment=False,
                )
                dataloader = DataLoader(dataset, batch_size=64, shuffle=False)

                pred = []
                with torch.no_grad():
                    for batch in dataloader:
                        big5 = batch["big5"].to(self.device)
                        cmi = batch["cmi"].to(self.device)
                        rppg = batch["rppg"].to(self.device)
                        voice = batch["voice"].to(self.device)

                        p = model(big5, cmi, rppg, voice)
                        pred.extend(p.cpu().numpy().flatten())

                predictions[name] = np.array(pred)
            else:
                # Scikit-learn ëª¨ë¸ ì˜ˆì¸¡
                X_combined = np.concatenate(
                    [X["big5"], X["cmi"], X["rppg"], X["voice"]], axis=1
                )

                predictions[name] = model_info["model"].predict(X_combined)

        # ê°€ì¤‘ í‰ê· 
        ensemble_pred = np.zeros(len(predictions[list(predictions.keys())[0]]))
        for name, pred in predictions.items():
            ensemble_pred += self.weights[name] * pred

        return ensemble_pred


def create_improved_models(big5_dim=14, cmi_dim=29, rppg_dim=16, voice_dim=26):
    """ê°œì„ ëœ ëª¨ë¸ ìƒì„± (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ëœ ì°¨ì›ì— ë§ì¶¤)"""
    print("ğŸ¯ ê°œì„ ëœ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ìƒì„±!")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ê°œì„ ëœ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
    ensemble = ImprovedEnsembleModel(device)

    # 1. ì‹ ê²½ë§ ëª¨ë¸ë“¤ ì¶”ê°€ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ëœ ì°¨ì›ì— ë§ì¶¤)
    model1 = TransferLearningMultimodalNet(
        big5_dim=big5_dim,
        cmi_dim=cmi_dim,
        rppg_dim=rppg_dim,
        voice_dim=voice_dim,
        hidden_dim=512,
        dropout_rate=0.3,
        use_pretrained=False,
    )
    ensemble.add_model("neural_net_large", model1, "neural_network", weight=1.0)

    model2 = TransferLearningMultimodalNet(
        big5_dim=big5_dim,
        cmi_dim=cmi_dim,
        rppg_dim=rppg_dim,
        voice_dim=voice_dim,
        hidden_dim=256,
        dropout_rate=0.4,
        use_pretrained=False,
    )
    ensemble.add_model("neural_net_medium", model2, "neural_network", weight=1.0)

    # 2. Scikit-learn ëª¨ë¸ë“¤ ì¶”ê°€ (ë” ê°•ë ¥í•œ ëª¨ë¸)
    ensemble.add_model(
        "random_forest_strong",
        RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42),
        "sklearn",
        weight=1.0,
    )
    ensemble.add_model(
        "gradient_boosting_strong",
        GradientBoostingRegressor(n_estimators=200, max_depth=8, random_state=42),
        "sklearn",
        weight=1.0,
    )
    ensemble.add_model("ridge_optimized", Ridge(alpha=0.1), "sklearn", weight=1.0)

    return ensemble


def test_improved_model():
    """ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘!")

    # 1. BigQuery ë°ì´í„° ë¡œë“œ (ë” ë§ì€ ìƒ˜í”Œ)
    data_loader = ImprovedBigQueryDataLoader()
    data = data_loader.load_competition_data(limit=20000)  # 2ë§Œê°œ ìƒ˜í”Œ

    # 2. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    feature_engineer = FeatureEngineer()
    enhanced_data = feature_engineer.create_advanced_features(data)

    # 3. ë°ì´í„° ë¶„í• 
    train_idx, test_idx = train_test_split(
        range(len(enhanced_data["targets"])), test_size=0.3, random_state=42
    )
    train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42)

    X_train = {
        modality: enhanced_data[modality][train_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }
    X_val = {
        modality: enhanced_data[modality][val_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }
    X_test = {
        modality: enhanced_data[modality][test_idx]
        for modality in ["big5", "cmi", "rppg", "voice"]
    }

    y_train = enhanced_data["targets"][train_idx]
    y_val = enhanced_data["targets"][val_idx]
    y_test = enhanced_data["targets"][test_idx]

    print(f"í›ˆë ¨ ë°ì´í„°: {len(y_train)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(y_val)}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(y_test)}ê°œ")

    # 4. ê°œì„ ëœ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° í›ˆë ¨ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ëœ ì°¨ì› ì „ë‹¬)
    ensemble = create_improved_models(
        big5_dim=X_train["big5"].shape[1],
        cmi_dim=X_train["cmi"].shape[1],
        rppg_dim=X_train["rppg"].shape[1],
        voice_dim=X_train["voice"].shape[1],
    )
    ensemble.train_models(X_train, y_train, X_val, y_val)

    # 5. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    test_predictions = ensemble.predict(X_test)

    # 6. ì„±ëŠ¥ í‰ê°€
    test_r2 = r2_score(y_test, test_predictions)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
    test_mae = mean_absolute_error(y_test, test_predictions)

    print("\nğŸ“Š ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ MAE: {test_mae:.4f}")

    # 7. ê²°ê³¼ ì €ì¥
    results = {
        "improved_model_test": {
            "test_r2": test_r2,
            "test_rmse": test_rmse,
            "test_mae": test_mae,
            "model_count": len(ensemble.models),
            "weights": ensemble.weights,
            "data_source": "BigQuery" if data_loader.client else "Enhanced_Simulated",
            "sample_count": len(enhanced_data["targets"]),
        }
    }

    with open("improved_model_test_results.json", "w") as f:
        json.dump(results, f, indent=2)

    print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
    print("ğŸ“ improved_model_test_results.json")

    return results


if __name__ == "__main__":
    results = test_improved_model()
