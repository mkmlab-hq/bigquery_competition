#!/usr/bin/env python3
"""
ê³ ê¸‰ Big Five í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ìë™ ê²°ì •
- í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
"""

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.metrics import (
    calinski_harabasz_score,
    davies_bouldin_score,
    silhouette_score,
)
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class AdvancedBig5Clustering:
    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scaler = StandardScaler()
        self.results_dir = "advanced_clustering_results"
        import os

        os.makedirs(self.results_dir, exist_ok=True)

    def load_big5_data(self, limit: int = 2000):
        """Big Five ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Big Five ë°ì´í„° ë¡œë“œ ì¤‘...")

        from vector_search_system import Big5VectorSearch

        vs = Big5VectorSearch(project_id=self.project_id)
        data = vs.load_data(limit=limit)

        # Big Five íŠ¹ì„± ì¶”ì¶œ
        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        big5_data = []
        for trait in big5_traits:
            trait_cols = [col for col in data.columns if col.startswith(trait)]
            if trait_cols:
                trait_mean = data[trait_cols].mean(axis=1)
                big5_data.append(trait_mean)

        big5_df = pd.DataFrame(np.array(big5_data).T, columns=big5_traits)
        big5_df["country"] = data["country"].values

        print(f"âœ… Big Five ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(big5_df)}ëª…")
        return big5_df

    def find_optimal_clusters(self, data, max_clusters=10):
        """ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°"""
        print("ğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„ì„ ì¤‘...")

        cluster_range = range(2, max_clusters + 1)
        inertias = []
        silhouette_scores = []
        calinski_scores = []
        davies_bouldin_scores = []

        for k in cluster_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(data)

            inertias.append(kmeans.inertia_)
            silhouette_avg = silhouette_score(data, cluster_labels)
            silhouette_scores.append(silhouette_avg)

            calinski_avg = calinski_harabasz_score(data, cluster_labels)
            calinski_scores.append(calinski_avg)

            davies_avg = davies_bouldin_score(data, cluster_labels)
            davies_bouldin_scores.append(davies_avg)

            print(
                f"í´ëŸ¬ìŠ¤í„° {k}ê°œ: Silhouette={silhouette_avg:.3f}, Calinski={calinski_avg:.1f}, Davies-Bouldin={davies_avg:.3f}"
            )

        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        optimal_k_silhouette = cluster_range[np.argmax(silhouette_scores)]
        optimal_k_calinski = cluster_range[np.argmax(calinski_scores)]
        optimal_k_davies = cluster_range[np.argmin(davies_bouldin_scores)]

        print(f"\nğŸ“Š ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„ì„ ê²°ê³¼:")
        print(f"  Silhouette Score ê¸°ì¤€: {optimal_k_silhouette}ê°œ")
        print(f"  Calinski-Harabasz Score ê¸°ì¤€: {optimal_k_calinski}ê°œ")
        print(f"  Davies-Bouldin Score ê¸°ì¤€: {optimal_k_davies}ê°œ")

        return {
            "cluster_range": list(cluster_range),
            "inertias": inertias,
            "silhouette_scores": silhouette_scores,
            "calinski_scores": calinski_scores,
            "davies_bouldin_scores": davies_bouldin_scores,
            "optimal_k_silhouette": optimal_k_silhouette,
            "optimal_k_calinski": optimal_k_calinski,
            "optimal_k_davies": optimal_k_davies,
        }

    def compare_clustering_algorithms(self, data, n_clusters=5):
        """ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ"""
        print(f"\nğŸ”„ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ (í´ëŸ¬ìŠ¤í„° {n_clusters}ê°œ)...")

        algorithms = {
            "K-Means": KMeans(n_clusters=n_clusters, random_state=42, n_init=10),
            "Gaussian Mixture": GaussianMixture(
                n_components=n_clusters, random_state=42
            ),
            "Agglomerative": AgglomerativeClustering(n_clusters=n_clusters),
            "DBSCAN": DBSCAN(eps=0.5, min_samples=5),
        }

        results = {}

        for name, algorithm in algorithms.items():
            try:
                if name == "DBSCAN":
                    cluster_labels = algorithm.fit_predict(data)
                    n_clusters_found = len(set(cluster_labels)) - (
                        1 if -1 in cluster_labels else 0
                    )
                    print(f"  {name}: {n_clusters_found}ê°œ í´ëŸ¬ìŠ¤í„° ë°œê²¬")
                else:
                    cluster_labels = algorithm.fit_predict(data)
                    n_clusters_found = n_clusters

                if n_clusters_found > 1:
                    silhouette_avg = silhouette_score(data, cluster_labels)
                    calinski_avg = calinski_harabasz_score(data, cluster_labels)
                    davies_avg = davies_bouldin_score(data, cluster_labels)

                    results[name] = {
                        "labels": cluster_labels,
                        "n_clusters": n_clusters_found,
                        "silhouette": silhouette_avg,
                        "calinski": calinski_avg,
                        "davies_bouldin": davies_avg,
                    }

                    print(
                        f"    Silhouette: {silhouette_avg:.3f}, Calinski: {calinski_avg:.1f}, Davies-Bouldin: {davies_avg:.3f}"
                    )
                else:
                    print(f"    {name}: í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶€ì¡±)")

            except Exception as e:
                print(f"    {name}: ì˜¤ë¥˜ ë°œìƒ - {e}")

        return results

    def create_hierarchical_clustering(self, data, max_clusters=10):
        """ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        print("\nğŸŒ³ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¤‘...")

        # ì—°ê²° í–‰ë ¬ ê³„ì‚°
        linkage_matrix = linkage(data, method="ward")

        # ë´ë“œë¡œê·¸ë¨ ìƒì„±
        plt.figure(figsize=(15, 8))
        dendrogram(linkage_matrix, truncate_mode="level", p=5)
        plt.title("Big Five íŠ¹ì„± ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë´ë“œë¡œê·¸ë¨")
        plt.xlabel("ìƒ˜í”Œ ì¸ë±ìŠ¤")
        plt.ylabel("ê±°ë¦¬")
        plt.tight_layout()
        plt.savefig(
            f"{self.results_dir}/hierarchical_dendrogram.png",
            dpi=300,
            bbox_inches="tight",
        )
        plt.close()

        print(f"âœ… ë´ë“œë¡œê·¸ë¨ ì €ì¥: {self.results_dir}/hierarchical_dendrogram.png")

        return linkage_matrix

    def analyze_cluster_characteristics(self, data, labels, algorithm_name):
        """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„"""
        print(f"\nğŸ“Š {algorithm_name} í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        unique_labels = np.unique(labels)

        cluster_analysis = {}

        for cluster_id in unique_labels:
            if cluster_id == -1:  # DBSCANì˜ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
                continue

            cluster_mask = labels == cluster_id
            cluster_data = data[cluster_mask]

            cluster_means = np.mean(cluster_data, axis=0)
            cluster_std = np.std(cluster_data, axis=0)

            cluster_analysis[cluster_id] = {
                "size": len(cluster_data),
                "percentage": len(cluster_data) / len(data) * 100,
                "means": dict(zip(big5_traits, cluster_means)),
                "std": dict(zip(big5_traits, cluster_std)),
            }

            print(
                f"\ní´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_data)}ëª…, {len(cluster_data)/len(data)*100:.1f}%):"
            )
            for trait in big5_traits:
                trait_idx = big5_traits.index(trait)
                print(
                    f"  {trait}: {cluster_means[trait_idx]:.3f} Â± {cluster_std[trait_idx]:.3f}"
                )

        return cluster_analysis

    def create_clustering_visualizations(self, data, results, optimal_analysis):
        """í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” ìƒì„± ì¤‘...")

        # 1. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„ì„ ê·¸ë˜í”„
        plt.figure(figsize=(20, 12))

        # ì„œë¸Œí”Œë¡¯ 1: Elbow Method
        plt.subplot(2, 4, 1)
        plt.plot(optimal_analysis["cluster_range"], optimal_analysis["inertias"], "bo-")
        plt.title("Elbow Method")
        plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
        plt.ylabel("Inertia")
        plt.grid(True)

        # ì„œë¸Œí”Œë¡¯ 2: Silhouette Score
        plt.subplot(2, 4, 2)
        plt.plot(
            optimal_analysis["cluster_range"],
            optimal_analysis["silhouette_scores"],
            "ro-",
        )
        plt.title("Silhouette Score")
        plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
        plt.ylabel("Silhouette Score")
        plt.grid(True)

        # ì„œë¸Œí”Œë¡¯ 3: Calinski-Harabasz Score
        plt.subplot(2, 4, 3)
        plt.plot(
            optimal_analysis["cluster_range"],
            optimal_analysis["calinski_scores"],
            "go-",
        )
        plt.title("Calinski-Harabasz Score")
        plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
        plt.ylabel("Calinski-Harabasz Score")
        plt.grid(True)

        # ì„œë¸Œí”Œë¡¯ 4: Davies-Bouldin Score
        plt.subplot(2, 4, 4)
        plt.plot(
            optimal_analysis["cluster_range"],
            optimal_analysis["davies_bouldin_scores"],
            "mo-",
        )
        plt.title("Davies-Bouldin Score")
        plt.xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜")
        plt.ylabel("Davies-Bouldin Score")
        plt.grid(True)

        # ì„œë¸Œí”Œë¡¯ 5-8: ì•Œê³ ë¦¬ì¦˜ë³„ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë¹„êµ
        algorithm_names = list(results.keys())
        silhouette_scores = [results[alg]["silhouette"] for alg in algorithm_names]
        calinski_scores = [results[alg]["calinski"] for alg in algorithm_names]
        davies_scores = [results[alg]["davies_bouldin"] for alg in algorithm_names]

        plt.subplot(2, 4, 5)
        plt.bar(algorithm_names, silhouette_scores)
        plt.title("Silhouette Score ë¹„êµ")
        plt.xticks(rotation=45)
        plt.ylabel("Silhouette Score")

        plt.subplot(2, 4, 6)
        plt.bar(algorithm_names, calinski_scores)
        plt.title("Calinski-Harabasz Score ë¹„êµ")
        plt.xticks(rotation=45)
        plt.ylabel("Calinski-Harabasz Score")

        plt.subplot(2, 4, 7)
        plt.bar(algorithm_names, davies_scores)
        plt.title("Davies-Bouldin Score ë¹„êµ")
        plt.xticks(rotation=45)
        plt.ylabel("Davies-Bouldin Score")

        # ì„œë¸Œí”Œë¡¯ 8: ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
        best_algorithm = max(algorithm_names, key=lambda x: results[x]["silhouette"])
        plt.subplot(2, 4, 8)
        plt.text(
            0.5,
            0.5,
            f'ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜:\n{best_algorithm}\nSilhouette: {results[best_algorithm]["silhouette"]:.3f}',
            ha="center",
            va="center",
            fontsize=12,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"),
        )
        plt.axis("off")

        plt.tight_layout()
        plt.savefig(
            f"{self.results_dir}/clustering_analysis.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        print(
            f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œê°í™” ì™„ë£Œ: {self.results_dir}/clustering_analysis.png"
        )

        return best_algorithm

    def run_advanced_clustering_analysis(self, limit: int = 2000):
        """ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ Big Five í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        big5_df = self.load_big5_data(limit)
        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        big5_data = big5_df[big5_traits].values

        # 2. ë°ì´í„° ì •ê·œí™”
        big5_scaled = self.scaler.fit_transform(big5_data)

        # 3. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        optimal_analysis = self.find_optimal_clusters(big5_scaled)

        # 4. ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
        n_clusters = optimal_analysis["optimal_k_silhouette"]
        clustering_results = self.compare_clustering_algorithms(big5_scaled, n_clusters)

        # 5. ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        linkage_matrix = self.create_hierarchical_clustering(big5_scaled)

        # 6. ì‹œê°í™” ìƒì„±
        best_algorithm = self.create_clustering_visualizations(
            big5_scaled, clustering_results, optimal_analysis
        )

        # 7. ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
        if best_algorithm in clustering_results:
            best_labels = clustering_results[best_algorithm]["labels"]
            cluster_characteristics = self.analyze_cluster_characteristics(
                big5_scaled, best_labels, best_algorithm
            )

            # ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€
            big5_df["cluster"] = best_labels
            big5_df["algorithm"] = best_algorithm

            return {
                "big5_df": big5_df,
                "optimal_analysis": optimal_analysis,
                "clustering_results": clustering_results,
                "best_algorithm": best_algorithm,
                "cluster_characteristics": cluster_characteristics,
                "linkage_matrix": linkage_matrix,
            }
        else:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨")
            return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§  ê³ ê¸‰ Big Five í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œìŠ¤í…œ")

    analyzer = AdvancedBig5Clustering()
    results = analyzer.run_advanced_clustering_analysis(limit=2000)

    if results:
        print("\nğŸ‰ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ!")
        print(f"   ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜: {results['best_algorithm']}")
        print(f"   ê²°ê³¼ ì €ì¥: {analyzer.results_dir}/")
    else:
        print("\nâŒ ë¶„ì„ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
