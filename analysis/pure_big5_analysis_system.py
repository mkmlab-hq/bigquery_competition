#!/usr/bin/env python3
"""
ìˆœìˆ˜ Big Five ì´ë¡  ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ
- ì‚¬ìƒì²´ì§ˆ ë° MKM12 ì´ë¡  ì œì™¸
- ì„œì–‘ Big Five ì´ë¡  ì¤‘ì‹¬
- BigQuery ëŒ€íšŒ ìµœì í™”
"""

import warnings
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings("ignore", category=UserWarning)


class PureBig5AnalysisSystem:
    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.scaler = StandardScaler()
        self.results_dir = "pure_big5_results"
        import os

        os.makedirs(self.results_dir, exist_ok=True)

    def load_big5_data(self, limit: int = 2000):
        """Big Five ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Big Five ë°ì´í„° ë¡œë“œ ì¤‘...")

        from vector_search_system import Big5VectorSearch

        vs = Big5VectorSearch(project_id=self.project_id)
        data = vs.load_data(limit=limit)

        # Big Five íŠ¹ì„± ì¶”ì¶œ
        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        big5_data = []
        for trait in big5_traits:
            trait_cols = [col for col in data.columns if col.startswith(trait)]
            if trait_cols:
                trait_mean = data[trait_cols].mean(axis=1)
                big5_data.append(trait_mean)

        big5_df = pd.DataFrame(np.array(big5_data).T, columns=big5_traits)
        big5_df["country"] = data["country"].values

        print(f"âœ… Big Five ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(big5_df)}ëª…")
        return big5_df

    def analyze_big5_characteristics(self, big5_df):
        """Big Five íŠ¹ì„± ë¶„ì„"""
        print("ğŸ” Big Five íŠ¹ì„± ë¶„ì„ ì¤‘...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        trait_names = {
            "EXT": "Extraversion (ì™¸í–¥ì„±)",
            "EST": "Neuroticism (ì‹ ê²½ì¦)",
            "AGR": "Agreeableness (ì¹œí™”ì„±)",
            "CSN": "Conscientiousness (ì„±ì‹¤ì„±)",
            "OPN": "Openness (ê°œë°©ì„±)",
        }

        analysis_results = {}

        for trait in big5_traits:
            trait_data = big5_df[trait]

            # ê¸°ë³¸ í†µê³„
            stats = {
                "mean": trait_data.mean(),
                "std": trait_data.std(),
                "min": trait_data.min(),
                "max": trait_data.max(),
                "median": trait_data.median(),
            }

            # ë¶„í¬ ë¶„ì„
            high_count = np.sum(trait_data > 4.0)
            low_count = np.sum(trait_data < 2.0)
            medium_count = len(trait_data) - high_count - low_count

            distribution = {
                "high": {
                    "count": high_count,
                    "percentage": high_count / len(trait_data) * 100,
                },
                "medium": {
                    "count": medium_count,
                    "percentage": medium_count / len(trait_data) * 100,
                },
                "low": {
                    "count": low_count,
                    "percentage": low_count / len(trait_data) * 100,
                },
            }

            analysis_results[trait] = {
                "name": trait_names[trait],
                "stats": stats,
                "distribution": distribution,
            }

            print(f"\\n{trait_names[trait]}:")
            print(f"  í‰ê· : {stats['mean']:.3f}")
            print(f"  í‘œì¤€í¸ì°¨: {stats['std']:.3f}")
            print(
                f"  ë†’ì€ ì ìˆ˜: {high_count}ëª… ({distribution['high']['percentage']:.1f}%)"
            )
            print(
                f"  ë‚®ì€ ì ìˆ˜: {low_count}ëª… ({distribution['low']['percentage']:.1f}%)"
            )

        return analysis_results

    def analyze_correlations(self, big5_df):
        """Big Five íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        print("\\nğŸ”— Big Five íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        correlation_matrix = big5_df[big5_traits].corr()

        print("\\nìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤:")
        print(correlation_matrix.round(3))

        # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
        strong_correlations = []
        for i in range(len(big5_traits)):
            for j in range(i + 1, len(big5_traits)):
                corr_val = correlation_matrix.iloc[i, j]
                if abs(corr_val) > 0.3:
                    strong_correlations.append(
                        {
                            "trait1": big5_traits[i],
                            "trait2": big5_traits[j],
                            "correlation": corr_val,
                        }
                    )

        print(f"\\nê°•í•œ ìƒê´€ê´€ê³„ (|r| > 0.3): {len(strong_correlations)}ê°œ")
        for corr in strong_correlations:
            print(f"  {corr['trait1']}-{corr['trait2']}: {corr['correlation']:.3f}")

        return correlation_matrix, strong_correlations

    def create_personality_clusters(self, big5_df):
        """Big Five ê¸°ë°˜ ì„±ê²© í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        print("\\nğŸ­ Big Five ê¸°ë°˜ ì„±ê²© í´ëŸ¬ìŠ¤í„° ìƒì„± ì¤‘...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        big5_data = big5_df[big5_traits].values

        # ì •ê·œí™”
        big5_scaled = self.scaler.fit_transform(big5_data)

        # K-means í´ëŸ¬ìŠ¤í„°ë§ (ìˆœìˆ˜ Big Five ì´ë¡  ê¸°ë°˜ - 5ê°œ í´ëŸ¬ìŠ¤í„°)
        kmeans = KMeans(n_clusters=5, random_state=42)
        clusters = kmeans.fit_predict(big5_scaled)

        big5_df["personality_cluster"] = clusters

        print(f"âœ… ì„±ê²© í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ: 5ê°œ í´ëŸ¬ìŠ¤í„° (ìˆœìˆ˜ Big Five ì´ë¡ )")

        # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
        cluster_analysis = {}
        for i in range(5):
            cluster_data = big5_df[big5_df["personality_cluster"] == i]
            cluster_means = cluster_data[big5_traits].mean()

            cluster_analysis[i] = {
                "size": len(cluster_data),
                "percentage": len(cluster_data) / len(big5_df) * 100,
                "means": cluster_means.to_dict(),
            }

            print(
                f"\\ní´ëŸ¬ìŠ¤í„° {i+1} ({len(cluster_data)}ëª…, {len(cluster_data)/len(big5_df)*100:.1f}%):"
            )
            for trait in big5_traits:
                print(f"  {trait}: {cluster_means[trait]:.3f}")

        return cluster_analysis

    def generate_personality_insights(self, big5_df, cluster_analysis):
        """ì„±ê²© ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        print("\\nğŸ’¡ ì„±ê²© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]
        trait_names = {
            "EXT": "ì™¸í–¥ì„±",
            "EST": "ì‹ ê²½ì¦",
            "AGR": "ì¹œí™”ì„±",
            "CSN": "ì„±ì‹¤ì„±",
            "OPN": "ê°œë°©ì„±",
        }

        insights = []

        for cluster_id, cluster_info in cluster_analysis.items():
            means = cluster_info["means"]

            # ê° íŠ¹ì„±ì˜ ìˆ˜ì¤€ íŒë‹¨
            trait_levels = {}
            for trait in big5_traits:
                if trait == "EST":  # ì‹ ê²½ì¦ì€ ì—­ë°©í–¥
                    if means[trait] < 2.5:
                        trait_levels[trait] = "ë‚®ìŒ (ì•ˆì •ì )"
                    elif means[trait] < 3.5:
                        trait_levels[trait] = "ì¤‘ê°„"
                    else:
                        trait_levels[trait] = "ë†’ìŒ (ë¶ˆì•ˆì •)"
                else:
                    if means[trait] > 4.0:
                        trait_levels[trait] = "ë†’ìŒ"
                    elif means[trait] > 3.0:
                        trait_levels[trait] = "ì¤‘ê°„"
                    else:
                        trait_levels[trait] = "ë‚®ìŒ"

            # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ìš”ì•½
            cluster_summary = f"í´ëŸ¬ìŠ¤í„° {cluster_id+1}: "
            for trait in big5_traits:
                cluster_summary += f"{trait_names[trait]}({trait_levels[trait]}) "

            insights.append(
                {
                    "cluster_id": cluster_id,
                    "size": cluster_info["size"],
                    "percentage": cluster_info["percentage"],
                    "summary": cluster_summary,
                    "trait_levels": trait_levels,
                }
            )

        print("\\nì„±ê²© í´ëŸ¬ìŠ¤í„° ìš”ì•½:")
        for insight in insights:
            print(f"  {insight['summary']}")

        return insights

    def create_visualizations(self, big5_df, correlation_matrix, cluster_analysis):
        """ì‹œê°í™” ìƒì„±"""
        print("\\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

        big5_traits = ["EXT", "EST", "AGR", "CSN", "OPN"]

        # 1. Big Five íŠ¹ì„± ë¶„í¬
        plt.figure(figsize=(15, 10))

        # ì„œë¸Œí”Œë¡¯ 1: Big Five íŠ¹ì„± ë¶„í¬
        plt.subplot(2, 3, 1)
        big5_df[big5_traits].boxplot()
        plt.title("Big Five íŠ¹ì„± ë¶„í¬")
        plt.xticks(rotation=45)

        # ì„œë¸Œí”Œë¡¯ 2: ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.subplot(2, 3, 2)
        sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", center=0)
        plt.title("Big Five íŠ¹ì„± ìƒê´€ê´€ê³„")

        # ì„œë¸Œí”Œë¡¯ 3: í´ëŸ¬ìŠ¤í„° ë¶„í¬
        plt.subplot(2, 3, 3)
        cluster_counts = big5_df["personality_cluster"].value_counts().sort_index()
        plt.bar(cluster_counts.index, cluster_counts.values)
        plt.title("ì„±ê²© í´ëŸ¬ìŠ¤í„° ë¶„í¬")
        plt.xlabel("í´ëŸ¬ìŠ¤í„°")
        plt.ylabel("ì¸ì› ìˆ˜")

        # ì„œë¸Œí”Œë¡¯ 4-6: í´ëŸ¬ìŠ¤í„°ë³„ Big Five íŠ¹ì„± (ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„°)
        cluster_counts = big5_df["personality_cluster"].value_counts()
        top_clusters = cluster_counts.head(3).index

        for i, cluster_id in enumerate(top_clusters):
            plt.subplot(2, 3, 4 + i)
            cluster_data = big5_df[big5_df["personality_cluster"] == cluster_id]
            if len(cluster_data) > 0:
                cluster_means = cluster_data[big5_traits].mean()
                plt.bar(big5_traits, cluster_means.values)
                plt.title(f"í´ëŸ¬ìŠ¤í„° {cluster_id+1} íŠ¹ì„± ({len(cluster_data)}ëª…)")
                plt.xticks(rotation=45)

        plt.tight_layout()
        plt.savefig(
            f"{self.results_dir}/pure_big5_analysis.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        print(f"âœ… ì‹œê°í™” ì™„ë£Œ: {self.results_dir}/pure_big5_analysis.png")

    def run_pure_big5_analysis(self, limit: int = 2000):
        """ìˆœìˆ˜ Big Five ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ìˆœìˆ˜ Big Five ì´ë¡  ê¸°ë°˜ ë¶„ì„ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        big5_df = self.load_big5_data(limit)

        # 2. Big Five íŠ¹ì„± ë¶„ì„
        trait_analysis = self.analyze_big5_characteristics(big5_df)

        # 3. ìƒê´€ê´€ê³„ ë¶„ì„
        correlation_matrix, strong_correlations = self.analyze_correlations(big5_df)

        # 4. ì„±ê²© í´ëŸ¬ìŠ¤í„° ìƒì„±
        cluster_analysis = self.create_personality_clusters(big5_df)

        # 5. ì„±ê²© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.generate_personality_insights(big5_df, cluster_analysis)

        # 6. ì‹œê°í™” ìƒì„±
        self.create_visualizations(big5_df, correlation_matrix, cluster_analysis)

        return {
            "big5_df": big5_df,
            "trait_analysis": trait_analysis,
            "correlation_matrix": correlation_matrix,
            "strong_correlations": strong_correlations,
            "cluster_analysis": cluster_analysis,
            "insights": insights,
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§  ìˆœìˆ˜ Big Five ì´ë¡  ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ")

    analyzer = PureBig5AnalysisSystem()
    results = analyzer.run_pure_big5_analysis(limit=2000)

    print("\\nğŸ‰ ìˆœìˆ˜ Big Five ë¶„ì„ ì™„ë£Œ!")
    print(f"   ê²°ê³¼ ì €ì¥: {analyzer.results_dir}/")


if __name__ == "__main__":
    main()
