#!/usr/bin/env python3
"""
í–¥ìƒëœ SHAP ë¶„ì„ ì‹œìŠ¤í…œ
- ë°ì´í„° ì˜ì¡´ì„± ì™„í™”
- ê°€ì¤‘ì¹˜ ìµœì í™”
- ì„±ëŠ¥ ìµœì í™”
- ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
"""

import json
import os
from typing import Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score, train_test_split

from vector_search_system import Big5VectorSearch


class EnhancedSHAPAnalyzer:
    def __init__(self, project_id: str = "persona-diary-service"):
        self.project_id = project_id
        self.vector_search = Big5VectorSearch(project_id)
        self.results_dir = "shap_analysis_results"
        os.makedirs(self.results_dir, exist_ok=True)

    def load_data_with_fallback(self, limit: int = 2000) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ì•ˆ ì œê³µ"""
        try:
            print("BigQueryì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
            data = self.vector_search.load_data(limit=limit)
            if data.empty:
                raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(data)}ê±´")
            return data
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ëŒ€ì²´ ë°ì´í„° ìƒì„± ì¤‘...")
            return self._generate_fallback_data(limit)

    def _generate_fallback_data(self, limit: int) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°ì´í„° ìƒì„±"""
        np.random.seed(42)

        # Big5 íŠ¹ì„± ì»¬ëŸ¼ ìƒì„±
        big5_cols = []
        for trait in ["EXT", "EST", "AGR", "CSN", "OPN"]:
            for i in range(1, 11):
                big5_cols.append(f"{trait}{i}")

        # ëœë¤ ë°ì´í„° ìƒì„± (1-6 ë²”ìœ„)
        data = {}
        for col in big5_cols:
            if col.startswith(("EST", "AGR")):
                data[col] = np.random.randint(2, 7, limit)  # 2-6 ë²”ìœ„
            else:
                data[col] = np.random.randint(1, 6, limit)  # 1-5 ë²”ìœ„

        # êµ­ê°€ ì •ë³´ ì¶”ê°€
        countries = ["US", "GB", "CA", "AU", "IN", "DE", "FR", "JP", "KR", "BR"]
        data["country"] = np.random.choice(countries, limit)

        df = pd.DataFrame(data)
        print(f"âœ… ëŒ€ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê±´")
        return df

    def optimize_target_weights(self, data: pd.DataFrame) -> Dict[str, float]:
        """ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”"""
        print("ğŸ”§ íƒ€ê²Ÿ ë³€ìˆ˜ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...")

        # ì‹¤ì œ ê´€ì°°ëœ í–‰ë™ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜ (ë” í˜„ì‹¤ì ì¸ íƒ€ê²Ÿ)
        # ì‚¬íšŒì  í™œë™ ì°¸ì—¬ë„
        social_activity = (
            data["EXT1"] * 0.4
            + data["EXT2"] * 0.3
            + data["EXT3"] * 0.3
            + data["AGR1"] * 0.2
            + data["AGR2"] * 0.2
        )

        # ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ (EST ì—­ìƒê´€)
        stress_management = 6 - (data["EST1"] + data["EST2"] + data["EST3"]) / 3

        # ëª©í‘œ ë‹¬ì„± (CSN ê¸°ë°˜)
        goal_achievement = (data["CSN1"] + data["CSN2"] + data["CSN3"]) / 3

        # ì°½ì˜ì„± (OPN ê¸°ë°˜)
        creativity = (data["OPN1"] + data["OPN2"] + data["OPN3"]) / 3

        # ì‹¤ì œ íƒ€ê²Ÿ ë³€ìˆ˜ (ê°€ì¤‘ í‰ê· )
        y_actual = (
            social_activity * 0.3
            + stress_management * 0.25
            + goal_achievement * 0.25
            + creativity * 0.2
        )

        # ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìœ„í•œ íŠ¹ì„± ë§¤íŠ¸ë¦­ìŠ¤
        X_features = pd.DataFrame(
            {
                "social": social_activity,
                "stress_mgmt": stress_management,
                "goal_ach": goal_achievement,
                "creativity": creativity,
            }
        )

        # ì„ í˜• íšŒê·€ë¡œ ìµœì  ê°€ì¤‘ì¹˜ í•™ìŠµ
        reg = LinearRegression().fit(X_features, y_actual)

        optimized_weights = {
            "social": float(reg.coef_[0]),
            "stress_mgmt": float(reg.coef_[1]),
            "goal_ach": float(reg.coef_[2]),
            "creativity": float(reg.coef_[3]),
        }

        print(f"âœ… ìµœì í™”ëœ ê°€ì¤‘ì¹˜: {optimized_weights}")
        return optimized_weights

    def generate_optimized_target_variable(
        self, data: pd.DataFrame, weights: Dict[str, float]
    ) -> pd.Series:
        """ìµœì í™”ëœ ê°€ì¤‘ì¹˜ë¡œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
        print("ğŸ¯ ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

        # ê° í–‰ë™ íŒ¨í„´ ê³„ì‚°
        social_activity = (
            data["EXT1"] * 0.4
            + data["EXT2"] * 0.3
            + data["EXT3"] * 0.3
            + data["AGR1"] * 0.2
            + data["AGR2"] * 0.2
        )

        stress_management = 6 - (data["EST1"] + data["EST2"] + data["EST3"]) / 3
        goal_achievement = (data["CSN1"] + data["CSN2"] + data["CSN3"]) / 3
        creativity = (data["OPN1"] + data["OPN2"] + data["OPN3"]) / 3

        # ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©
        satisfaction = (
            social_activity * weights["social"]
            + stress_management * weights["stress_mgmt"]
            + goal_achievement * weights["goal_ach"]
            + creativity * weights["creativity"]
        )

        # ì •ê·œí™” (1-10 ìŠ¤ì¼€ì¼)
        satisfaction = (
            (satisfaction - satisfaction.min())
            / (satisfaction.max() - satisfaction.min())
        ) * 9 + 1

        print(
            f"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ: í‰ê·  {satisfaction.mean():.2f}, í‘œì¤€í¸ì°¨ {satisfaction.std():.2f}"
        )
        return satisfaction

    def train_enhanced_model(
        self, X: pd.DataFrame, y: pd.Series
    ) -> RandomForestRegressor:
        """í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""
        print("ğŸ¤– í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ ì¤‘...")

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
        model = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=3,
            min_samples_leaf=1,
            max_features="sqrt",
            random_state=42,
        )

        model.fit(X_train, y_train)

        # ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        y_pred = model.predict(X_test)

        performance_metrics = {
            "r2_score": r2_score(y_test, y_pred),
            "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
            "mae": mean_absolute_error(y_test, y_pred),
            "cv_r2_mean": cross_val_score(model, X, y, cv=5, scoring="r2").mean(),
            "cv_r2_std": cross_val_score(model, X, y, cv=5, scoring="r2").std(),
        }

        print(f"âœ… ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   RÂ² Score: {performance_metrics['r2_score']:.3f}")
        print(f"   RMSE: {performance_metrics['rmse']:.3f}")
        print(f"   MAE: {performance_metrics['mae']:.3f}")
        print(
            f"   CV RÂ²: {performance_metrics['cv_r2_mean']:.3f} Â± {performance_metrics['cv_r2_std']:.3f}"
        )

        return model, performance_metrics

    def optimized_shap_analysis(
        self, model: RandomForestRegressor, X: pd.DataFrame, sample_size: int = 500
    ) -> dict:
        """ìµœì í™”ëœ SHAP ë¶„ì„"""
        print(f"ğŸ”¬ ìµœì í™”ëœ SHAP ë¶„ì„ ì¤‘... (ìƒ˜í”Œ í¬ê¸°: {sample_size})")

        # ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚° ë¹„ìš© ìµœì í™”
        if len(X) > sample_size:
            X_sample = X.sample(n=sample_size, random_state=42)
            print(f"   ìƒ˜í”Œë§ ì ìš©: {len(X)} â†’ {len(X_sample)}")
        else:
            X_sample = X

        # SHAP ë¶„ì„
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_sample)

        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = pd.DataFrame(
            {
                "feature": X_sample.columns,
                "importance": np.abs(shap_values).mean(0),
                "mean_shap": shap_values.mean(0),
                "std_shap": np.std(shap_values, axis=0),
            }
        ).sort_values("importance", ascending=False)

        # ì„±ê²© íŠ¹ì„±ë³„ ì˜í–¥ë ¥ ì§‘ê³„
        trait_impact = {}
        for trait in ["EXT", "EST", "AGR", "CSN", "OPN"]:
            trait_cols = [col for col in X_sample.columns if col.startswith(trait)]
            if trait_cols:
                trait_importance = feature_importance[
                    feature_importance["feature"].isin(trait_cols)
                ]["importance"].mean()
                trait_impact[trait] = float(trait_importance)

        return {
            "shap_values": shap_values,
            "feature_importance": feature_importance,
            "trait_impact": trait_impact,
            "sample_size": len(X_sample),
        }

    def save_results(
        self, analysis_result: dict, performance_metrics: dict, weights: dict
    ):
        """ê²°ê³¼ ì €ì¥"""
        print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # 1. íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        analysis_result["feature_importance"].to_csv(
            f"{self.results_dir}/feature_importance.csv", index=False
        )

        # 2. ì„±ëŠ¥ ì§€í‘œ ì €ì¥
        with open(f"{self.results_dir}/performance_metrics.json", "w") as f:
            json.dump(performance_metrics, f, indent=2)

        # 3. ê°€ì¤‘ì¹˜ ì €ì¥
        with open(f"{self.results_dir}/optimized_weights.json", "w") as f:
            json.dump(weights, f, indent=2)

        # 4. ì¢…í•© ë³´ê³ ì„œ ì €ì¥
        report = {
            "performance_metrics": performance_metrics,
            "trait_impact": analysis_result["trait_impact"],
            "optimized_weights": weights,
            "sample_size": analysis_result["sample_size"],
        }

        with open(f"{self.results_dir}/comprehensive_report.json", "w") as f:
            json.dump(report, f, indent=2)

        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.results_dir}/")

    def create_visualizations(self, analysis_result: dict, performance_metrics: dict):
        """ì‹œê°í™” ìƒì„±"""
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

        # 1. íŠ¹ì„± ì¤‘ìš”ë„ ë§‰ëŒ€ ê·¸ë˜í”„
        plt.figure(figsize=(12, 8))
        top_features = analysis_result["feature_importance"].head(15)
        plt.barh(range(len(top_features)), top_features["importance"])
        plt.yticks(range(len(top_features)), top_features["feature"])
        plt.xlabel("SHAP Importance")
        plt.title("Top 15 Feature Importance (SHAP)")
        plt.tight_layout()
        plt.savefig(
            f"{self.results_dir}/feature_importance.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        # 2. ì„±ê²© íŠ¹ì„±ë³„ ì˜í–¥ë ¥
        plt.figure(figsize=(10, 6))
        traits = list(analysis_result["trait_impact"].keys())
        impacts = list(analysis_result["trait_impact"].values())

        bars = plt.bar(
            traits,
            impacts,
            color=["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FFEAA7"],
        )
        plt.xlabel("Big5 Personality Traits")
        plt.ylabel("SHAP Impact")
        plt.title("Personality Trait Impact on Satisfaction")

        # ê°’ í‘œì‹œ
        for bar, impact in zip(bars, impacts):
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.001,
                f"{impact:.3f}",
                ha="center",
                va="bottom",
            )

        plt.tight_layout()
        plt.savefig(
            f"{self.results_dir}/trait_impact.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        # 3. SHAP ìš”ì•½ í”Œë¡¯
        if len(analysis_result["shap_values"]) > 0:
            plt.figure(figsize=(10, 8))
            shap.summary_plot(
                analysis_result["shap_values"],
                analysis_result["feature_importance"]["feature"].values.reshape(1, -1)[
                    0
                ][: len(analysis_result["shap_values"][0])],
                show=False,
            )
            plt.title("SHAP Summary Plot")
            plt.tight_layout()
            plt.savefig(
                f"{self.results_dir}/shap_summary.png", dpi=300, bbox_inches="tight"
            )
            plt.close()

        print(f"âœ… ì‹œê°í™” ì™„ë£Œ: {self.results_dir}/")

    def run_enhanced_analysis(
        self, data_limit: int = 2000, sample_size: int = 500
    ) -> dict:
        """í–¥ìƒëœ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ í–¥ìƒëœ SHAP ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ (ëŒ€ì²´ ë°©ì•ˆ í¬í•¨)
        data = self.load_data_with_fallback(data_limit)

        # 2. ê°€ì¤‘ì¹˜ ìµœì í™”
        weights = self.optimize_target_weights(data)

        # 3. ìµœì í™”ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        y = self.generate_optimized_target_variable(data, weights)

        # 4. Big5 íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ
        big5_cols = [
            col
            for col in data.columns
            if any(trait in col for trait in ["EXT", "EST", "AGR", "CSN", "OPN"])
        ]
        X = data[big5_cols]

        # 5. í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨
        model, performance_metrics = self.train_enhanced_model(X, y)

        # 6. ìµœì í™”ëœ SHAP ë¶„ì„
        analysis_result = self.optimized_shap_analysis(model, X, sample_size)

        # 7. ê²°ê³¼ ì €ì¥
        self.save_results(analysis_result, performance_metrics, weights)

        # 8. ì‹œê°í™” ìƒì„±
        self.create_visualizations(analysis_result, performance_metrics)

        return {
            "analysis_result": analysis_result,
            "performance_metrics": performance_metrics,
            "optimized_weights": weights,
            "data_info": {
                "total_records": len(data),
                "sample_size": analysis_result["sample_size"],
            },
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ í–¥ìƒëœ SHAP ë¶„ì„ ì‹œìŠ¤í…œ")

    analyzer = EnhancedSHAPAnalyzer()

    # í–¥ìƒëœ ë¶„ì„ ì‹¤í–‰
    results = analyzer.run_enhanced_analysis(data_limit=2000, sample_size=500)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š í–¥ìƒëœ SHAP ë¶„ì„ ê²°ê³¼")
    print("=" * 60)

    print(f"\nğŸ¯ ëª¨ë¸ ì„±ëŠ¥:")
    metrics = results["performance_metrics"]
    print(f"   RÂ² Score: {metrics['r2_score']:.3f}")
    print(f"   RMSE: {metrics['rmse']:.3f}")
    print(f"   MAE: {metrics['mae']:.3f}")
    print(f"   CV RÂ²: {metrics['cv_r2_mean']:.3f} Â± {metrics['cv_r2_std']:.3f}")

    print(f"\nğŸ“ˆ ì„±ê²© íŠ¹ì„±ë³„ ì˜í–¥ë ¥:")
    for trait, impact in sorted(
        results["analysis_result"]["trait_impact"].items(),
        key=lambda x: x[1],
        reverse=True,
    ):
        print(f"   {trait}: {impact:.3f}")

    print(f"\nâš–ï¸ ìµœì í™”ëœ ê°€ì¤‘ì¹˜:")
    for feature, weight in results["optimized_weights"].items():
        print(f"   {feature}: {weight:.3f}")

    print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {results['data_info']['total_records']:,}")
    print(f"   SHAP ë¶„ì„ ìƒ˜í”Œ ìˆ˜: {results['data_info']['sample_size']:,}")

    print(f"\nğŸ’¾ ì €ì¥ëœ íŒŒì¼:")
    print(f"   ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {analyzer.results_dir}/")
    print(f"   ğŸ“„ feature_importance.csv")
    print(f"   ğŸ“„ performance_metrics.json")
    print(f"   ğŸ“„ optimized_weights.json")
    print(f"   ğŸ“„ comprehensive_report.json")
    print(f"   ğŸ–¼ï¸ feature_importance.png")
    print(f"   ğŸ–¼ï¸ trait_impact.png")
    print(f"   ğŸ–¼ï¸ shap_summary.png")

    print(f"\nâœ… í–¥ìƒëœ SHAP ë¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
